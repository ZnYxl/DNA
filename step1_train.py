import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch.nn.functional as F
from collections import defaultdict

# ==========================================
# å®šä¹‰æƒé‡å‚æ•°
# ==========================================
alpha = 1.0
beta = 0.01
gamma = 0.01

# ==========================================
# å¯¼å…¥åŸºç¡€ç»„ä»¶
# ==========================================
try:
    from models.conmamba import ConmambaBlock
    print("âœ… æˆåŠŸå¯¼å…¥ ConmambaBlock")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ==========================================
# ç®€åŒ–çš„æ ¸å¿ƒç»„ä»¶
# ==========================================

class SimpleEncoder(nn.Module):
    """ç®€åŒ–çš„ç¼–ç å™¨ - åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, input_dim=4, hidden_dim=64, seq_len=150):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # ç®€å•çš„ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # ConMambaå—ç”¨äºåºåˆ—å»ºæ¨¡
        self.conmamba = ConmambaBlock(dim=hidden_dim)
        
    def forward(self, x):
        # x: [B*N, L, 4] 
        x = self.feature_extractor(x)  # [B*N, L, hidden_dim]
        x = self.conmamba(x)           # [B*N, L, hidden_dim]
        return x

class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—"""
    def __init__(self, hidden_dim=64, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
    
    def forward(self, embeddings):
        # embeddings: [B*N, L, hidden_dim]
        # å–å¹³å‡æ± åŒ–ä½œä¸ºåºåˆ—è¡¨ç¤º
        seq_repr = torch.mean(embeddings, dim=1)  # [B*N, hidden_dim]
        projected = self.projection(seq_repr)     # [B*N, hidden_dim//2]
        return F.normalize(projected, dim=-1)

class EvidenceDecoder(nn.Module):
    """è¯æ®è§£ç å™¨ - è¾“å‡ºEvidenceå‘é‡"""
    def __init__(self, hidden_dim=64, output_dim=4):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softplus()  # ç¡®ä¿è¾“å‡ºä¸ºæ­£å€¼ (Evidence)
        )
        
    def forward(self, x):
        # x: [B*N, L, hidden_dim]
        evidence = self.decoder(x)  # [B*N, L, 4]
        return evidence

class EvidenceFusion(nn.Module):
    """è¯æ®èåˆæ¨¡å—"""
    def __init__(self):
        super().__init__()
        
    def calculate_strength(self, evidence):
        """è®¡ç®—è¯æ®å¼ºåº¦ä½œä¸ºèåˆæƒé‡"""
        # evidence: [N, L, 4]
        strength = torch.sum(evidence, dim=-1, keepdim=True)  # [N, L, 1]
        return strength
    
    def forward(self, evidence_batch):
        """
        evidence_batch: [N, L, 4] - Næ¡readsçš„evidence
        """
        # è®¡ç®—æ¯æ¡readçš„è¯æ®å¼ºåº¦
        strengths = self.calculate_strength(evidence_batch)  # [N, L, 1]
        
        # åŠ æƒèåˆ
        weighted_evidence = evidence_batch * strengths       # [N, L, 4]
        fused_evidence = torch.sum(weighted_evidence, dim=0) # [L, 4]
        total_weight = torch.sum(strengths, dim=0)           # [L, 1]
        
        # é¿å…é™¤é›¶
        fused_evidence = fused_evidence / (total_weight + 1e-8)
        
        return fused_evidence, strengths

class SimplifiedFedDNA(nn.Module):
    """ç®€åŒ–ç‰ˆFedDNA - åªä¿ç•™æ ¸å¿ƒç»„ä»¶"""
    def __init__(self, input_dim=4, hidden_dim=64, seq_len=150):
        super().__init__()
        self.encoder = SimpleEncoder(input_dim, hidden_dim, seq_len)
        self.contrastive = ContrastiveLearning(hidden_dim)
        self.evidence_decoder = EvidenceDecoder(hidden_dim, input_dim)
        self.evidence_fusion = EvidenceFusion()
        
    def forward(self, reads_batch):
        """
        reads_batch: [B, N, L, 4] - Bä¸ªbatchï¼Œæ¯ä¸ªbatchæœ‰Næ¡reads
        """
        B, N, L, D = reads_batch.shape
        
        # é‡å¡‘ä¸º [B*N, L, D]
        reads_flat = reads_batch.view(B * N, L, D)
        
        # 1. ç¼–ç 
        embeddings = self.encoder(reads_flat)  # [B*N, L, hidden_dim]
        
        # 2. å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        contrastive_features = self.contrastive(embeddings)  # [B*N, hidden_dim//2]
        
        # 3. è¯æ®è§£ç 
        evidence = self.evidence_decoder(embeddings)  # [B*N, L, 4]
        
        # 4. é‡å¡‘å›batchå½¢å¼
        evidence = evidence.view(B, N, L, D)  # [B, N, L, 4]
        contrastive_features = contrastive_features.view(B, N, -1)  # [B, N, hidden_dim//2]
        
        return evidence, contrastive_features

# ==========================================
# ç»¼åˆæŸå¤±å‡½æ•°
# ==========================================

class ComprehensiveLoss(nn.Module):
    """ç»¼åˆæŸå¤±å‡½æ•° - åŒ…å«é‡æ„ã€å¯¹æ¯”å­¦ä¹ ã€KLæ•£åº¦"""
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01, temperature=0.1):
        super().__init__()
        self.alpha = alpha      # é‡æ„æŸå¤±æƒé‡
        self.beta = beta        # å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡  
        self.gamma = gamma      # KLæ•£åº¦æŸå¤±æƒé‡
        self.temperature = temperature
        
    def contrastive_loss(self, features, cluster_labels=None):
        """
        å¯¹æ¯”å­¦ä¹ æŸå¤± - åŒç°‡å†…çš„readsåº”è¯¥ç›¸ä¼¼ï¼Œä¸åŒç°‡åº”è¯¥ä¸åŒ
        features: [N, feature_dim] - Næ¡readsçš„å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        """
        if features.shape[0] <= 1:
            return torch.tensor(0.0, device=features.device)
            
        # ç®€åŒ–ç‰ˆï¼šå‡è®¾åŒä¸€ä¸ªbatchå†…çš„readså±äºåŒä¸€ç°‡
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        features_norm = F.normalize(features, dim=1)
        similarity_matrix = torch.matmul(features_norm, features_norm.T) / self.temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬mask (åŒä¸€batchå†…ä¸ºæ­£æ ·æœ¬)
        batch_size = features.shape[0]
        mask = torch.eye(batch_size, device=features.device).bool()
        
        # è®¡ç®—InfoNCEæŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        exp_sim = exp_sim.masked_fill(mask, 0)  # ç§»é™¤è‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦
        
        # æ­£æ ·æœ¬ï¼šåŒbatchå†…å…¶ä»–æ ·æœ¬çš„å¹³å‡
        pos_sim = torch.sum(exp_sim, dim=1) / (batch_size - 1)
        # è´Ÿæ ·æœ¬ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨æ‰€æœ‰æ ·æœ¬
        neg_sim = torch.sum(exp_sim, dim=1)
        
        # InfoNCEæŸå¤±
        loss = -torch.log(pos_sim / (neg_sim + 1e-8))
        return torch.mean(loss)
    
    def kl_divergence_loss(self, evidence):
        """
        KLæ•£åº¦æŸå¤± - è¡¡é‡è¯æ®åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§
        evidence: [L, 4] - èåˆåçš„è¯æ®å‘é‡
        """
        # å°†evidenceè½¬æ¢ä¸ºDirichletåˆ†å¸ƒå‚æ•°
        alpha = evidence + 1  # [L, 4]
        
        # è®¡ç®—ä¸å‡åŒ€åˆ†å¸ƒçš„KLæ•£åº¦ (ç®€åŒ–ç‰ˆæœ¬)
        # ä½¿ç”¨è¯æ®çš„æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
        evidence_normalized = F.softmax(evidence, dim=-1)
        uniform_dist = torch.ones_like(evidence_normalized) / evidence_normalized.shape[-1]
        
        # KLæ•£åº¦: KL(P||Q) = sum(P * log(P/Q))
        kl_div = torch.sum(evidence_normalized * torch.log(evidence_normalized / (uniform_dist + 1e-8) + 1e-8), dim=-1)
        
        return torch.mean(kl_div)
    
    def forward(self, fused_evidence, ref, contrastive_features):
        """
        è®¡ç®—æ€»æŸå¤±
        fused_evidence: [L, 4] - èåˆåçš„è¯æ®
        ref: [L, 4] - å‚è€ƒåºåˆ—
        contrastive_features: [N, feature_dim] - å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        """
        # 1. é‡æ„æŸå¤±
        reconstruction_loss = F.mse_loss(fused_evidence, ref)
        
        # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.contrastive_loss(contrastive_features)
        
        # 3. KLæ•£åº¦æŸå¤±
        kl_loss = self.kl_divergence_loss(fused_evidence)
        
        # 4. æ€»æŸå¤±
        total_loss = (self.alpha * reconstruction_loss + 
                     self.beta * contrastive_loss + 
                     self.gamma * kl_loss)
        
        return {
            'total_loss': total_loss,
            'reconstruction_loss': reconstruction_loss,
            'contrastive_loss': contrastive_loss,
            'kl_loss': kl_loss
        }

# ==========================================
# æ•°æ®é›† (ä¿æŒä¸å˜)
# ==========================================
class CloverClusterDataset(Dataset):
    def __init__(self, data_dir, seq_len=150):
        self.seq_len = seq_len
        self.clusters = [] 
        
        read_path = os.path.join(data_dir, "read.txt")
        ref_path = os.path.join(data_dir, "ref.txt")
        if not os.path.exists(ref_path):
            ref_path = os.path.join(data_dir, "reference.txt")
            
        if not os.path.exists(read_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {read_path}")
            return
        
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {data_dir}")
        with open(ref_path, 'r') as f:
            refs = [line.strip() for line in f if line.strip()]
        with open(read_path, 'r') as f:
            content = f.read().strip()
        raw_clusters = content.split("===============================")
        
        for i, cluster_block in enumerate(raw_clusters):
            if not cluster_block.strip(): continue
            if i >= len(refs): break
            reads = [r.strip() for r in cluster_block.strip().split('\n') if r.strip()]
            if len(reads) > 0:
                self.clusters.append({'ref': refs[i], 'reads': reads})
                
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.clusters)} ä¸ªç°‡")

    def one_hot(self, seq):
        mapping = {'A':0, 'C':1, 'G':2, 'T':3}
        arr = np.zeros((self.seq_len, 4), dtype=np.float32)
        l = min(len(seq), self.seq_len)
        for i in range(l):
            char = seq[i]
            if char in mapping: arr[i, mapping[char]] = 1.0
        return arr

    def __len__(self): return len(self.clusters)

    def __getitem__(self, idx):
        cluster = self.clusters[idx]
        reads_vec = np.array([self.one_hot(r) for r in cluster['reads']])
        ref_vec = self.one_hot(cluster['ref'])
        return torch.tensor(reads_vec), torch.tensor(ref_vec)

# ==========================================
# æ­¥éª¤2: å›°éš¾æ ·æœ¬ä¿®æ­£æ¨¡å—
# ==========================================

class EvidenceRefinement(nn.Module):
    """è¯æ®é©±åŠ¨çš„å›°éš¾æ ·æœ¬ä¿®æ­£æ¨¡å—"""
    
    def __init__(self, confidence_threshold=0.5, distance_threshold=2.0):
        super().__init__()
        self.confidence_threshold = confidence_threshold
        self.distance_threshold = distance_threshold
        
    def calculate_confidence_score(self, evidence_strengths):
        """
        è®¡ç®—è¯æ®ç½®ä¿¡åº¦åˆ†æ•°
        evidence_strengths: [N, L, 1] - Næ¡readsçš„è¯æ®å¼ºåº¦
        è¿”å›: [N] - æ¯æ¡readçš„ç½®ä¿¡åº¦åˆ†æ•°
        """
        # æ–¹æ³•1: ä½¿ç”¨è¯æ®å¼ºåº¦çš„å¹³å‡å€¼ä½œä¸ºç½®ä¿¡åº¦
        confidence_scores = torch.mean(evidence_strengths.squeeze(-1), dim=1)  # [N]
        
        return confidence_scores
    
    def compute_cluster_centers(self, embeddings, labels, num_clusters):
        """
        è®¡ç®—ç°‡ä¸­å¿ƒ
        embeddings: [N, feature_dim] - readsçš„åµŒå…¥è¡¨ç¤º
        labels: [N] - å½“å‰æ ‡ç­¾
        num_clusters: int - ç°‡çš„æ•°é‡
        è¿”å›: [K, feature_dim] - Kä¸ªç°‡ä¸­å¿ƒ
        """
        device = embeddings.device
        feature_dim = embeddings.shape[1]
        centers = torch.zeros(num_clusters, feature_dim, device=device)
        
        for k in range(num_clusters):
            mask = (labels == k)
            if mask.sum() > 0:
                centers[k] = torch.mean(embeddings[mask], dim=0)
            else:
                # å¦‚æœæŸä¸ªç°‡ä¸ºç©ºï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–
                centers[k] = torch.randn(feature_dim, device=device)
                
        return centers
    
    def reassign_hard_samples(self, hard_embeddings, cluster_centers):
        """
        é‡åˆ†é…å›°éš¾æ ·æœ¬
        hard_embeddings: [M, feature_dim] - å›°éš¾æ ·æœ¬çš„åµŒå…¥
        cluster_centers: [K, feature_dim] - ç°‡ä¸­å¿ƒ
        è¿”å›: [M] - æ–°çš„æ ‡ç­¾åˆ†é… (-1è¡¨ç¤ºå™ªå£°)
        """
        if hard_embeddings.shape[0] == 0:
            return torch.tensor([], dtype=torch.long, device=hard_embeddings.device)
            
        # è®¡ç®—åˆ°æ‰€æœ‰ç°‡ä¸­å¿ƒçš„è·ç¦»
        distances = torch.cdist(hard_embeddings, cluster_centers)  # [M, K]
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç°‡
        min_distances, nearest_clusters = torch.min(distances, dim=1)  # [M]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°ï¼ˆè·ç¦»æ‰€æœ‰ç°‡éƒ½å¤ªè¿œï¼‰
        new_labels = nearest_clusters.clone()
        noise_mask = min_distances > self.distance_threshold
        new_labels[noise_mask] = -1  # æ ‡è®°ä¸ºå™ªå£°
        
        return new_labels, min_distances
    
    def forward(self, embeddings, evidence_strengths, current_labels, num_clusters):
        """
        æ‰§è¡Œå®Œæ•´çš„ä¿®æ­£æµç¨‹
        
        å‚æ•°:
        - embeddings: [N, feature_dim] - readsçš„åµŒå…¥è¡¨ç¤º
        - evidence_strengths: [N, L, 1] - è¯æ®å¼ºåº¦
        - current_labels: [N] - å½“å‰æ ‡ç­¾
        - num_clusters: int - ç°‡æ•°é‡
        
        è¿”å›:
        - new_labels: [N] - ä¿®æ­£åçš„æ ‡ç­¾
        - refinement_stats: dict - ä¿®æ­£ç»Ÿè®¡ä¿¡æ¯
        """
        N = embeddings.shape[0]
        device = embeddings.device
        
        # 1. è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        confidence_scores = self.calculate_confidence_score(evidence_strengths)
        
        # 2. é˜ˆå€¼åˆ¤æ–­ - è¯†åˆ«å›°éš¾æ ·æœ¬
        high_confidence_mask = confidence_scores > self.confidence_threshold
        hard_sample_mask = ~high_confidence_mask
        
        # 3. ä¿ç•™é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„æ ‡ç­¾
        new_labels = current_labels.clone()
        
        # 4. å¤„ç†å›°éš¾æ ·æœ¬
        if hard_sample_mask.sum() > 0:
            # è®¡ç®—å½“å‰ç°‡ä¸­å¿ƒï¼ˆåŸºäºé«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼‰
            high_conf_embeddings = embeddings[high_confidence_mask]
            high_conf_labels = current_labels[high_confidence_mask]
            
            if high_conf_embeddings.shape[0] > 0:
                cluster_centers = self.compute_cluster_centers(
                    high_conf_embeddings, high_conf_labels, num_clusters
                )
                
                # é‡åˆ†é…å›°éš¾æ ·æœ¬
                hard_embeddings = embeddings[hard_sample_mask]
                reassigned_labels, distances = self.reassign_hard_samples(
                    hard_embeddings, cluster_centers
                )
                
                # æ›´æ–°å›°éš¾æ ·æœ¬çš„æ ‡ç­¾
                new_labels[hard_sample_mask] = reassigned_labels
        
        # 5. ç»Ÿè®¡ä¿®æ­£ä¿¡æ¯
        refinement_stats = {
            'total_samples': N,
            'high_confidence_count': high_confidence_mask.sum().item(),
            'hard_samples_count': hard_sample_mask.sum().item(),
            'noise_samples_count': (new_labels == -1).sum().item(),
            'label_changes': (new_labels != current_labels).sum().item(),
            'refinement_ratio': (new_labels != current_labels).float().mean().item(),
            'avg_confidence': confidence_scores.mean().item(),
            'min_confidence': confidence_scores.min().item(),
            'max_confidence': confidence_scores.max().item()
        }
        
        return new_labels, refinement_stats

# ==========================================
# æ‰©å±•çš„è®­ç»ƒå¾ªç¯ - åŒ…å«ä¿®æ­£é˜¶æ®µ
# ==========================================

class RefinementTrainer:
    """åŒ…å«ä¿®æ­£é˜¶æ®µçš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, criterion, optimizer, refinement_module, 
                 convergence_threshold=0.01, max_epochs=10):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.refinement = refinement_module
        self.convergence_threshold = convergence_threshold
        self.max_epochs = max_epochs
        
    def train_epoch_with_refinement(self, dataloader, device, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒ…å«ä¿®æ­£é˜¶æ®µ"""
        
        self.model.train()
        epoch_losses = {'total_loss': 0, 'reconstruction_loss': 0, 'contrastive_loss': 0, 'kl_loss': 0}
        all_refinement_stats = []
        step_count = 0
        
        print(f"\nğŸ”„ Epoch {epoch+1} - å¼€å§‹è®­ç»ƒ+ä¿®æ­£é˜¶æ®µ...")
        
        for i, (reads, ref) in enumerate(dataloader):
            reads = reads.to(device)  # [1, N, 150, 4]
            ref = ref.squeeze(0).to(device)  # [150, 4]
            N = reads.shape[1]
            
            # === æ­¥éª¤1: æ­£å¸¸è®­ç»ƒ ===
            self.optimizer.zero_grad()
            
            # Forward pass
            evidence_batch, contrastive_features = self.model(reads)
            
            # è¯æ®èåˆ
            evidence_single_batch = evidence_batch.squeeze(0)  # [N, 150, 4]
            fused_evidence, strengths = self.model.evidence_fusion(evidence_single_batch)
            
            # è®¡ç®—æŸå¤±
            contrastive_features_flat = contrastive_features.squeeze(0)  # [N, feature_dim]
            losses = self.criterion(fused_evidence, ref, contrastive_features_flat)
            
            # åå‘ä¼ æ’­
            losses['total_loss'].backward()
            self.optimizer.step()
            
            # === æ­¥éª¤2: å›°éš¾æ ·æœ¬ä¿®æ­£ ===
            with torch.no_grad():
                # å‡è®¾åˆå§‹æ ‡ç­¾éƒ½æ˜¯0ï¼ˆåŒä¸€ä¸ªç°‡ï¼‰
                current_labels = torch.zeros(N, dtype=torch.long, device=device)
                
                # æ‰§è¡Œä¿®æ­£
                new_labels, refinement_stats = self.refinement(
                    embeddings=contrastive_features_flat,
                    evidence_strengths=strengths.unsqueeze(-1),  # [N, L] -> [N, L, 1]
                    current_labels=current_labels,
                    num_clusters=1  # ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªbatchæ˜¯ä¸€ä¸ªç°‡
                )
                
                all_refinement_stats.append(refinement_stats)
            
            # è®°å½•æŸå¤±
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            step_count += 1
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 5 == 0:
                print(f"  ğŸ“Š Step {i+1:3d} | Loss: {losses['total_loss'].item():.4f} | "
                      f"ä¿®æ­£ç‡: {refinement_stats['refinement_ratio']:.3f} | "
                      f"ç½®ä¿¡åº¦: {refinement_stats['avg_confidence']:.3f} | "
                      f"å™ªå£°: {refinement_stats['noise_samples_count']}")
        
        # è®¡ç®—epochç»Ÿè®¡
        avg_losses = {key: val / max(1, step_count) for key, val in epoch_losses.items()}
        
        # æ±‡æ€»ä¿®æ­£ç»Ÿè®¡
        if all_refinement_stats:
            avg_refinement_ratio = np.mean([s['refinement_ratio'] for s in all_refinement_stats])
            avg_confidence = np.mean([s['avg_confidence'] for s in all_refinement_stats])
            total_noise = sum([s['noise_samples_count'] for s in all_refinement_stats])
        else:
            avg_refinement_ratio = 0.0
            avg_confidence = 0.0
            total_noise = 0
            
        return avg_losses, {
            'refinement_ratio': avg_refinement_ratio,
            'avg_confidence': avg_confidence,
            'total_noise_samples': total_noise
        }
    
    def train_with_refinement(self, dataloader, device):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æ”¶æ•›åˆ¤æ–­"""
        
        print("ğŸš€ å¼€å§‹è¯æ®é©±åŠ¨çš„ä¿®æ­£è®­ç»ƒ...")
        print(f"ğŸ“‹ é…ç½®: æ”¶æ•›é˜ˆå€¼={self.convergence_threshold}, æœ€å¤§è½®æ•°={self.max_epochs}")
        
        training_history = {
            'losses': [],
            'refinement_ratios': [],
            'confidences': [],
            'noise_counts': []
        }
        
        for epoch in range(self.max_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            avg_losses, refinement_stats = self.train_epoch_with_refinement(
                dataloader, device, epoch
            )
            
            # è®°å½•å†å²
            training_history['losses'].append(avg_losses)
            training_history['refinement_ratios'].append(refinement_stats['refinement_ratio'])
            training_history['confidences'].append(refinement_stats['avg_confidence'])
            training_history['noise_counts'].append(refinement_stats['total_noise_samples'])
            
            # æ‰“å°epochæ€»ç»“
            print(f"\nğŸ“ˆ Epoch {epoch+1} å®Œæˆ:")
            print(f"   æ€»æŸå¤±: {avg_losses['total_loss']:.6f}")
            print(f"   ä¿®æ­£æ¯”ä¾‹: {refinement_stats['refinement_ratio']:.4f} ({refinement_stats['refinement_ratio']*100:.2f}%)")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {refinement_stats['avg_confidence']:.4f}")
            print(f"   å™ªå£°æ ·æœ¬æ•°: {refinement_stats['total_noise_samples']}")
            
            # æ”¶æ•›åˆ¤æ–­
            if refinement_stats['refinement_ratio'] < self.convergence_threshold:
                print(f"\nâœ… æ”¶æ•›è¾¾æˆï¼ä¿®æ­£æ¯”ä¾‹ {refinement_stats['refinement_ratio']:.4f} < é˜ˆå€¼ {self.convergence_threshold}")
                print(f"ğŸ¯ è®­ç»ƒåœ¨ç¬¬ {epoch+1} è½®æ”¶æ•›")
                break
            else:
                print(f"   ğŸ”„ ç»§ç»­è®­ç»ƒ (ä¿®æ­£æ¯”ä¾‹ {refinement_stats['refinement_ratio']:.4f} >= {self.convergence_threshold})")
            
            print("-" * 70)
        
        return training_history

# ==========================================
# æ›´æ–°åçš„ä¸»è®­ç»ƒå‡½æ•° - åŒ…å«ä¿®æ­£é˜¶æ®µ
# ==========================================

def train_with_refinement():
    """åŒ…å«ä¿®æ­£é˜¶æ®µçš„å®Œæ•´è®­ç»ƒå‡½æ•°"""
    
    DATA_DIR = "Dataset/CloverExp/train"
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # è®­ç»ƒå‚æ•°
    input_dim = 4
    hidden_dim = 64
    seq_len = 150
    lr = 1e-3
    
    # æŸå¤±æƒé‡
    alpha = 1.0
    beta = 0.01  # é™ä½å¯¹æ¯”å­¦ä¹ æƒé‡
    gamma = 0.01
    
    # ä¿®æ­£å‚æ•°
    confidence_threshold = 0.3  # ç½®ä¿¡åº¦é˜ˆå€¼
    distance_threshold = 1.5    # è·ç¦»é˜ˆå€¼
    convergence_threshold = 0.01  # æ”¶æ•›é˜ˆå€¼ (1%)
    max_epochs = 8
    
    try:
        # æ£€æŸ¥æ•°æ®ç›®å½•
        if not os.path.exists(DATA_DIR):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
            return None
            
        # åŠ è½½æ•°æ®
        dataset = CloverClusterDataset(DATA_DIR)
        if len(dataset) == 0:
            print(f"âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
            return None
            
        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = SimplifiedFedDNA(input_dim, hidden_dim, seq_len).to(DEVICE)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = ComprehensiveLoss(alpha=alpha, beta=beta, gamma=gamma)
        
        # åˆå§‹åŒ–ä¿®æ­£æ¨¡å—
        refinement_module = EvidenceRefinement(
            confidence_threshold=confidence_threshold,
            distance_threshold=distance_threshold
        ).to(DEVICE)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RefinementTrainer(
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            refinement_module=refinement_module,
            convergence_threshold=convergence_threshold,
            max_epochs=max_epochs
        )
        
        print(f"ğŸ”§ æ¨¡å‹é…ç½®:")
        print(f"   è®¾å¤‡: {DEVICE}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªç°‡")
        print(f"   æŸå¤±æƒé‡: é‡æ„={alpha}, å¯¹æ¯”å­¦ä¹ ={beta}, KLæ•£åº¦={gamma}")
        print(f"   ä¿®æ­£å‚æ•°: ç½®ä¿¡åº¦é˜ˆå€¼={confidence_threshold}, è·ç¦»é˜ˆå€¼={distance_threshold}")
        print(f"   æ”¶æ•›æ¡ä»¶: ä¿®æ­£æ¯”ä¾‹ < {convergence_threshold*100}%")
        
        # å¼€å§‹è®­ç»ƒ
        training_history = trainer.train_with_refinement(dataloader, DEVICE)
        
        # ä¿å­˜ç»“æœ
        save_dict = {
            'model_state_dict': model.state_dict(),
            'refinement_state_dict': refinement_module.state_dict(),
            'training_history': training_history,
            'config': {
                'model_config': {
                    'input_dim': input_dim,
                    'hidden_dim': hidden_dim,
                    'seq_len': seq_len,
                },
                'training_config': {
                    'learning_rate': lr,
                    'loss_weights': {'alpha': alpha, 'beta': beta, 'gamma': gamma},
                    'max_epochs': max_epochs
                },
                'refinement_config': {
                    'confidence_threshold': confidence_threshold,
                    'distance_threshold': distance_threshold,
                    'convergence_threshold': convergence_threshold
                }
            }
        }
        
        torch.save(save_dict, "refined_model.pth")
        print(f"\nğŸ’¾ å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°: refined_model.pth")
        
        # è®­ç»ƒæ€»ç»“
        final_refinement_ratio = training_history['refinement_ratios'][-1]
        final_confidence = training_history['confidences'][-1]
        
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆæ€»ç»“:")
        print(f"   æœ€ç»ˆä¿®æ­£æ¯”ä¾‹: {final_refinement_ratio:.4f} ({final_refinement_ratio*100:.2f}%)")
        print(f"   æœ€ç»ˆå¹³å‡ç½®ä¿¡åº¦: {final_confidence:.4f}")
        print(f"   æ€»è®­ç»ƒè½®æ•°: {len(training_history['losses'])}")
        
        if final_refinement_ratio < convergence_threshold:
            print(f"   âœ… æˆåŠŸæ”¶æ•›ï¼")
        else:
            print(f"   âš ï¸  æœªå®Œå…¨æ”¶æ•›ï¼Œå¯è€ƒè™‘å¢åŠ è®­ç»ƒè½®æ•°")
            
        return training_history
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

# ==========================================
# åŸå§‹è®­ç»ƒå‡½æ•° (ä¿ç•™ä½œä¸ºå¤‡ç”¨)
# ==========================================
def train():
    """åŸå§‹çš„è®­ç»ƒå‡½æ•° - ä»…åŒ…å«Step1"""
    DATA_DIR = "Dataset/CloverExp/train"
    EPOCHS = 5
    LR = 1e-3
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # å‚æ•°é…ç½®
    input_dim = 4
    hidden_dim = 64
    seq_len = 150
    
    try:
        if not os.path.exists(DATA_DIR):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
            return

        dataset = CloverClusterDataset(DATA_DIR)
        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
        
        print(f"ğŸ”§ åˆå§‹åŒ–ç®€åŒ–ç‰ˆFedDNAæ¨¡å‹...")
        model = SimplifiedFedDNA(input_dim, hidden_dim, seq_len).to(DEVICE)
        
        print("ğŸ‰ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    optimizer = optim.Adam(model.parameters(), lr=LR)
    
    # ä½¿ç”¨ç»¼åˆæŸå¤±å‡½æ•°
    criterion = ComprehensiveLoss(alpha=alpha, beta=beta, gamma=gamma)

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ | Device: {DEVICE} | Epochs: {EPOCHS}")
    print(f"ğŸ“Š æŸå¤±å‡½æ•°æƒé‡: é‡æ„={alpha}, å¯¹æ¯”å­¦ä¹ ={beta}, KLæ•£åº¦={gamma}")
    
    # è®°å½•è®­ç»ƒå†å²
    train_history = {
        'total_loss': [],
        'reconstruction_loss': [],
        'contrastive_loss': [],
        'kl_loss': []
    }
    
    model.train()
    for epoch in range(EPOCHS):
        epoch_losses = {'total_loss': 0, 'reconstruction_loss': 0, 'contrastive_loss': 0, 'kl_loss': 0}
        step_count = 0
        
        for i, (reads, ref) in enumerate(dataloader):
            reads = reads.to(DEVICE)  # [1, N, 150, 4]
            ref = ref.squeeze(0).to(DEVICE)  # [150, 4]
            
            optimizer.zero_grad()
            
            # Forward pass
            evidence_batch, contrastive_features = model(reads)
            
            # è¯æ®èåˆ
            evidence_single_batch = evidence_batch.squeeze(0)  # [N, 150, 4]
            fused_evidence, strengths = model.evidence_fusion(evidence_single_batch)
            
            # è®¡ç®—ç»¼åˆæŸå¤±
            contrastive_features_flat = contrastive_features.squeeze(0)  # [N, feature_dim]
            losses = criterion(fused_evidence, ref, contrastive_features_flat)
            
            # åå‘ä¼ æ’­
            losses['total_loss'].backward()
            optimizer.step()
            
            # è®°å½•æŸå¤±
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            step_count += 1
            
            # æ‰“å°è¯¦ç»†æŸå¤±ä¿¡æ¯
            if (i + 1) % 10 == 0:
                print(f"  Step {i+1:3d} | Total: {losses['total_loss'].item():.6f} | "
                      f"Recon: {losses['reconstruction_loss'].item():.6f} | "
                      f"Contra: {losses['contrastive_loss'].item():.6f} | "
                      f"KL: {losses['kl_loss'].item():.6f}")
            
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            avg_loss = epoch_losses[key] / max(1, step_count)
            train_history[key].append(avg_loss)
            
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{EPOCHS} å®Œæˆ:")
        print(f"   Total Loss:    {train_history['total_loss'][-1]:.6f}")
        print(f"   Reconstruction: {train_history['reconstruction_loss'][-1]:.6f}")
        print(f"   Contrastive:   {train_history['contrastive_loss'][-1]:.6f}")
        print(f"   KL Divergence: {train_history['kl_loss'][-1]:.6f}")
        print("-" * 60)
        
    print("\nâœ… è®­ç»ƒå®Œæˆï¼ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒå†å²...")
    
    # ä¿å­˜å®Œæ•´çš„è®­ç»ƒç»“æœ
    save_dict = {
        'model_state_dict': model.state_dict(),
        'train_history': train_history,
        'config': {
            'input_dim': input_dim,
            'hidden_dim': hidden_dim,
            'seq_len': seq_len,
            'epochs': EPOCHS,
            'learning_rate': LR,
            'loss_weights': {
                'alpha': alpha,
                'beta': beta, 
                'gamma': gamma
            }
        }
    }
    
    torch.save(save_dict, "comprehensive_model.pth")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: comprehensive_model.pth")
    
    # æ‰“å°è®­ç»ƒæ€»ç»“
    print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   æœ€ç»ˆæ€»æŸå¤±: {train_history['total_loss'][-1]:.6f}")
    print(f"   æŸå¤±ä¸‹é™: {train_history['total_loss'][0]:.6f} â†’ {train_history['total_loss'][-1]:.6f}")
    print(f"   æ”¹å–„å¹…åº¦: {((train_history['total_loss'][0] - train_history['total_loss'][-1]) / train_history['total_loss'][0] * 100):.2f}%")
    
    return train_history

# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    print("ğŸ¯ é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. å®Œæ•´è®­ç»ƒ (Step1 + Step2 å›°éš¾æ ·æœ¬ä¿®æ­£)")
    print("2. åŸºç¡€è®­ç»ƒ (ä»…Step1)")
    
    # é»˜è®¤ä½¿ç”¨å®Œæ•´è®­ç»ƒ
    print("ğŸš€ å¯åŠ¨å®Œæ•´è®­ç»ƒæ¨¡å¼ (åŒ…å«å›°éš¾æ ·æœ¬ä¿®æ­£)...")
    train_history = train_with_refinement()
    
    # å¦‚æœéœ€è¦ä½¿ç”¨åŸºç¡€è®­ç»ƒï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # train_history = train()
