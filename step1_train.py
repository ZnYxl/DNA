#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DNA èšç±» Metadata ç”Ÿæˆè„šæœ¬ - å®Œæ•´Dirichlet Evidence Learningç‰ˆæœ¬
å°† Clover è¾“å‡ºè½¬æ¢ä¸º CSV å…ƒæ•°æ®æ–‡ä»¶
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch.nn.functional as F
from collections import defaultdict

# ==========================================
# å®šä¹‰æƒé‡å‚æ•°
# ==========================================
alpha = 1.0
beta = 0.01
gamma = 0.01

# ==========================================
# å¯¼å…¥åŸºç¡€ç»„ä»¶
# ==========================================
try:
    from models.conmamba import ConmambaBlock
    print("âœ… æˆåŠŸå¯¼å…¥ ConmambaBlock")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ==========================================
# ç®€åŒ–çš„æ ¸å¿ƒç»„ä»¶
# ==========================================

class SimpleEncoder(nn.Module):
    """ç®€åŒ–çš„ç¼–ç å™¨ - åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, input_dim=4, hidden_dim=64, seq_len=150):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # ç®€å•çš„ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # ConMambaå—ç”¨äºåºåˆ—å»ºæ¨¡
        self.conmamba = ConmambaBlock(dim=hidden_dim)
        
    def forward(self, x):
        # x: [B*N, L, 4] 
        x = self.feature_extractor(x)  # [B*N, L, hidden_dim]
        x = self.conmamba(x)           # [B*N, L, hidden_dim]
        return x

class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—"""
    def __init__(self, hidden_dim=64, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
    
    def forward(self, embeddings):
        # embeddings: [B*N, L, hidden_dim]
        # å–å¹³å‡æ± åŒ–ä½œä¸ºåºåˆ—è¡¨ç¤º
        seq_repr = torch.mean(embeddings, dim=1)  # [B*N, hidden_dim]
        projected = self.projection(seq_repr)     # [B*N, hidden_dim//2]
        return F.normalize(projected, dim=-1)

class DirichletEvidenceDecoder(nn.Module):
    """ğŸ”¥ ä¸¥æ ¼çš„Dirichletè¯æ®è§£ç å™¨"""
    def __init__(self, hidden_dim=64, output_dim=4):
        super().__init__()
        self.output_dim = output_dim
        
        # è¯æ®ç½‘ç»œ - è¾“å‡ºéè´Ÿevidence
        self.evidence_net = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softplus()  # ç¡®ä¿evidence > 0
        )
        
    def forward(self, x):
        """
        x: [B*N, L, hidden_dim]
        è¿”å›: Dirichletå‚æ•°å’Œç›¸å…³ç»Ÿè®¡é‡
        """
        # 1ï¸âƒ£ è·å–evidence
        evidence = self.evidence_net(x)  # [B*N, L, K]
        
        # 2ï¸âƒ£ è¯æ® â†’ Dirichlet å‚æ•°
        alpha = evidence + 1.0  # [B*N, L, K]
        
        # 3ï¸âƒ£ é¢„æµ‹å‡å€¼ï¼ˆä¸æ˜¯softmaxï¼ï¼‰
        alpha_sum = torch.sum(alpha, dim=-1, keepdim=True)  # [B*N, L, 1]
        predictions = alpha / alpha_sum  # [B*N, L, K]
        
        # 4ï¸âƒ£ ä¸ç¡®å®šæ€§è®¡ç®—
        K = self.output_dim
        uncertainty = K / alpha_sum.squeeze(-1)  # [B*N, L]
        
        # 5ï¸âƒ£ è¯æ®å¼ºåº¦ï¼ˆç”¨äºèåˆæƒé‡ï¼‰
        evidence_strength = torch.sum(evidence, dim=-1)  # [B*N, L]
        
        return {
            'evidence': evidence,           # [B*N, L, K] - åŸå§‹evidence
            'alpha': alpha,                # [B*N, L, K] - Dirichletå‚æ•°
            'predictions': predictions,     # [B*N, L, K] - é¢„æµ‹æ¦‚ç‡
            'uncertainty': uncertainty,     # [B*N, L] - ä¸ç¡®å®šæ€§
            'strength': evidence_strength   # [B*N, L] - è¯æ®å¼ºåº¦
        }

class DirichletEvidenceFusion(nn.Module):
    """ğŸ”¥ åŸºäºDirichletçš„è¯æ®èåˆæ¨¡å—"""
    def __init__(self):
        super().__init__()
        
    def forward(self, dirichlet_outputs):
        """
        dirichlet_outputs: dict with keys ['evidence', 'alpha', 'predictions', 'uncertainty', 'strength']
        æ¯ä¸ªå€¼çš„shape: [N, L, K] æˆ– [N, L]
        """
        evidence = dirichlet_outputs['evidence']      # [N, L, K]
        uncertainty = dirichlet_outputs['uncertainty'] # [N, L]
        
        # ğŸ”¥ ä½¿ç”¨ä¸ç¡®å®šæ€§çš„å€’æ•°ä½œä¸ºèåˆæƒé‡ï¼ˆä¸ç¡®å®šæ€§è¶Šä½ï¼Œæƒé‡è¶Šé«˜ï¼‰
        fusion_weights = 1.0 / (uncertainty + 1e-8)  # [N, L]
        fusion_weights = fusion_weights.unsqueeze(-1)  # [N, L, 1]
        
        # åŠ æƒèåˆevidence
        weighted_evidence = evidence * fusion_weights  # [N, L, K]
        fused_evidence = torch.sum(weighted_evidence, dim=0)  # [L, K]
        total_weights = torch.sum(fusion_weights, dim=0)      # [L, 1]
        
        # å½’ä¸€åŒ–
        fused_evidence = fused_evidence / (total_weights + 1e-8)  # [L, K]
        
        # é‡æ–°è®¡ç®—èåˆåçš„Dirichletå‚æ•°
        fused_alpha = fused_evidence + 1.0  # [L, K]
        fused_alpha_sum = torch.sum(fused_alpha, dim=-1, keepdim=True)  # [L, 1]
        fused_predictions = fused_alpha / fused_alpha_sum  # [L, K]
        fused_uncertainty = evidence.shape[-1] / fused_alpha_sum.squeeze(-1)  # [L]
        
        return {
            'fused_evidence': fused_evidence,
            'fused_alpha': fused_alpha,
            'fused_predictions': fused_predictions,
            'fused_uncertainty': fused_uncertainty,
            'fusion_weights': fusion_weights.squeeze(-1)  # [N, L]
        }

class SimplifiedFedDNA(nn.Module):
    """ç®€åŒ–ç‰ˆFedDNA - ä½¿ç”¨å®Œæ•´Dirichletä»£æ•°"""
    def __init__(self, input_dim=4, hidden_dim=64, seq_len=150):
        super().__init__()
        self.encoder = SimpleEncoder(input_dim, hidden_dim, seq_len)
        self.contrastive = ContrastiveLearning(hidden_dim)
        self.evidence_decoder = DirichletEvidenceDecoder(hidden_dim, input_dim)
        self.evidence_fusion = DirichletEvidenceFusion()
        
    def forward(self, reads_batch):
        """
        reads_batch: [B, N, L, 4] - Bä¸ªbatchï¼Œæ¯ä¸ªbatchæœ‰Næ¡reads
        """
        B, N, L, D = reads_batch.shape
        
        # é‡å¡‘ä¸º [B*N, L, D]
        reads_flat = reads_batch.view(B * N, L, D)
        
        # 1. ç¼–ç 
        embeddings = self.encoder(reads_flat)  # [B*N, L, hidden_dim]
        
        # 2. å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        contrastive_features = self.contrastive(embeddings)  # [B*N, hidden_dim//2]
        
        # 3. Dirichletè¯æ®è§£ç 
        dirichlet_outputs = self.evidence_decoder(embeddings)
        
        # 4. é‡å¡‘å›batchå½¢å¼
        for key in dirichlet_outputs:
            if dirichlet_outputs[key].dim() == 3:  # [B*N, L, K]
                dirichlet_outputs[key] = dirichlet_outputs[key].view(B, N, L, -1)
            elif dirichlet_outputs[key].dim() == 2:  # [B*N, L]
                dirichlet_outputs[key] = dirichlet_outputs[key].view(B, N, L)
        
        contrastive_features = contrastive_features.view(B, N, -1)  # [B, N, hidden_dim//2]
        
        return dirichlet_outputs, contrastive_features

# ==========================================
# ğŸ”¥ å®Œæ•´DirichletæŸå¤±å‡½æ•°
# ==========================================

class DirichletComprehensiveLoss(nn.Module):
    """ğŸ”¥ åŸºäºDirichletä»£æ•°çš„ç»¼åˆæŸå¤±å‡½æ•°"""
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01, temperature=0.1):
        super().__init__()
        self.alpha = alpha      # Dirichlet Expected MSEæƒé‡
        self.beta = beta        # å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡  
        self.gamma = gamma      # Dirichlet KLæ•£åº¦æƒé‡
        self.temperature = temperature
        
    def dirichlet_expected_mse(self, fused_predictions, target):
        """
        ğŸ”¥ Dirichlet Expected MSE Loss
        fused_predictions: [L, K] - Dirichleté¢„æµ‹å‡å€¼
        target: [L, K] - ç›®æ ‡one-hot
        """
        mse = torch.mean((fused_predictions - target) ** 2)
        return mse
    
    def dirichlet_kl_divergence(self, alpha, target_alpha=None):
        """
        ğŸ”¥ Dirichlet KLæ•£åº¦çš„è§£æå¼
        alpha: [L, K] - Dirichletå‚æ•°
        target_alpha: [L, K] - ç›®æ ‡Dirichletå‚æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒï¼‰
        """
        if target_alpha is None:
            # ä¸å‡åŒ€Dirichletåˆ†å¸ƒçš„KLæ•£åº¦
            K = alpha.shape[-1]
            target_alpha = torch.ones_like(alpha)  # å‡åŒ€åˆ†å¸ƒå‚æ•°éƒ½æ˜¯1
        
        # Dirichlet KLæ•£åº¦çš„è§£æå…¬å¼
        alpha_sum = torch.sum(alpha, dim=-1, keepdim=True)  # [L, 1]
        target_alpha_sum = torch.sum(target_alpha, dim=-1, keepdim=True)  # [L, 1]
        
        # KL(Dir(Î±)||Dir(Î±â‚€)) = log(B(Î±â‚€)/B(Î±)) + Î£áµ¢(Î±áµ¢-Î±â‚€áµ¢)[Ïˆ(Î±áµ¢)-Ïˆ(Î£â±¼Î±â±¼)]
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å¯¹æ•°Gammaå‡½æ•°
        kl_div = (
            torch.lgamma(alpha_sum) - torch.lgamma(target_alpha_sum) +
            torch.sum(torch.lgamma(target_alpha) - torch.lgamma(alpha), dim=-1, keepdim=True) +
            torch.sum((alpha - target_alpha) * (torch.digamma(alpha) - torch.digamma(alpha_sum)), dim=-1, keepdim=True)
        )
        
        return torch.mean(kl_div)
    
    def contrastive_loss(self, features, cluster_labels=None):
        """å¯¹æ¯”å­¦ä¹ æŸå¤± - ä¿æŒä¸å˜"""
        if features.shape[0] <= 1:
            return torch.tensor(0.0, device=features.device)
            
        # ç®€åŒ–ç‰ˆï¼šå‡è®¾åŒä¸€ä¸ªbatchå†…çš„readså±äºåŒä¸€ç°‡
        features_norm = F.normalize(features, dim=1)
        similarity_matrix = torch.matmul(features_norm, features_norm.T) / self.temperature
        
        batch_size = features.shape[0]
        mask = torch.eye(batch_size, device=features.device).bool()
        
        exp_sim = torch.exp(similarity_matrix)
        exp_sim = exp_sim.masked_fill(mask, 0)
        
        pos_sim = torch.sum(exp_sim, dim=1) / (batch_size - 1)
        neg_sim = torch.sum(exp_sim, dim=1)
        
        loss = -torch.log(pos_sim / (neg_sim + 1e-8))
        return torch.mean(loss)
    
    def forward(self, fusion_results, target, contrastive_features):
        """
        è®¡ç®—æ€»æŸå¤±
        fusion_results: dict - èåˆç»“æœ
        target: [L, K] - ç›®æ ‡åºåˆ—
        contrastive_features: [N, feature_dim] - å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        """
        fused_predictions = fusion_results['fused_predictions']  # [L, K]
        fused_alpha = fusion_results['fused_alpha']              # [L, K]
        
        # 1ï¸âƒ£ Dirichlet Expected MSE
        expected_mse = self.dirichlet_expected_mse(fused_predictions, target)
        
        # 2ï¸âƒ£ å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.contrastive_loss(contrastive_features)
        
        # 3ï¸âƒ£ Dirichlet KLæ•£åº¦
        dirichlet_kl = self.dirichlet_kl_divergence(fused_alpha)
        
        # 4ï¸âƒ£ æ€»æŸå¤±
        total_loss = (self.alpha * expected_mse + 
                     self.beta * contrastive_loss + 
                     self.gamma * dirichlet_kl)
        
        return {
            'total_loss': total_loss,
            'expected_mse': expected_mse,
            'contrastive_loss': contrastive_loss,
            'dirichlet_kl': dirichlet_kl
        }

# ==========================================
# ğŸ”¥ åŸºäºDirichletä¸ç¡®å®šæ€§çš„ä¿®æ­£æ¨¡å—
# ==========================================

class DirichletEvidenceRefinement(nn.Module):
    """ğŸ”¥ æ¸è¿›å¼ä¿®æ­£ï¼šé¿å…è¿‡åº¦ä¿®æ­£"""
    
    def __init__(self, 
                 uncertainty_threshold_start=0.95,    # åˆå§‹ä¸¥æ ¼é˜ˆå€¼
                 uncertainty_threshold_end=0.85,      # æœ€ç»ˆé˜ˆå€¼
                 confidence_threshold_start=0.9,      # åˆå§‹ä¸¥æ ¼é˜ˆå€¼
                 confidence_threshold_end=0.15,        # æœ€ç»ˆé˜ˆå€¼
                 distance_threshold=0.3,
                 max_refinement_ratio=0.3):           # æœ€å¤§ä¿®æ­£æ¯”ä¾‹é™åˆ¶
        super().__init__()
        self.uncertainty_threshold_start = uncertainty_threshold_start
        self.uncertainty_threshold_end = uncertainty_threshold_end
        self.confidence_threshold_start = confidence_threshold_start
        self.confidence_threshold_end = confidence_threshold_end
        self.distance_threshold = distance_threshold
        self.max_refinement_ratio = max_refinement_ratio
        
    def get_adaptive_thresholds(self, epoch, max_epochs):
        """ğŸ”¥ è‡ªé€‚åº”é˜ˆå€¼ï¼šéšè®­ç»ƒè¿›ç¨‹è°ƒæ•´"""
        progress = min(epoch / max(max_epochs - 1, 1), 1.0)
        
        uncertainty_threshold = (self.uncertainty_threshold_start + 
                               progress * (self.uncertainty_threshold_end - self.uncertainty_threshold_start))
        
        confidence_threshold = (self.confidence_threshold_start + 
                              progress * (self.confidence_threshold_end - self.confidence_threshold_start))
        
        return uncertainty_threshold, confidence_threshold
    
    def calculate_dirichlet_confidence(self, uncertainty):
        """è®¡ç®—ç½®ä¿¡åº¦ - ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹æ³•"""
        avg_uncertainty = torch.mean(uncertainty, dim=1)  # [N]
        
        # ä½¿ç”¨æŒ‡æ•°å˜æ¢ï¼Œæ›´æ•æ„Ÿ
        confidence_scores = torch.exp(-2.0 * avg_uncertainty)  # æŒ‡æ•°è¡°å‡
        
        return confidence_scores, avg_uncertainty
    
    def identify_hard_samples_conservative(self, uncertainty, confidence_scores, 
                                         uncertainty_threshold, confidence_threshold):
        """ğŸ”¥ ä¿å®ˆçš„å›°éš¾æ ·æœ¬è¯†åˆ«"""
        N = uncertainty.shape[0]
        
        avg_uncertainty = torch.mean(uncertainty, dim=1)  # [N]
        
        # æ›´ä¸¥æ ¼çš„æ ‡å‡†
        high_uncertainty_mask = avg_uncertainty > uncertainty_threshold
        low_confidence_mask = confidence_scores < confidence_threshold
        
        # ä¸ç¡®å®šæ€§æ–¹å·® - åªé€‰æ‹©æœ€ä¸ç¨³å®šçš„10%
        uncertainty_var = torch.var(uncertainty, dim=1)  # [N]
        uncertainty_var_threshold = torch.quantile(uncertainty_var, 0.9)  # å‰10%
        high_variance_mask = uncertainty_var > uncertainty_var_threshold
        
        # ğŸ”¥ ä¸¥æ ¼æ ‡å‡†ï¼šå¿…é¡»åŒæ—¶æ»¡è¶³é«˜ä¸ç¡®å®šæ€§ANDä½ç½®ä¿¡åº¦
        hard_sample_mask = high_uncertainty_mask & low_confidence_mask
        
        # å¦‚æœè¿˜æ˜¯å¤ªå¤šï¼Œè¿›ä¸€æ­¥é™åˆ¶
        if hard_sample_mask.sum() > N * self.max_refinement_ratio:
            # é€‰æ‹©æœ€ä¸ï¿½ï¿½å®šçš„æ ·æœ¬
            num_hard = int(N * self.max_refinement_ratio)
            _, worst_indices = torch.topk(avg_uncertainty, num_hard)
            hard_sample_mask = torch.zeros(N, dtype=torch.bool, device=uncertainty.device)
            hard_sample_mask[worst_indices] = True
        
        return hard_sample_mask, {
            'high_uncertainty_count': high_uncertainty_mask.sum().item(),
            'low_confidence_count': low_confidence_mask.sum().item(),
            'high_variance_count': high_variance_mask.sum().item(),
            'avg_uncertainty': avg_uncertainty.mean().item(),
            'uncertainty_threshold_used': uncertainty_threshold,
            'confidence_threshold_used': confidence_threshold,
            'max_allowed_hard_samples': int(N * self.max_refinement_ratio)
        }
    
    def create_multi_cluster_assignment(self, embeddings, uncertainty, num_base_clusters=3):
        """åˆ›å»ºå¤šç°‡åˆ†é…"""
        N = embeddings.shape[0]
        device = embeddings.device
        
        if N <= num_base_clusters:
            return torch.arange(N, device=device)
        
        # åŸºäºä¸ç¡®å®šæ€§çš„ç®€å•åˆ†ç»„
        avg_uncertainty = torch.mean(uncertainty, dim=1)  # [N]
        
        # æŒ‰ä¸ç¡®å®šæ€§åˆ†æˆ3ç»„ï¼šä½ã€ä¸­ã€é«˜
        uncertainty_sorted, indices = torch.sort(avg_uncertainty)
        group_size = N // num_base_clusters
        
        initial_labels = torch.zeros(N, device=device, dtype=torch.long)
        for i in range(num_base_clusters):
            start_idx = i * group_size
            end_idx = (i + 1) * group_size if i < num_base_clusters - 1 else N
            group_indices = indices[start_idx:end_idx]
            initial_labels[group_indices] = i
        
        return initial_labels
    
    def compute_cluster_centers(self, embeddings, labels, num_clusters):
        """è®¡ç®—ç°‡ä¸­å¿ƒ"""
        device = embeddings.device
        feature_dim = embeddings.shape[1]
        centers = torch.zeros(num_clusters, feature_dim, device=device)
        
        for k in range(num_clusters):
            mask = (labels == k)
            if mask.sum() > 0:
                centers[k] = torch.mean(embeddings[mask], dim=0)
            else:
                centers[k] = torch.randn(feature_dim, device=device) * 0.1
                
        return centers
        
    def reassign_hard_samples_conservative(
        self,
        hard_embeddings,
        cluster_centers,
        hard_indices,
        current_labels
    ):
        """
        ğŸ”¥ çœŸÂ·ä¿å®ˆé‡åˆ†é…ç­–ç•¥ï¼ˆä¸ä¼šå¡æ­»ï¼Œä¹Ÿä¸ä¼šä¹±æ”¹ï¼‰
        """
        device = hard_embeddings.device
        M = hard_embeddings.shape[0]

        if M == 0:
            return (
                torch.empty(0, dtype=torch.long, device=device),
                torch.empty(0, device=device)
            )

        # 1ï¸âƒ£ è®¡ç®— hard â†’ æ‰€æœ‰ç°‡çš„è·ç¦»
        distances = torch.cdist(hard_embeddings, cluster_centers)  # [M, K]

        # å½“å‰æ ‡ç­¾
        current_hard_labels = current_labels[hard_indices]

        # å½“å‰ç°‡è·ç¦»
        current_distances = distances[
            torch.arange(M, device=device),
            current_hard_labels
        ]

        # æœ€è¿‘ç°‡
        min_distances, nearest_clusters = torch.min(distances, dim=1)

        # åˆå§‹åŒ–ï¼šé»˜è®¤ä¸æ”¹
        new_labels = current_hard_labels.clone()

        # ------------------------------------------------------------------
        # 2ï¸âƒ£ æ”¹æ ‡ç­¾çš„â€œæœ€ä½è§¦å‘æ¡ä»¶â€
        # ------------------------------------------------------------------

        # (1) æœ€è¿‘ç°‡ â‰  å½“å‰ç°‡
        different_cluster = nearest_clusters != current_hard_labels

        # (2) è·ç¦»æ”¹å–„æ¯”ä¾‹ï¼ˆæ¯”ä½ åŸæ¥å®½æ¾ä¸€ç‚¹ï¼‰
        improvement_ratio = (current_distances - min_distances) / (current_distances + 1e-8)

        # âœ… åªè¦æœ‰â€œæ˜æ˜¾æ”¹å–„â€å³å¯ï¼ˆ10% è€Œä¸æ˜¯ 20%ï¼‰
        significant_improvement = improvement_ratio > 0.10

        # (3) å½“å‰ç°‡åœ¨â€œè·ç¦»æ’åºä¸­å¾ˆé åâ€ï¼ˆè€Œä¸æ˜¯ç¬¬ä¸€æˆ–ç¬¬äºŒï¼‰
        rank_in_clusters = torch.argsort(distances, dim=1).argsort(dim=1)
        badly_ranked = rank_in_clusters[
            torch.arange(M, device=device),
            current_hard_labels
        ] >= 2   # å½“å‰ç°‡æ’åœ¨ç¬¬ 3 åä»¥å

        # ğŸ”¥ æœ€ç»ˆæ”¹æ ‡ç­¾æ¡ä»¶ï¼ˆä¸‰è€…åŒæ—¶ï¼‰
        change_mask = (
            different_cluster
            & significant_improvement
            & badly_ranked
        )

        new_labels[change_mask] = nearest_clusters[change_mask]

        # ------------------------------------------------------------------
        # 3ï¸âƒ£ å™ªå£°åˆ¤å®šï¼ˆæç«¯æƒ…å†µæ‰è§¦å‘ï¼‰
        # ------------------------------------------------------------------

        # ä½¿ç”¨ batch å†…è‡ªé€‚åº”é˜ˆå€¼ï¼Œè€Œä¸æ˜¯å›ºå®š distance_threshold
        noise_threshold = torch.quantile(min_distances, 0.95)

        noise_mask = min_distances > noise_threshold

        # âš ï¸ å™ªå£°ä¸ä¼šè¦†ç›–å·²ç»æˆåŠŸä¿®æ­£çš„æ ·æœ¬
        noise_mask = noise_mask & (~change_mask)

        new_labels[noise_mask] = -1

        return new_labels, min_distances

    
    def forward(self, embeddings, dirichlet_uncertainty, current_labels, num_clusters, 
                epoch=0, max_epochs=10):
        """ğŸ”¥ æ¸è¿›å¼ä¿®æ­£ä¸»æµç¨‹"""
        N = embeddings.shape[0]
        device = embeddings.device
        
        # ğŸ”¥ è·å–è‡ªé€‚åº”é˜ˆå€¼
        uncertainty_threshold, confidence_threshold = self.get_adaptive_thresholds(epoch, max_epochs)
        
        # å¦‚æœcurrent_labelséƒ½ç›¸åŒï¼Œåˆ›å»ºå¤šç°‡åˆå§‹åˆ†é…
        if len(torch.unique(current_labels)) == 1:
            current_labels = self.create_multi_cluster_assignment(
                embeddings, dirichlet_uncertainty, num_base_clusters=3
            )
            num_clusters = max(3, len(torch.unique(current_labels)))
        
        # 1ï¸âƒ£ è®¡ç®—ç½®ä¿¡åº¦
        confidence_scores, avg_uncertainty = self.calculate_dirichlet_confidence(dirichlet_uncertainty)
        
        # 2ï¸âƒ£ ä¿å®ˆçš„å›°éš¾æ ·æœ¬è¯†åˆ«
        hard_sample_mask, criteria_stats = self.identify_hard_samples_conservative(
            dirichlet_uncertainty, confidence_scores, uncertainty_threshold, confidence_threshold
        )
        # ğŸ” è°ƒè¯•ï¼šçœ‹çœ‹â€œæ˜¯å¦çœŸçš„æ²¡æœ‰å›°éš¾æ ·æœ¬â€
        if hard_sample_mask.any():
            print(
                "âš ï¸ Hard sample triggered!",
                "count =", hard_sample_mask.sum().item(),
                "uncertainty =", dirichlet_uncertainty[hard_sample_mask][:5].detach().cpu().numpy(),
                "confidence =", confidence_scores[hard_sample_mask][:5].detach().cpu().numpy()
            )
        else:
            print(
                "[Debug] No hard samples | "
                f"uncertainty mean={dirichlet_uncertainty.mean().item():.4f}, "
                f"max={dirichlet_uncertainty.max().item():.4f} | "
                f"confidence mean={confidence_scores.mean().item():.4f}, "
                f"min={confidence_scores.min().item():.4f}"
            )
        high_confidence_mask = ~hard_sample_mask
        
        # 3ï¸âƒ£ ä¿ç•™é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„æ ‡ç­¾
        new_labels = current_labels.clone()
        
        # 4ï¸âƒ£ ä¿å®ˆçš„å›°éš¾æ ·æœ¬å¤„ç†
        reassignment_stats = {'reassigned_count': 0, 'noise_count': 0, 'label_change_count': 0}
        
        if hard_sample_mask.sum() > 0:
            if high_confidence_mask.sum() > 0:
                high_conf_embeddings = embeddings[high_confidence_mask]
                high_conf_labels = current_labels[high_confidence_mask]
                cluster_centers = self.compute_cluster_centers(
                    high_conf_embeddings, high_conf_labels, num_clusters
                )
            else:
                cluster_centers = self.compute_cluster_centers(
                    embeddings, current_labels, num_clusters
                )
            
            hard_embeddings = embeddings[hard_sample_mask]
            hard_indices = torch.where(hard_sample_mask)[0]
            
            reassigned_labels, distances = self.reassign_hard_samples_conservative(
                hard_embeddings, cluster_centers, hard_indices, current_labels
            )
            if (reassigned_labels != current_labels[hard_indices]).any():
                print(
                    f"ğŸŸ¡ Label changed at epoch {epoch}:",
                    (reassigned_labels != current_labels[hard_indices]).sum().item()
                )

            old_hard_labels = new_labels[hard_sample_mask].clone()
            new_labels[hard_sample_mask] = reassigned_labels
            
            reassignment_stats['reassigned_count'] = (reassigned_labels != -1).sum().item()
            reassignment_stats['noise_count'] = (reassigned_labels == -1).sum().item()
            reassignment_stats['label_change_count'] = (reassigned_labels != old_hard_labels).sum().item()
        
        # 5ï¸âƒ£ ç»Ÿè®¡ä¿¡æ¯
        total_label_changes = (new_labels != current_labels).sum().item()
        
        refinement_stats = {
            'total_samples': N,
            'high_confidence_count': high_confidence_mask.sum().item(),
            'hard_samples_count': hard_sample_mask.sum().item(),
            'noise_samples_count': (new_labels == -1).sum().item(),
            'label_changes': total_label_changes,
            'refinement_ratio': total_label_changes / N if N > 0 else 0.0,
            'avg_confidence': confidence_scores.mean().item(),
            'avg_uncertainty': avg_uncertainty.mean().item(),
            'min_confidence': confidence_scores.min().item(),
            'max_confidence': confidence_scores.max().item(),
            'min_uncertainty': avg_uncertainty.min().item(),
            'max_uncertainty': avg_uncertainty.max().item(),
            'unique_labels_before': len(torch.unique(current_labels)),
            'unique_labels_after': len(torch.unique(new_labels[new_labels != -1])),
            'epoch': epoch,
            **criteria_stats,
            **reassignment_stats
        }
        
        return new_labels, refinement_stats

# ==========================================
# æ•°æ®é›† (ä¿æŒä¸å˜)
# ==========================================
class CloverClusterDataset(Dataset):
    def __init__(self, data_dir, seq_len=150):
        self.seq_len = seq_len
        self.clusters = [] 
        
        read_path = os.path.join(data_dir, "read.txt")
        ref_path = os.path.join(data_dir, "ref.txt")
        if not os.path.exists(ref_path):
            ref_path = os.path.join(data_dir, "reference.txt")
            
        if not os.path.exists(read_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {read_path}")
            return
        
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {data_dir}")
        with open(ref_path, 'r') as f:
            refs = [line.strip() for line in f if line.strip()]
        with open(read_path, 'r') as f:
            content = f.read().strip()
        raw_clusters = content.split("===============================")
        
        for i, cluster_block in enumerate(raw_clusters):
            if not cluster_block.strip(): continue
            if i >= len(refs): break
            reads = [r.strip() for r in cluster_block.strip().split('\n') if r.strip()]
            if len(reads) > 0:
                self.clusters.append({'ref': refs[i], 'reads': reads})
                
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.clusters)} ä¸ªç°‡")

    def one_hot(self, seq):
        mapping = {'A':0, 'C':1, 'G':2, 'T':3}
        arr = np.zeros((self.seq_len, 4), dtype=np.float32)
        l = min(len(seq), self.seq_len)
        for i in range(l):
            char = seq[i]
            if char in mapping: arr[i, mapping[char]] = 1.0
        return arr

    def __len__(self): return len(self.clusters)

    def __getitem__(self, idx):
        cluster = self.clusters[idx]
        reads_vec = np.array([self.one_hot(r) for r in cluster['reads']])
        ref_vec = self.one_hot(cluster['ref'])
        return torch.tensor(reads_vec), torch.tensor(ref_vec)

# ==========================================
# ğŸ”¥ æ”¹è¿›çš„è®­ç»ƒå™¨ - æ¸è¿›å¼æ”¶æ•›
# ==========================================

class DirichletRefinementTrainer:
    """ğŸ”¥ æ¸è¿›å¼è®­ç»ƒå™¨ - å®Œæ•´ç‰ˆ"""
    
    def __init__(self, model, criterion, optimizer, refinement_module, 
                 convergence_threshold=0.05, max_epochs=15, min_epochs=8,
                 uncertainty_improvement_threshold=0.02):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.refinement = refinement_module
        self.convergence_threshold = convergence_threshold
        self.max_epochs = max_epochs
        self.min_epochs = min_epochs
        self.uncertainty_improvement_threshold = uncertainty_improvement_threshold
        
    def train_epoch_with_refinement(self, dataloader, device, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        
        self.model.train()
        epoch_losses = {'total_loss': 0, 'expected_mse': 0, 'contrastive_loss': 0, 'dirichlet_kl': 0}
        all_refinement_stats = []
        step_count = 0
        
        print(f"\nğŸ”„ Epoch {epoch+1} - æ¸è¿›å¼Dirichletè®­ç»ƒ...")
        
        for i, (reads, ref) in enumerate(dataloader):
            reads = reads.to(device)
            ref = ref.squeeze(0).to(device)
            N = reads.shape[1]
            
            # === æ­¥éª¤1: æ­£å‘ä¼ æ’­ ===
            self.optimizer.zero_grad()
            
            dirichlet_outputs, contrastive_features = self.model(reads)
            
            single_batch_outputs = {}
            for key in dirichlet_outputs:
                single_batch_outputs[key] = dirichlet_outputs[key].squeeze(0)
            
            fusion_results = self.model.evidence_fusion(single_batch_outputs)
            
            contrastive_features_flat = contrastive_features.squeeze(0)
            losses = self.criterion(fusion_results, ref, contrastive_features_flat)
            
            losses['total_loss'].backward()
            self.optimizer.step()
            
            # === æ­¥éª¤2: æ¸è¿›å¼ä¿®æ­£ ===
            with torch.no_grad():
                current_labels = torch.randint(0, 3, (N,), device=device)
                dirichlet_uncertainty = single_batch_outputs['uncertainty']
                
                # ğŸ”¥ ä¼ é€’epochä¿¡æ¯ç»™ä¿®æ­£æ¨¡å—
                new_labels, refinement_stats = self.refinement(
                    embeddings=contrastive_features_flat,
                    dirichlet_uncertainty=dirichlet_uncertainty,
                    current_labels=current_labels,
                    num_clusters=3,
                    epoch=epoch,
                    max_epochs=self.max_epochs
                )
                
                all_refinement_stats.append(refinement_stats)
            
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            step_count += 1
            
            # è¯¦ç»†è¾“å‡º
            if (i + 1) % 5 == 0:
                print(f"  ğŸ“Š Step {i+1:3d} | Loss: {losses['total_loss'].item():.4f} | "
                      f"ä¿®æ­£: {refinement_stats['refinement_ratio']:.3f} | "
                      f"å›°éš¾æ ·æœ¬: {refinement_stats['hard_samples_count']}/{N}")
                print(f"       é˜ˆå€¼: U={refinement_stats['uncertainty_threshold_used']:.3f}, "
                      f"C={refinement_stats['confidence_threshold_used']:.3f} | "
                      f"ä¸ç¡®å®šæ€§: {refinement_stats['avg_uncertainty']:.3f}")
        
        # è®¡ç®—epochç»Ÿè®¡
        avg_losses = {key: val / max(1, step_count) for key, val in epoch_losses.items()}
        
        # æ±‡æ€»ä¿®æ­£ç»Ÿè®¡
        if all_refinement_stats:
            avg_refinement_ratio = np.mean([s['refinement_ratio'] for s in all_refinement_stats])
            avg_confidence = np.mean([s['avg_confidence'] for s in all_refinement_stats])
            avg_uncertainty = np.mean([s['avg_uncertainty'] for s in all_refinement_stats])
            total_noise = sum([s['noise_samples_count'] for s in all_refinement_stats])
            total_hard_samples = sum([s['hard_samples_count'] for s in all_refinement_stats])
            total_label_changes = sum([s['label_changes'] for s in all_refinement_stats])
        else:
            avg_refinement_ratio = 0.0
            avg_confidence = 0.0
            avg_uncertainty = 0.0
            total_noise = 0
            total_hard_samples = 0
            total_label_changes = 0
            
        return avg_losses, {
            'refinement_ratio': avg_refinement_ratio,
            'avg_confidence': avg_confidence,
            'avg_uncertainty': avg_uncertainty,
            'total_noise_samples': total_noise,
            'total_hard_samples': total_hard_samples,
            'total_label_changes': total_label_changes
        }
    
    def train_with_refinement(self, dataloader, device):
        """ğŸ”¥ æ¸è¿›å¼è®­ç»ƒæµç¨‹ - å®Œæ•´ç‰ˆ"""
        
        print("ğŸš€ å¼€å§‹æ¸è¿›å¼Dirichletè®­ç»ƒ...")
        print(f"ğŸ“‹ é…ç½®: æ”¶æ•›é˜ˆå€¼={self.convergence_threshold}, æœ€å°è½®æ•°={self.min_epochs}")
        
        training_history = {
            'losses': [],
            'refinement_ratios': [],
            'confidences': [],
            'uncertainties': [],
            'noise_counts': [],
            'hard_sample_counts': [],
            'label_changes': []
        }
        
        prev_uncertainty = float('inf')
        convergence_count = 0  # è¿ç»­æ»¡è¶³æ”¶æ•›æ¡ä»¶çš„æ¬¡æ•°
        
        for epoch in range(self.max_epochs):
            avg_losses, refinement_stats = self.train_epoch_with_refinement(
                dataloader, device, epoch
            )
            
            # è®°å½•å†å²
            training_history['losses'].append(avg_losses)
            training_history['refinement_ratios'].append(refinement_stats['refinement_ratio'])
            training_history['confidences'].append(refinement_stats['avg_confidence'])
            training_history['uncertainties'].append(refinement_stats['avg_uncertainty'])
            training_history['noise_counts'].append(refinement_stats['total_noise_samples'])
            training_history['hard_sample_counts'].append(refinement_stats['total_hard_samples'])
            training_history['label_changes'].append(refinement_stats['total_label_changes'])
            
            # è®¡ç®—ä¸ç¡®å®šæ€§æ”¹å–„
            uncertainty_improvement = prev_uncertainty - refinement_stats['avg_uncertainty']
            prev_uncertainty = refinement_stats['avg_uncertainty']
            
            # æ‰“å°epochæ€»ç»“
            print(f"\nğŸ“ˆ Epoch {epoch+1} å®Œæˆ:")
            print(f"   æ€»æŸå¤±: {avg_losses['total_loss']:.6f}")
            print(f"   Expected MSE: {avg_losses['expected_mse']:.6f}")
            print(f"   Dirichlet KL: {avg_losses['dirichlet_kl']:.6f}")
            print(f"   ä¿®æ­£æ¯”ä¾‹: {refinement_stats['refinement_ratio']:.4f} ({refinement_stats['refinement_ratio']*100:.2f}%)")
            print(f"   å›°éš¾æ ·æœ¬: {refinement_stats['total_hard_samples']}")
            print(f"   ä¸ç¡®å®šæ€§: {refinement_stats['avg_uncertainty']:.4f} (æ”¹å–„: {uncertainty_improvement:+.4f})")
            print(f"   ç½®ä¿¡åº¦: {refinement_stats['avg_confidence']:.4f}")
            
            # ğŸ”¥ å®Œæ•´çš„æ”¶æ•›åˆ¤æ–­é€»è¾‘
            refinement_converged = refinement_stats['refinement_ratio'] < self.convergence_threshold
            uncertainty_stable = abs(uncertainty_improvement) < self.uncertainty_improvement_threshold
            has_label_changes = refinement_stats['total_label_changes'] > 0
            min_epochs_reached = epoch >= self.min_epochs
            
            # ç»¼åˆæ”¶æ•›æ¡ä»¶
            current_converged = (
                min_epochs_reached and 
                refinement_converged and 
                (uncertainty_stable or refinement_stats['avg_uncertainty'] < 0.5)
            )
            
            if current_converged:
                convergence_count += 1
                print(f"   âœ… æ»¡è¶³æ”¶æ•›æ¡ä»¶ ({convergence_count}/2)")
            else:
                convergence_count = 0
                if not min_epochs_reached:
                    print(f"   ğŸ”„ ç»§ç»­è®­ç»ƒ (æœªè¾¾åˆ°æœ€å°è½®æ•° {self.min_epochs})")
                elif not refinement_converged:
                    print(f"   ğŸ”„ ç»§ç»­è®­ç»ƒ (ä¿®æ­£æ¯”ä¾‹ {refinement_stats['refinement_ratio']:.4f} >= {self.convergence_threshold})")
                elif not uncertainty_stable and refinement_stats['avg_uncertainty'] >= 0.5:
                    print(f"   ğŸ”„ ç»§ç»­è®­ç»ƒ (ä¸ç¡®å®šæ€§æœªç¨³å®š: {uncertainty_improvement:+.4f})")
            
            # è¿ç»­2è½®æ»¡è¶³æ”¶æ•›æ¡ä»¶æ‰çœŸæ­£æ”¶æ•›
            if convergence_count >= 2:
                print(f"\nâœ… æ”¶æ•›è¾¾æˆï¼è¿ç»­ {convergence_count} è½®æ»¡è¶³æ”¶æ•›æ¡ä»¶")
                print(f"ğŸ¯ è®­ç»ƒåœ¨ç¬¬ {epoch+1} è½®æ”¶æ•›")
                break
            
            # æ—©åœæ¡ä»¶ï¼šä¸ç¡®å®šæ€§ä¸å†æ”¹å–„ä¸”ä¿®æ­£æ¯”ä¾‹å¾ˆå°
            if (epoch >= self.min_epochs + 3 and 
                refinement_stats['refinement_ratio'] < 0.01 and 
                abs(uncertainty_improvement) < 0.001):
                print(f"\nğŸ›‘ æ—©åœï¼šæ¨¡å‹å·²ç¨³å®š")
                print(f"ğŸ¯ è®­ç»ƒåœ¨ç¬¬ {epoch+1} è½®æ—©åœ")
                break
            
            print("-" * 70)
        
        # ğŸ”¥ è®­ç»ƒå®Œæˆæ€»ç»“
        final_stats = {
            'total_epochs': epoch + 1,
            'converged': convergence_count >= 2,
            'final_refinement_ratio': refinement_stats['refinement_ratio'],
            'final_uncertainty': refinement_stats['avg_uncertainty'],
            'final_confidence': refinement_stats['avg_confidence'],
            'final_loss': avg_losses['total_loss'],
            'uncertainty_reduction': training_history['uncertainties'][0] - refinement_stats['avg_uncertainty'] if training_history['uncertainties'] else 0,
            'avg_refinement_ratio': np.mean(training_history['refinement_ratios']) if training_history['refinement_ratios'] else 0
        }
        
        print(f"\nğŸ¯ æ¸è¿›å¼Dirichletè®­ç»ƒå®Œæˆæ€»ç»“:")
        print(f"   æœ€ç»ˆä¿®æ­£æ¯”ä¾‹: {final_stats['final_refinement_ratio']:.4f}")
        print(f"   æœ€ç»ˆä¸ç¡®å®šæ€§: {final_stats['final_uncertainty']:.4f}")
        print(f"   æœ€ç»ˆç½®ä¿¡åº¦: {final_stats['final_confidence']:.4f}")
        print(f"   ä¸ç¡®å®šæ€§é™ä½: {final_stats['uncertainty_reduction']:+.4f}")
        print(f"   æ€»è®­ç»ƒè½®æ•°: {final_stats['total_epochs']}")
        print(f"   å¹³å‡ä¿®æ­£æ¯”ä¾‹: {final_stats['avg_refinement_ratio']:.4f}")
        
        if final_stats['converged']:
            print("   âœ… æˆåŠŸæ”¶æ•›ï¼")
        else:
            print("   âš ï¸  æœªå®Œå…¨æ”¶æ•›ï¼Œå¯è€ƒè™‘:")
            print("      - å¢åŠ è®­ç»ƒè½®æ•°")
            print("      - è°ƒæ•´æ”¶æ•›é˜ˆå€¼")
            print("      - æ£€æŸ¥æ•°æ®è´¨é‡")
        
        return training_history, final_stats

# ==========================================
# ğŸ”¥ å®Œæ•´çš„è®­ç»ƒä¸»å‡½æ•°
# ==========================================

def train_with_improved_dirichlet_refinement():
    """ğŸ”¥ ä½¿ç”¨æ”¹è¿›æ•°æ®å’Œæ¸è¿›å¼è®­ç»ƒçš„å®Œæ•´æµç¨‹"""
    
    print("ğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆDirichlet Evidence Learningè®­ç»ƒ...")
    
    # 1ï¸âƒ£ æ•°æ®å‡†å¤‡
    print("\nğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨é»˜è®¤æ•°æ®è·¯å¾„
    DATA_DIR = "CC/Step0/Experiments/20251216_145746_Improved_Data_Test/03_FedDNA_In"
    if not os.path.exists(DATA_DIR):
        DATA_DIR = "/hy-tmp/data"  # å¤‡ç”¨è·¯å¾„
        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤æ•°æ®é›†: {DATA_DIR}")
    else:
        print(f"âœ… ä½¿ç”¨æ•°æ®é›†: {DATA_DIR}")
    
    # 2ï¸âƒ£ æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶åˆå§‹åŒ–
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½
    dataset = CloverClusterDataset(DATA_DIR)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    print(f"ğŸ“¦ æ•°æ®åŠ è½½å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
    
    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç±»å’Œå‚æ•°
    model = SimplifiedFedDNA(
        input_dim=4,
        hidden_dim=128,  # å¢åŠ éšè—ç»´åº¦
        seq_len=150
    ).to(device)
    
    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æŸå¤±å‡½æ•°ç±»
    criterion = DirichletComprehensiveLoss(
        alpha=1.0,      # Dirichlet Expected MSEæƒé‡
        beta=0.1,       # å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡
        gamma=0.01,     # Dirichlet KLæ•£åº¦æƒé‡
        temperature=0.1
    )
    
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=1e-5
    )
    
    # ğŸ”¥ æ¸è¿›å¼ä¿®æ­£æ¨¡å—
    refinement_module = DirichletEvidenceRefinement(
        uncertainty_threshold_start=0.9,    # åˆå§‹ä¸¥æ ¼
        uncertainty_threshold_end=0.7,      # æœ€ç»ˆæ”¾æ¾
        confidence_threshold_start=0.1,     # åˆå§‹ä¸¥æ ¼
        confidence_threshold_end=0.3,       # æœ€ç»ˆæ”¾æ¾
        distance_threshold=1.0,
        max_refinement_ratio=0.2            # æœ€å¤§20%ä¿®æ­£
    )
    
    # ğŸ”¥ æ¸è¿›å¼è®­ç»ƒå™¨
    trainer = DirichletRefinementTrainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        refinement_module=refinement_module,
        convergence_threshold=0.08,         # æ”¾å®½æ”¶æ•›é˜ˆå€¼
        max_epochs=12,
        min_epochs=5,
        uncertainty_improvement_threshold=0.01
    )
    
    # 3ï¸âƒ£ å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹æ¸è¿›å¼è®­ç»ƒ...")
    training_history, final_stats = trainer.train_with_refinement(dataloader, device)
    
    # 4ï¸âƒ£ ä¿å­˜æ¨¡å‹
    model_save_path = "improved_dirichlet_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'training_history': training_history,
        'final_stats': final_stats,
        'model_config': {
            'input_dim': 4,
            'hidden_dim': 128,
            'seq_len': 150
        }
    }, model_save_path)
    
    print(f"\nğŸ’¾ æ”¹è¿›ç‰ˆDirichletæ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")
    
    # 5ï¸âƒ£ è®­ç»ƒç»“æœå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
    try:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # æŸå¤±æ›²çº¿
        losses = [l['total_loss'] for l in training_history['losses']]
        axes[0,0].plot(losses)
        axes[0,0].set_title('Training Loss')
        axes[0,0].set_xlabel('Epoch')
        axes[0,0].set_ylabel('Loss')
        
        # ä¿®æ­£æ¯”ä¾‹
        axes[0,1].plot(training_history['refinement_ratios'])
        axes[0,1].axhline(y=0.08, color='r', linestyle='--', label='Convergence Threshold')
        axes[0,1].set_title('Refinement Ratio')
        axes[0,1].set_xlabel('Epoch')
        axes[0,1].set_ylabel('Ratio')
        axes[0,1].legend()
        
        # ä¸ç¡®å®šæ€§
        axes[1,0].plot(training_history['uncertainties'])
        axes[1,0].set_title('Average Uncertainty')
        axes[1,0].set_xlabel('Epoch')
        axes[1,0].set_ylabel('Uncertainty')
        
        # ç½®ä¿¡åº¦
        axes[1,1].plot(training_history['confidences'])
        axes[1,1].set_title('Average Confidence')
        axes[1,1].set_xlabel('Epoch')
        axes[1,1].set_ylabel('Confidence')
        
        plt.tight_layout()
        plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: training_curves.png")
        
    except ImportError:
        print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
    
    return model, training_history, final_stats

if __name__ == "__main__":
    model, history, stats = train_with_improved_dirichlet_refinement()
