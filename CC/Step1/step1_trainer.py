"""
Step1 è®­ç»ƒå™¨
è´Ÿè´£åŸºç¡€è®­ç»ƒå¾ªç¯çš„æ‰§è¡Œ
"""

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from step1_model import SimplifiedFedDNA
from step1_loss import ComprehensiveLoss
from step1_data import CloverClusterDataset

class BasicTrainer:
    """Step1åŸºç¡€è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config['device'] if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = SimplifiedFedDNA(**config['model_params']).to(self.device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(self.model.parameters(), lr=config['training_params']['lr'])
        self.criterion = ComprehensiveLoss(**config['training_params']['loss_weights'])
        
        # åŠ è½½æ•°æ®
        dataset = CloverClusterDataset(config['data_dir'])
        self.dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
        
        print(f"ğŸ”§ Step1è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_losses = {'total_loss': 0, 'reconstruction_loss': 0, 'contrastive_loss': 0, 'kl_loss': 0}
        step_count = 0
        
        for i, (reads, ref) in enumerate(self.dataloader):
            reads = reads.to(self.device)
            ref = ref.squeeze(0).to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            evidence_batch, contrastive_features = self.model(reads)
            
            # è¯æ®èåˆ
            evidence_single_batch = evidence_batch.squeeze(0)
            fused_evidence, strengths = self.model.evidence_fusion(evidence_single_batch)
            
            # è®¡ç®—æŸå¤±
            contrastive_features_flat = contrastive_features.squeeze(0)
            losses = self.criterion(fused_evidence, ref, contrastive_features_flat)
            
            # åå‘ä¼ æ’­
            losses['total_loss'].backward()
            self.optimizer.step()
            
            # è®°å½•æŸå¤±
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            step_count += 1
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 5 == 0:
                print(f"  ğŸ“Š Step {i+1:3d} | Loss: {losses['total_loss'].item():.6f}")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {key: val / max(1, step_count) for key, val in epoch_losses.items()}
        return avg_losses
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        epochs = self.config['training_params']['epochs']
        history = []
        
        print(f"\nğŸš€ å¼€å§‹Step1è®­ç»ƒ | Epochs: {epochs}")
        weights = self.config['training_params']['loss_weights']
        print(f"ğŸ“Š æŸå¤±å‡½æ•°æƒé‡: é‡æ„={weights['alpha']}, å¯¹æ¯”å­¦ä¹ ={weights['beta']}, KLæ•£åº¦={weights['gamma']}")
        
        for epoch in range(epochs):
            print(f"\nğŸ”„ Epoch {epoch+1}/{epochs}")
            
            avg_losses = self.train_epoch(epoch)
            history.append(avg_losses)
            
            # æ‰“å°epochæ€»ç»“
            print(f"ğŸ“ˆ Epoch {epoch+1} å®Œæˆ:")
            print(f"   Total Loss:    {avg_losses['total_loss']:.6f}")
            print(f"   Reconstruction: {avg_losses['reconstruction_loss']:.6f}")
            print(f"   Contrastive:   {avg_losses['contrastive_loss']:.6f}")
            print(f"   KL Divergence: {avg_losses['kl_loss']:.6f}")
            print("-" * 60)
        
        print("âœ… Step1è®­ç»ƒå®Œæˆï¼")
        return history
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'model_class': 'SimplifiedFedDNA'
        }
        torch.save(save_dict, filepath)
        print(f"ğŸ’¾ Step1æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
