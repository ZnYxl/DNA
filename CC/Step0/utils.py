import os
import random
import ast
import collections
import numpy as np
from random import seed, shuffle, randint, choice
import multiprocessing  # âœ… æ–°å¢å¤šè¿›ç¨‹åº“
from functools import partial

# ==============================================================================
# Part 1: å¸ˆå…„æä¾›çš„æ¨¡æ‹Ÿä»£ç  (å®Œå…¨ä¿ç•™ï¼ŒåŸå°ä¸åŠ¨)
# ==============================================================================

rd = random.Random() 

def channel_model_unit(code, pr_dict):
    # ä¿¡é“æ¨¡æ‹Ÿå‡½æ•°
    del_num = 0
    ins_num = 0
    sub_num = 0
    pt_num = 0
    unit_list = ["A","T","G","C"]
    af_code = ""
    if rd.random() <= pr_dict["column"]:
        return ""
    else:
        for i in range(len(code)):
            ins_times = 0
            while ins_times < 1:  
                if rd.random() <= pr_dict["pi"]:
                    af_code += random.choice(unit_list)
                    ins_num = ins_num + 1
                else:
                    break
            if rd.random() <= pr_dict["pd"]:
                del_num += 1
                continue
            else:
                pt_num += 1
                if rd.random() <= pr_dict["ps"]:
                    target = choice(list(filter(lambda base: base != code[i], ["A", "C", "G", "T"])))
                    sub_num += 1
                    af_code+=target
                else:
                    af_code+=code[i]
    return af_code

def channel_simulation(dna_reads_list, depth, random_sample=False, pr_dict={"column":0,"pi":0,"pd":0,"ps":0}):
    channel_reads_list = []
    dna_nums = len(dna_reads_list)
    seq_nums = dna_nums*depth

    if random_sample == True:
        for _ in range(seq_nums):
            index = random.randint(0,dna_nums-1)
            now_read = dna_reads_list[index]
            channel_reads_list.append(channel_model_unit(now_read,pr_dict))
        return channel_reads_list
    else:
        for read in dna_reads_list:
            for _ in range(depth):
                channel_reads_list.append(channel_model_unit(read,pr_dict))
        shuffle(channel_reads_list)  
        return channel_reads_list

# ==============================================================================
# Part 2: è¾…åŠ©å‡½æ•° 
# ==============================================================================

def generate_diverse_references(num_clusters, seq_len, min_distance=0.3):
    """ç”Ÿæˆå…·æœ‰è¶³å¤ŸåŒºåˆ†åº¦çš„å‚è€ƒåºåˆ—"""
    bases = ['A', 'C', 'G', 'T']
    references = []
    # ä¸ºäº†é€Ÿåº¦ï¼Œå¤§æ•°é‡æ—¶ç®€åŒ–è·ç¦»æ£€æŸ¥ï¼Œä¸»è¦ä¾èµ–éšæœºæ€§
    # 10000æ¡éšæœºåºåˆ—ç¢°æ’æ¦‚ç‡æä½
    for i in range(num_clusters):
        candidate = "".join(random.choice(bases) for _ in range(seq_len))
        references.append(candidate)
    return references

# ==============================================================================
# Part 3: å¹¶è¡ŒåŒ–å¤„ç†å•å…ƒ (æ–°å¢)
# ==============================================================================

def process_single_cluster(args):
    """
    å•ä¸ªç°‡çš„å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹è°ƒç”¨
    """
    cluster_idx, ref_seq, reads_per_cluster, pr_dict = args
    
    # æ¨¡æ‹Ÿç°‡å¤§å°æ³¢åŠ¨
    low = int(reads_per_cluster * 0.6)
    high = int(reads_per_cluster * 1.4)
    actual_depth = random.randint(low, high)
    
    # è°ƒç”¨å¸ˆå…„ä»£ç 
    simulated_reads = channel_simulation(
        dna_reads_list=[ref_seq], 
        depth=actual_depth, 
        random_sample=False, 
        pr_dict=pr_dict
    )
    
    results = []
    for r_seq in simulated_reads:
        if not r_seq: continue
        # è¿”å›æ—¶ä¸å¸¦ IDï¼Œç”±ä¸»è¿›ç¨‹ç»Ÿä¸€åˆ†é… IDï¼Œé¿å…å†²çª
        # æ ¼å¼: (Seq, ClusterID, RefSeq, Quality)
        results.append((r_seq, cluster_idx, ref_seq, "simulated"))
        
    return results

# ==============================================================================
# Part 4: æ•°æ®ç”Ÿæˆä¸»æ§ (å¤šè¿›ç¨‹ç‰ˆ)
# ==============================================================================

def generate_data(output_dir, num_clusters=100, reads_per_cluster=50, seq_len=150, reference_type="diverse"):
    raw_path = os.path.join(output_dir, "raw_reads.txt")
    read_gt_path = os.path.join(output_dir, "ground_truth_reads.txt")
    cluster_gt_path = os.path.join(output_dir, "ground_truth_clusters.txt")
    
    pr_dict = {"column": 0.0001, "pi": 0.0005, "pd": 0.0005, "ps": 0.008}
    print(f"ğŸ”§ [Generator] å¤šè¿›ç¨‹åŠ é€Ÿå¯åŠ¨ã€‚å‚æ•°: {pr_dict}")

    # 1. ç”Ÿæˆå‚è€ƒåºåˆ—
    print("   ... ç”Ÿæˆ Payload å‚è€ƒåºåˆ—")
    ground_truths = generate_diverse_references(num_clusters, seq_len)
    
    with open(cluster_gt_path, 'w') as f:
        f.write("Cluster_ID\tRef_Seq\n")
        for cid, seq in enumerate(ground_truths):
            f.write(f"{cid}\t{seq}\n")

    # 2. å‡†å¤‡å¹¶è¡Œä»»åŠ¡
    print(f"   ... æ­£åœ¨å¯åŠ¨å¤šè¿›ç¨‹æ±  (Clusters: {num_clusters})")
    
    # å‡†å¤‡å‚æ•°åˆ—è¡¨
    tasks = []
    for cluster_idx, ref_seq in enumerate(ground_truths):
        tasks.append((cluster_idx, ref_seq, reads_per_cluster, pr_dict))
    
    # è·å–CPUæ ¸å¿ƒæ•° (ä¿ç•™2ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ)
    num_workers = max(1, multiprocessing.cpu_count() - 2)
    
    all_reads_data = []
    counter = 0
    
    # âœ… å¼€å¯å¹¶è¡Œå¤„ç†
    with multiprocessing.Pool(processes=num_workers) as pool:
        # ä½¿ç”¨ imap_unordered ç¨å¾®å¿«ä¸€ç‚¹ï¼Œä¸”èƒ½æ˜¾ç¤ºè¿›åº¦
        for result_batch in pool.imap_unordered(process_single_cluster, tasks, chunksize=100):
            for item in result_batch:
                counter += 1
                read_id = str(counter)
                # itemæ˜¯ (Seq, ClusterID, RefSeq, Quality)
                # åŠ ä¸Š read_id
                all_reads_data.append((read_id,) + item)
            
            if len(all_reads_data) % 100000 == 0:
                print(f"      å·²ç”Ÿæˆ {len(all_reads_data)} æ¡ reads...")

    print("   ... æ­£åœ¨æ‰“ä¹±æ•°æ® (Shuffle)")
    random.shuffle(all_reads_data)

    # 3. å†™å…¥æ–‡ä»¶
    print(f"   ... å†™å…¥ç¡¬ç›˜ ({len(all_reads_data)} æ¡)")
    with open(raw_path, 'w') as f:
        for item in all_reads_data:
            f.write(f"{item[0]}\t{item[1]}\n")
            
    with open(read_gt_path, 'w') as f:
        f.write("Read_ID\tCluster_ID\tRef_Seq\tQuality\n")
        for item in all_reads_data:
            f.write(f"{item[0]}\t{item[2]}\t{item[3]}\t{item[4]}\n")
            
    return raw_path, read_gt_path, cluster_gt_path

# ==============================================================================
# Part 5: æ ¼å¼è½¬æ¢ (Clover -> FedDNA) - ä¿æŒä¸å˜
# ==============================================================================

def load_raw_reads(file_path):
    d = {}
    with open(file_path, 'r') as f:
        for line in f:
            p = line.strip().split()
            if len(p) >= 2: d[p[0]] = p[1]
    return d

def clover_to_feddna(clover_out_path, raw_reads_path, output_dir):
    # ... (ä¿æŒåŸä»£ç ä¸å˜) ...
    raw_reads = load_raw_reads(raw_reads_path)
    clusters = collections.defaultdict(list)
    try:
        with open(clover_out_path, 'r') as f:
            content = f.read().strip()
            if content.startswith("[") and content.endswith("]"):
                pairs = ast.literal_eval(content)
                for item in pairs:
                    if str(item[1]) not in ['-1', -1]:
                        clusters[str(item[1])].append(str(item[0]))
            else:
                f.seek(0)
                for line in f:
                    p = line.replace(',', ' ').split()
                    if len(p) >= 2 and p[1] not in ['-1', '-1']:
                        clusters[p[1]].append(p[0])
    except Exception as e:
        print(f"âŒ è§£æ Clover è¾“å‡ºå¤±è´¥: {e}")
        return 0, ""

    out_read = os.path.join(output_dir, "read.txt")
    out_ref = os.path.join(output_dir, "ref.txt")
    
    valid_count = 0
    with open(out_read, 'w') as fr, open(out_ref, 'w') as ff:
        for cid, mems in clusters.items():
            if cid in raw_reads:
                ff.write(raw_reads[cid] + "\n") 
                fr.write(raw_reads[cid] + "\n") 
                for m in mems:
                    if m in raw_reads:
                        fr.write(raw_reads[m] + "\n")
                fr.write("===============================\n")
                valid_count += 1
    return valid_count, out_read