# utils.py - æ”¹è¿›ç‰ˆ
import random
import os
import ast
import collections
import numpy as np

# ==================== æ¨¡å—1: æ”¹è¿›çš„æ•°æ®ç”Ÿæˆ ====================

def generate_diverse_references(num_clusters, seq_len, min_distance=0.25):
    """
    ğŸ”¥ ç”Ÿæˆå…·æœ‰è¶³å¤ŸåŒºåˆ†åº¦çš„å‚è€ƒåºåˆ—
    é™ä½min_distanceåˆ°0.25ï¼Œé€‚åˆCloverèšç±»
    """
    bases = ['A', 'C', 'G', 'T']
    references = []
    max_attempts = 1000
    
    # ç”Ÿæˆç¬¬ä¸€ä¸ªå‚è€ƒåºåˆ—
    first_ref = "".join(random.choice(bases) for _ in range(seq_len))
    references.append(first_ref)
    
    # ç”Ÿæˆå…¶ä»–å‚è€ƒåºåˆ—ï¼Œç¡®ä¿è¶³å¤Ÿçš„åŒºåˆ†åº¦
    for i in range(1, num_clusters):
        attempts = 0
        while attempts < max_attempts:
            candidate = "".join(random.choice(bases) for _ in range(seq_len))
            
            # æ£€æŸ¥ä¸å·²æœ‰åºåˆ—çš„è·ç¦»
            min_dist = float('inf')
            for existing_ref in references:
                hamming_dist = sum(c1 != c2 for c1, c2 in zip(candidate, existing_ref))
                hamming_ratio = hamming_dist / seq_len
                min_dist = min(min_dist, hamming_ratio)
            
            if min_dist >= min_distance:
                references.append(candidate)
                break
            attempts += 1
        
        if attempts == max_attempts:
            print(f"âš ï¸  è­¦å‘Š: ç°‡ {i} çš„å‚è€ƒåºåˆ—å¯èƒ½ä¸å…¶ä»–ç°‡ç›¸ä¼¼åº¦è¿‡é«˜")
            # å¼ºåˆ¶ç”Ÿæˆä¸€ä¸ªå·®å¼‚è¾ƒå¤§çš„åºåˆ—
            candidate = list(references[0])
            # éšæœºæ”¹å˜è‡³å°‘min_distanceæ¯”ä¾‹çš„ä½ç½®
            positions_to_change = random.sample(range(seq_len), 
                                               int(seq_len * min_distance) + 1)
            for pos in positions_to_change:
                original_base = candidate[pos]
                new_bases = [b for b in bases if b != original_base]
                candidate[pos] = random.choice(new_bases)
            references.append("".join(candidate))
    
    return references

def create_motif_based_references(num_clusters, seq_len):
    """
    ğŸ”¥ åŸºäºmotifçš„å‚è€ƒåºåˆ—ç”Ÿæˆ - æ›´çœŸå®çš„ç”Ÿç‰©å­¦æ¨¡å¼
    """
    # å®šä¹‰ä¸€äº›ç”Ÿç‰©å­¦ä¸Šæœ‰æ„ä¹‰çš„motif
    motifs = [
        "ATCGATCG",      # ç®€å•é‡å¤
        "GCGCGCGC",      # GCå¯Œé›†
        "ATATATATAT",    # ATå¯Œé›†
        "CGTACGTA",      # å›æ–‡åºåˆ—
        "TGCATGCA",      # å¦ä¸€ä¸ªå›æ–‡
        "AAGCTTAAGCTT",  # é™åˆ¶é…¶ä½ç‚¹
        "GAATTCGAATTC",  # EcoRIä½ç‚¹
        "GGATCCGGATCC",  # BamHIä½ç‚¹
        "CCGGCCGG",      # é«˜GCå«é‡
        "TTAATTAA",      # ä½GCå«é‡
    ]
    
    references = []
    for i in range(num_clusters):
        # é€‰æ‹©ä¸»è¦motif
        main_motif = motifs[i % len(motifs)]
        
        # æ„å»ºåºåˆ—
        sequence = []
        pos = 0
        while pos < seq_len:
            if pos + len(main_motif) <= seq_len and random.random() < 0.5:
                # 50%æ¦‚ç‡æ’å…¥motif
                sequence.extend(list(main_motif))
                pos += len(main_motif)
            else:
                # æ’å…¥éšæœºç¢±åŸº
                sequence.append(random.choice(['A', 'C', 'G', 'T']))
                pos += 1
        
        # æˆªæ–­åˆ°æŒ‡å®šé•¿åº¦
        ref_seq = "".join(sequence[:seq_len])
        
        # å¦‚æœå¤ªçŸ­ï¼Œç”¨éšæœºåºåˆ—è¡¥é½
        while len(ref_seq) < seq_len:
            ref_seq += random.choice(['A', 'C', 'G', 'T'])
        
        references.append(ref_seq)
    
    return references

def mutate_sequence_realistic(sequence, sub_rate=0.008, del_rate=0.003, ins_rate=0.003):
    """
    ğŸ”¥ æ›´çœŸå®çš„åºåˆ—çªå˜æ¨¡å‹ - é™ä½é”™è¯¯ç‡ï¼Œé€‚åˆClover
    """
    bases = ['A', 'C', 'G', 'T']
    result = list(sequence)
    
    i = 0
    while i < len(result):
        # æ›¿æ¢çªå˜ - æ¨¡æ‹ŸçœŸå®çš„è½¬æ¢åå¥½
        if random.random() < sub_rate:
            original = result[i]
            # è½¬æ¢åå¥½ï¼šA<->G (å˜Œå‘¤), C<->T (å˜§å•¶)
            if original == 'A':
                result[i] = 'G' if random.random() < 0.6 else random.choice(['C', 'T'])
            elif original == 'G':
                result[i] = 'A' if random.random() < 0.6 else random.choice(['C', 'T'])
            elif original == 'C':
                result[i] = 'T' if random.random() < 0.6 else random.choice(['A', 'G'])
            elif original == 'T':
                result[i] = 'C' if random.random() < 0.6 else random.choice(['A', 'G'])
        
        # åˆ é™¤çªå˜
        if random.random() < del_rate:
            result.pop(i)
            continue
        
        # æ’å…¥çªå˜
        if random.random() < ins_rate:
            insert_base = random.choice(bases)
            result.insert(i, insert_base)
            i += 1
        
        i += 1
    
    return "".join(result)

def add_quality_variation(reads_data, high_quality_ratio=0.75):
    """
    ğŸ”¥ æ·»åŠ è´¨é‡å˜åŒ– - é€‚åˆCloverèšç±»çš„è´¨é‡åˆ†å¸ƒ
    """
    enhanced_reads = []
    
    for read_id, sequence, cluster_id, ref_seq in reads_data:
        if random.random() < high_quality_ratio:
            # é«˜è´¨é‡read - å¾ˆä½é”™è¯¯ç‡
            noisy_seq = mutate_sequence_realistic(sequence, 
                                                sub_rate=0.003, 
                                                del_rate=0.001, 
                                                ins_rate=0.001)
            quality = "high"
        else:
            # ä½è´¨é‡read - ä¸­ç­‰é”™è¯¯ç‡
            noisy_seq = mutate_sequence_realistic(sequence, 
                                                sub_rate=0.015, 
                                                del_rate=0.008, 
                                                ins_rate=0.008)
            quality = "low"
        
        enhanced_reads.append((read_id, noisy_seq, cluster_id, ref_seq, quality))
    
    return enhanced_reads

def generate_data(output_dir, num_clusters, reads_per_cluster, seq_len, 
                 reference_type="diverse", min_distance=0.25):
    """
    ğŸ”¥ æ”¹è¿›çš„æ•°æ®ç”Ÿæˆå‡½æ•° - é€‚åˆClover + ç¥ç»ç½‘ç»œçš„æµæ°´çº¿
    """
    os.makedirs(output_dir, exist_ok=True)
    
    raw_path = os.path.join(output_dir, "raw_reads.txt")
    gt_path = os.path.join(output_dir, "ground_truth.txt")
    stats_path = os.path.join(output_dir, "data_stats.txt")
    
    print(f"ğŸ”§ ç”Ÿæˆæ”¹è¿›æ•°æ®: {num_clusters}ç°‡ x {reads_per_cluster}reads, é•¿åº¦={seq_len}")
    
    # 1ï¸âƒ£ ç”Ÿæˆé«˜åŒºåˆ†åº¦çš„å‚è€ƒåºåˆ—
    if reference_type == "motif":
        ground_truths = create_motif_based_references(num_clusters, seq_len)
        print("   ä½¿ç”¨åŸºäºmotifçš„å‚è€ƒåºåˆ—")
    else:
        ground_truths = generate_diverse_references(num_clusters, seq_len, min_distance)
        print(f"   ä½¿ç”¨é«˜åŒºåˆ†åº¦çš„éšæœºå‚è€ƒåºåˆ— (æœ€å°è·ç¦»={min_distance})")
    
    # 2ï¸âƒ£ è®¡ç®—å‚è€ƒåºåˆ—é—´çš„è·ç¦»ç»Ÿè®¡
    distances = []
    for i in range(num_clusters):
        for j in range(i+1, num_clusters):
            hamming_dist = sum(c1 != c2 for c1, c2 in zip(ground_truths[i], ground_truths[j]))
            distances.append(hamming_dist / seq_len)
    
    min_distance_actual = min(distances) if distances else 0
    avg_distance = np.mean(distances) if distances else 0
    print(f"   å®é™…ç°‡é—´è·ç¦»: æœ€å°={min_distance_actual:.3f}, å¹³å‡={avg_distance:.3f}")
    
    # 3ï¸âƒ£ ç”Ÿæˆreadsæ•°æ®
    all_reads_data = []
    counter = 0
    
    for cluster_idx, ref_seq in enumerate(ground_truths):
        for read_idx in range(reads_per_cluster):
            counter += 1
            read_id = f"read_{counter:06d}"
            all_reads_data.append((read_id, ref_seq, cluster_idx, ref_seq))
    
    # 4ï¸âƒ£ æ·»åŠ è´¨é‡å˜åŒ–å’Œçªå˜
    enhanced_reads = add_quality_variation(all_reads_data, high_quality_ratio=0.75)
    
    # 5ï¸âƒ£ éšæœºæ‰“ä¹±
    random.shuffle(enhanced_reads)
    
    # 6ï¸âƒ£ ä¿å­˜æ•°æ® - ä¿æŒåŸæ ¼å¼å…¼å®¹æ€§
    with open(raw_path, 'w') as f:
        for read_id, noisy_seq, cluster_id, ref_seq, quality in enhanced_reads:
            f.write(f"{read_id}\t{noisy_seq}\n")
    
    with open(gt_path, 'w') as f:
        f.write("Read_ID\tCluster_ID\tRef_Seq\tQuality\n")
        for read_id, noisy_seq, cluster_id, ref_seq, quality in enhanced_reads:
            f.write(f"{read_id}\t{cluster_id}\t{ref_seq}\t{quality}\n")
    
    # 7ï¸âƒ£ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(stats_path, 'w') as f:
        f.write("=== æ”¹è¿›æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ ===\n")
        f.write(f"ç°‡æ•°é‡: {num_clusters}\n")
        f.write(f"æ¯ç°‡readsæ•°: {reads_per_cluster}\n")
        f.write(f"åºåˆ—é•¿åº¦: {seq_len}\n")
        f.write(f"æ€»readsæ•°: {len(enhanced_reads)}\n")
        f.write(f"å‚è€ƒåºåˆ—ç±»å‹: {reference_type}\n")
        f.write(f"ç›®æ ‡æœ€å°è·ç¦»: {min_distance}\n")
        f.write(f"å®é™…æœ€å°è·ç¦»: {min_distance_actual:.3f}\n")
        f.write(f"å®é™…å¹³å‡è·ç¦»: {avg_distance:.3f}\n")
        f.write("\n=== å‚è€ƒåºåˆ— ===\n")
        for i, ref in enumerate(ground_truths):
            f.write(f"Cluster_{i}: {ref}\n")
        
        # è´¨é‡åˆ†å¸ƒç»Ÿè®¡
        high_quality_count = sum(1 for _, _, _, _, q in enhanced_reads if q == "high")
        f.write(f"\n=== è´¨é‡åˆ†å¸ƒ ===\n")
        f.write(f"é«˜è´¨é‡reads: {high_quality_count} ({high_quality_count/len(enhanced_reads)*100:.1f}%)\n")
        f.write(f"ä½è´¨é‡reads: {len(enhanced_reads)-high_quality_count} ({(len(enhanced_reads)-high_quality_count)/len(enhanced_reads)*100:.1f}%)\n")
        
        # é”™è¯¯ç‡ç»Ÿè®¡
        f.write(f"\n=== é”™è¯¯ç‡è®¾ç½® ===\n")
        f.write(f"é«˜è´¨é‡reads: æ›¿æ¢0.3%, æ’å…¥/åˆ é™¤0.1%\n")
        f.write(f"ä½è´¨é‡reads: æ›¿æ¢1.5%, æ’å…¥/åˆ é™¤0.8%\n")
    
    print(f"âœ… æ”¹è¿›æ•°æ®å·²ä¿å­˜:")
    print(f"   Raw reads: {raw_path}")
    print(f"   Ground truth: {gt_path}")
    print(f"   Statistics: {stats_path}")
    
    return raw_path, gt_path

# ==================== æ¨¡å—2: æ ¼å¼è½¬æ¢ (ä¿æŒä¸å˜) ====================
def load_raw_reads(file_path):
    d = {}
    with open(file_path, 'r') as f:
        for line in f:
            p = line.strip().split()
            if len(p) >= 2: 
                d[p[0]] = p[1]
    return d

def clover_to_feddna(clover_out_path, raw_reads_path, output_dir):
    """å°†Cloverç»“æœè½¬æ¢ä¸ºFedDNAæ ¼å¼ - å¢å¼ºç‰ˆ"""
    raw_reads = load_raw_reads(raw_reads_path)
    clusters = collections.defaultdict(list)
    
    print(f"ğŸ“Š è§£æCloverè¾“å‡º: {clover_out_path}")
    
    # æ™ºèƒ½è§£æ Clover è¾“å‡º
    with open(clover_out_path, 'r') as f:
        content = f.read().strip()
        if content.startswith("[") and content.endswith("]"):
            # åˆ—è¡¨æ ¼å¼è§£æ
            try:
                pairs = ast.literal_eval(content)
                for item in pairs:
                    if str(item[1]) not in ['-1', -1]:
                        clusters[str(item[1])].append(str(item[0]))
                print(f"   âœ… åˆ—è¡¨æ ¼å¼è§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(pairs)} ä¸ªåˆ†é…")
            except Exception as e:
                print(f"   âŒ åˆ—è¡¨æ ¼å¼è§£æå¤±è´¥: {e}")
        else:
            # é€è¡Œæ ¼å¼è§£æ
            f.seek(0)
            line_count = 0
            for line in f:
                line_count += 1
                p = line.replace(',', ' ').split()
                if len(p) >= 2 and p[1] not in ['-1', '-1']:
                    clusters[p[1]].append(p[0])
            print(f"   âœ… é€è¡Œæ ¼å¼è§£æå®Œæˆï¼Œå¤„ç† {line_count} è¡Œ")
    
    # ç»Ÿè®¡èšç±»ç»“æœ
    valid_clusters = {k: v for k, v in clusters.items() if len(v) > 0}
    cluster_sizes = [len(v) for v in valid_clusters.values()]
    
    print(f"ğŸ“ˆ Cloverèšç±»ç»Ÿè®¡:")
    print(f"   æœ‰æ•ˆç°‡æ•°: {len(valid_clusters)}")
    if cluster_sizes:
        print(f"   ç°‡å¤§å°: æœ€å°={min(cluster_sizes)}, æœ€å¤§={max(cluster_sizes)}, å¹³å‡={np.mean(cluster_sizes):.1f}")
    
    # å†™å…¥ç»“æœ
    out_read = os.path.join(output_dir, "read.txt")
    out_ref = os.path.join(output_dir, "reference.txt")
    
    valid_count = 0
    total_reads_written = 0
    
    with open(out_read, 'w') as fr, open(out_ref, 'w') as ff:
        for cid, mems in valid_clusters.items():
            # é€‰æ‹©ç¬¬ä¸€ä¸ªæˆå‘˜ä½œä¸ºä¼ªå‚è€ƒåºåˆ—
            if mems and mems[0] in raw_reads:
                ff.write(raw_reads[mems[0]] + "\n")  # ä¼ªReference
                
                # å†™å…¥è¯¥ç°‡çš„æ‰€æœ‰reads
                cluster_read_count = 0
                for m in mems:
                    if m in raw_reads:
                        fr.write(raw_reads[m] + "\n")
                        cluster_read_count += 1
                        total_reads_written += 1
                
                fr.write("===============================\n")
                valid_count += 1
                
                if cluster_read_count > 0:
                    print(f"   ç°‡ {cid}: {cluster_read_count} reads")
    
    print(f"âœ… æ ¼å¼è½¬æ¢å®Œæˆ:")
    print(f"   æœ‰æ•ˆç°‡æ•°: {valid_count}")
    print(f"   æ€»readsæ•°: {total_reads_written}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {out_read}")
    
    return valid_count, out_read

# ==================== æ¨¡å—3: æ•°æ®éªŒè¯ ====================
def validate_generated_data(gt_path, raw_path):
    """éªŒè¯ç”Ÿæˆæ•°æ®çš„è´¨é‡"""
    try:
        # è¯»å–ground truth
        cluster_stats = collections.defaultdict(int)
        total_reads = 0
        
        with open(gt_path, 'r') as f:
            next(f)  # è·³è¿‡header
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    cluster_id = parts[1]
                    cluster_stats[cluster_id] += 1
                    total_reads += 1
        
        # è¯»å–raw reads
        raw_count = 0
        with open(raw_path, 'r') as f:
            for line in f:
                if line.strip():
                    raw_count += 1
        
        print(f"ğŸ“Š æ•°æ®éªŒè¯ç»“æœ:")
        print(f"   Ground truth reads: {total_reads}")
        print(f"   Raw reads: {raw_count}")
        print(f"   ç°‡æ•°é‡: {len(cluster_stats)}")
        print(f"   æ¯ç°‡readsæ•°åˆ†å¸ƒ: {dict(collections.Counter(cluster_stats.values()))}")
        
        return total_reads == raw_count and len(cluster_stats) > 0
        
    except Exception as e:
        print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
        return False
