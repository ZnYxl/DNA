import os
import sys
import array
from collections import defaultdict, Counter

# ================= é…ç½®åŒºåŸŸ =================
# 1. åŸå§‹å¸¦æ ‡ç­¾çš„æ•°æ®æ–‡ä»¶ (Ground Truth)
# Goldman æ•°æ®é›†çš„çœŸå®æ ‡ç­¾æ–‡ä»¶
GT_FILE = "ç»™å¸ˆå¦¹çš„cloveræ•°æ®é›†/now_goldman_tags_reads.txt"

# 2. Clover èšç±»è¾“å‡ºæ–‡ä»¶ (Prediction)
# å¯¹åº” run_real_data_Goldman.py ç”Ÿæˆçš„è¾“å‡ºä½ç½®
CLOVER_OUT_FILE = "/hy-tmp/code/CC/Step0/Experiments/Goldman_Real/02_CloverOut/clover_result_merged.txt"

# ===========================================

class CompactGroundTruth:
    def __init__(self):
        self.tag_to_id = {}    # å”¯ä¸€ Tag (str) -> int ID
        self.id_to_tag = []    # int ID -> å”¯ä¸€ Tag (str)
        # ä½¿ç”¨ 'I' (unsigned int) æ•°ç»„å­˜å‚¨ï¼Œæçœå†…å­˜
        self.read_labels = array.array('I') 
        
    def load(self, file_path):
        print(f"ğŸ“– [1/3] æ­£åœ¨åŠ è½½ Ground Truth (Goldman)...")
        if not os.path.exists(file_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
            sys.exit(1)

        with open(file_path, 'r') as f:
            for i, line in enumerate(f):
                parts = line.strip().split()
                if not parts:
                    self.read_labels.append(0)
                    continue
                
                # Goldman çš„ tag æ˜¯ç¬¬ä¸€åˆ— (ä¾‹å¦‚ "4022", "153133")
                tag = parts[0]
                
                if tag not in self.tag_to_id:
                    new_id = len(self.id_to_tag) + 1 
                    self.tag_to_id[tag] = new_id
                    self.id_to_tag.append(tag)
                
                self.read_labels.append(self.tag_to_id[tag])
                
                if (i + 1) % 5000000 == 0:
                    print(f"   å·²ç´¢å¼• {i + 1} è¡Œ...", end='\r')
                        
        print(f"\n   âœ… GT åŠ è½½å®Œæˆã€‚æ€» Reads: {len(self.read_labels)}")
        print(f"   - å”¯ä¸€ Tags æ•°: {len(self.id_to_tag)}")
        
        # æ‰“å°å†…å­˜å ç”¨
        mem_mb = self.read_labels.buffer_info()[1] * self.read_labels.itemsize / (1024*1024)
        print(f"   - æ ‡ç­¾æ•°ç»„å†…å­˜å ç”¨: {mem_mb:.2f} MB")

    def get_tag_id(self, read_idx):
        if read_idx < 0 or read_idx >= len(self.read_labels):
            return 0
        return self.read_labels[read_idx]


def stream_clover_tokens(file_path, chunk_size=1024*1024):
    """
    æµå¼è¯»å–ç”Ÿæˆå™¨ï¼š
    æ¸…æ´—æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œå¼•å·ï¼Œåªäº§å‡ºæœ‰æ•ˆçš„æ•°æ® token
    """
    with open(file_path, 'r') as f:
        buffer = ""
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            
            # å½»åº•æ¸…æ´—ï¼šå»é™¤ [] () , ' "
            cleaned_chunk = chunk.replace('[', ' ').replace(']', ' ')\
                                 .replace('(', ' ').replace(')', ' ')\
                                 .replace(',', ' ')\
                                 .replace("'", " ").replace('"', " ")
            
            buffer += cleaned_chunk
            tokens = buffer.split()
            
            # åˆ¤æ–­ buffer æœ«å°¾æ˜¯å¦å®Œæ•´
            if chunk[-1].isspace() or chunk[-1] in "[](),'\"": 
                for token in tokens:
                    yield token
                buffer = ""
            else:
                if len(tokens) > 0:
                    for token in tokens[:-1]:
                        yield token
                    buffer = tokens[-1]
                else:
                    pass
        
        if buffer.strip():
            yield buffer.strip()

def evaluate(gt_data, clover_path):
    print(f"\nğŸš€ [2/3] æµå¼å¤„ç†èšç±»ç»“æœ (Token Stream)...")
    
    cluster_stats = defaultdict(Counter)
    total_reads_clustered = 0
    
    # è·å– token æµç”Ÿæˆå™¨
    token_stream = stream_clover_tokens(clover_path)
    
    try:
        # æ¯æ¬¡å–ä¸¤ä¸ª tokenï¼š(index, cluster_id)
        while True:
            try:
                idx_token = next(token_stream)
                cid_token = next(token_stream)
            except StopIteration:
                break
            
            line_idx_1based = int(idx_token)
            cluster_id = cid_token 
            
            # è¿‡æ»¤ Clover çš„å™ªå£°æ ‡è®° (-1)
            if str(cluster_id) == '-1':
                continue
                
            # è½¬æ¢ç´¢å¼• (1-based -> 0-based)
            read_idx = line_idx_1based - 1
            
            # è·å–çœŸå® Tag ID
            true_tag_id = gt_data.get_tag_id(read_idx)
            
            if true_tag_id != 0:
                cluster_stats[str(cluster_id)][true_tag_id] += 1
                total_reads_clustered += 1
                
            if total_reads_clustered % 5000000 == 0 and total_reads_clustered > 0:
                print(f"   å·²ç»Ÿè®¡ {total_reads_clustered} ä¸ªèšç±»æˆå‘˜...", end='\r')

    except ValueError as e:
        print(f"\nâš ï¸ è§£æè­¦å‘Š: æ•°æ®æ ¼å¼å¼‚å¸¸: {e}")

    # === [3/3] ç”ŸæˆæŠ¥å‘Š ===
    print(f"\n\nğŸ“Š æ­£åœ¨æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯...")
    
    correct_reads_count = 0
    recovered_tag_ids = set()
    cluster_purities = []
    
    for cid, counts in cluster_stats.items():
        if not counts: continue
        
        # æ‰¾å‡ºè¯¥ç°‡çš„ä¸»å¯¼ Tag
        dominant_tag_id, dominant_count = counts.most_common(1)[0]
        total_in_cluster = sum(counts.values())
        
        purity = dominant_count / total_in_cluster
        cluster_purities.append(purity)
        
        correct_reads_count += dominant_count
        recovered_tag_ids.add(dominant_tag_id)

    total_unique_tags = len(gt_data.id_to_tag)
    avg_purity = sum(cluster_purities) / len(cluster_purities) if cluster_purities else 0
    micro_accuracy = correct_reads_count / total_reads_clustered if total_reads_clustered else 0
    recovery_rate = len(recovered_tag_ids) / total_unique_tags if total_unique_tags else 0
    
    print("\n" + "="*40)
    print("       ğŸ“Š CLOVER è¯„ä¼°æŠ¥å‘Š - Goldman æ•°æ®é›†")
    print("="*40)
    print(f"1. åŸå§‹ Tag æ¢å¤ç‡ (Recovery Rate):")
    print(f"   {len(recovered_tag_ids)} / {total_unique_tags}  ({recovery_rate*100:.2f}%)")
    print(f"   (æœ‰å¤šå°‘ä¸ªåŸå§‹æ–‡ä»¶è¢«æˆåŠŸæ‰¾å›)")
    
    print(f"\n2. å¹³å‡ç°‡çº¯åº¦ (Average Purity):")
    print(f"   {avg_purity*100:.2f}%")
    print(f"   (ç°‡å†…éƒ¨çš„ä¸€è‡´æ€§)")
    
    print(f"\n3. æ•´ä½“å‡†ç¡®ç‡ (Micro Accuracy):")
    print(f"   {micro_accuracy*100:.2f}%")
    
    print(f"\n4. ç»Ÿè®¡æ‘˜è¦:")
    print(f"   æœ‰æ•ˆèšç±» Reads æ•°: {total_reads_clustered}")
    print(f"   ç”Ÿæˆçš„ç°‡æ•°é‡: {len(cluster_stats)}")
    print("="*40)

if __name__ == "__main__":
    if not os.path.exists(CLOVER_OUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ° Clover è¾“å‡ºæ–‡ä»¶: {CLOVER_OUT_FILE}")
        print(f"   è¯·å…ˆè¿è¡Œ run_real_data_Goldman.py ç”Ÿæˆç»“æœã€‚")
        sys.exit(1)

    gt = CompactGroundTruth()
    gt.load(GT_FILE)
    evaluate(gt, CLOVER_OUT_FILE)