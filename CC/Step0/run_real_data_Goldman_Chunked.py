import os
import sys
import subprocess
import glob
import shutil
from collections import defaultdict

# ================= å®éªŒé…ç½® =================
DATASET_NAME = "Goldman"
SOURCE_DIR = "ç»™å¸ˆå¦¹çš„cloveræ•°æ®é›†"
SEQ_LENGTH = 117
CHUNK_SIZE = 5000000  
CLOVER_PROCESSES = 0  

# ===========================================

def load_fasta_references(fasta_path):
    print(f"ğŸ“– [Ref] è¯»å–å‚è€ƒåºåˆ—: {os.path.basename(fasta_path)} ...")
    refs = {}
    current_tag = None
    with open(fasta_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            if line.startswith(">"):
                current_tag = line[1:]
            else:
                if current_tag:
                    refs[current_tag] = line
    return refs

def process_goldman_chunked():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    exp_dir = os.path.join(current_dir, "Experiments", f"{DATASET_NAME}_Real")
    
    dir_raw = os.path.join(exp_dir, "01_FormattedInput")
    dir_clover = os.path.join(exp_dir, "02_CloverOut")
    dir_feddna = os.path.join(exp_dir, "03_FedDNA_In")
    dir_temp = os.path.join(exp_dir, "99_Temp")
    dir_chunks = os.path.join(exp_dir, "00_Chunks")
    
    for d in [dir_raw, dir_clover, dir_feddna, dir_temp, dir_chunks]:
        os.makedirs(d, exist_ok=True)

    reads_file = "now_goldman_tags_reads.txt"
    ref_file = "Goldman_fa.fasta"
    src_reads_path = os.path.join(SOURCE_DIR, reads_file)
    src_ref_path = os.path.join(SOURCE_DIR, ref_file)
    
    # === Step 1: åˆ‡ç‰‡ (åŒå‰) ===
    existing_chunks = sorted(glob.glob(os.path.join(dir_chunks, "chunk_*.txt")))
    if not existing_chunks:
        print(f"\n[Step 1] æ­£åœ¨å°†å¤§æ–‡ä»¶åˆ‡åˆ†ä¸º {CHUNK_SIZE} æ¡/å—...")
        if not os.path.exists(src_reads_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æºæ–‡ä»¶ {src_reads_path}")
            return
        chunk_idx = 0
        line_count = 0
        current_out = None
        with open(src_reads_path, 'r') as fin:
            for line in fin:
                if line_count % CHUNK_SIZE == 0:
                    if current_out: current_out.close()
                    chunk_name = os.path.join(dir_chunks, f"chunk_{chunk_idx:03d}.txt")
                    current_out = open(chunk_name, 'w')
                    print(f"   æ­£åœ¨ç”Ÿæˆåˆ‡ç‰‡: chunk_{chunk_idx:03d}.txt ...", end='\r')
                    chunk_idx += 1
                global_line_idx = line_count + 1
                parts = line.strip().split()
                if len(parts) >= 2:
                    current_out.write(f"{global_line_idx} {parts[-1]}\n")
                line_count += 1
        if current_out: current_out.close()
        print(f"\n   âœ… åˆ‡åˆ†å®Œæˆï¼å…± {line_count} æ¡ã€‚")
        existing_chunks = sorted(glob.glob(os.path.join(dir_chunks, "chunk_*.txt")))
    else:
        print(f"\n[Step 1] æ£€æµ‹åˆ° {len(existing_chunks)} ä¸ªå·²æœ‰åˆ‡ç‰‡ï¼Œè·³è¿‡åˆ‡åˆ†ã€‚")

    # === Step 2: é€å—è¿è¡Œ (Verbose Mode) ===
    print(f"\n[Step 2] å¼€å§‹é€å—è¿è¡Œ Clover (Verbose)...")
    env = os.environ.copy()
    env["PYTHONPATH"] = os.path.join(current_dir, "Clover") + os.pathsep + env.get("PYTHONPATH", "")
    
    final_clover_result = os.path.join(dir_clover, "clover_result_merged.txt")
    
    if not os.path.exists(final_clover_result):
        with open(final_clover_result, 'w') as f_merged:
            pass 
            
        for i, chunk_path in enumerate(existing_chunks):
            chunk_name = os.path.basename(chunk_path)
            chunk_out_base = os.path.join(dir_chunks, f"out_{chunk_name}")
            chunk_out_txt = chunk_out_base + ".txt"
            
            print(f"   ğŸš€ [{i+1}/{len(existing_chunks)}] å¤„ç†åˆ‡ç‰‡: {chunk_name}")
            
            if not os.path.exists(chunk_out_txt) or os.path.getsize(chunk_out_txt) == 0:
                # ã€ä¿®æ­£ã€‘ï¼šç§»é™¤äº† stdout/stderr çš„å±è”½ï¼Œè®© subprocess ç›´æ¥è¾“å‡ºåˆ°å±å¹•
                cmd = [sys.executable, "-m", "clover.main", "-I", chunk_path, 
                       "-O", chunk_out_base, "-L", str(SEQ_LENGTH), "-P", str(CLOVER_PROCESSES), "--no-tag"]
                try:
                    subprocess.run(cmd, check=True, env=env) # è¿™é‡Œä¸å†åæ‰è¾“å‡ºäº†
                except subprocess.CalledProcessError as e:
                    print(f"\nâŒ åˆ‡ç‰‡ {chunk_name} å¤„ç†å¤±è´¥ï¼")
                    print(f"   é€€å‡ºä»£ç : {e.returncode}")
                    return
            else:
                print(f"      (å·²å­˜åœ¨ï¼Œè·³è¿‡)")
            
            print(f"      æ­£åœ¨åˆå¹¶ç»“æœ...")
            with open(chunk_out_txt, 'r') as f_in, open(final_clover_result, 'a') as f_out:
                shutil.copyfileobj(f_in, f_out)
            os.remove(chunk_out_txt)
            
        print(f"   âœ… æ‰€æœ‰åˆ‡ç‰‡å¤„ç†å®Œæ¯•ã€‚")
    else:
         print(f"   âœ… æ£€æµ‹åˆ°åˆå¹¶ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡ Step 2ã€‚")

    # === Step 3: åˆå¹¶ä¸æ’åº (åŒå‰) ===
    print(f"\n[Step 3] ç”Ÿæˆ FedDNA è¾“å…¥...")
    import array
    cluster_map = array.array('i') 
    
    def stream_tokens(path):
        with open(path, 'r') as f:
            buf = ""
            while True:
                chunk = f.read(1024*1024)
                if not chunk: break
                cleaned = chunk.replace('[', ' ').replace(']', ' ')\
                               .replace('(', ' ').replace(')', ' ')\
                               .replace(',', ' ').replace("'", " ").replace('"', " ")
                buf += cleaned
                tokens = buf.split()
                if chunk[-1].isspace() or chunk[-1] in "[](),'\"":
                    for t in tokens: yield t
                    buf = ""
                else:
                    if tokens:
                        for t in tokens[:-1]: yield t
                        buf = tokens[-1]
                    else: pass
            if buf.strip(): yield buf.strip()

    if os.path.exists(final_clover_result):
        token_gen = stream_tokens(final_clover_result)
        try:
            while True:
                idx_str = next(token_gen)
                cid_str = next(token_gen)
                cluster_map.append(int(cid_str))
        except StopIteration:
            pass
        print(f"      æ˜ å°„åŠ è½½å®Œæ¯•ï¼Œå…± {len(cluster_map)} æ¡ã€‚")

        print("   [3.2] æŠ•ç¥¨ Reference...")
        cluster_votes = defaultdict(lambda: defaultdict(int)) 
        with open(src_reads_path, 'r') as f:
            valid_idx = 0
            for line in f:
                parts = line.strip().split()
                if len(parts) < 2: continue
                if valid_idx < len(cluster_map):
                    cid = cluster_map[valid_idx]
                    if cid != -1: cluster_votes[cid][parts[0]] += 1
                valid_idx += 1
                if valid_idx % 5000000 == 0: print(f"      å·²æŠ•ç¥¨ {valid_idx} æ¡...", end='\r')

        print("\n      ç»“ç®—æŠ•ç¥¨...")
        ref_dict = load_fasta_references(src_ref_path)
        cluster_ref_seqs = {} 
        for cid, votes in cluster_votes.items():
            if not votes: continue
            best_tag = max(votes, key=votes.get)
            if best_tag in ref_dict: cluster_ref_seqs[cid] = ref_dict[best_tag]
            elif best_tag.replace(">", "") in ref_dict: cluster_ref_seqs[cid] = ref_dict[best_tag.replace(">", "")]

        del cluster_votes, ref_dict
        
        print("   [3.3] ç”Ÿæˆæ’åºä¸­é—´æ–‡ä»¶...")
        temp_unsorted = os.path.join(dir_temp, "unsorted_reads.txt")
        temp_sorted = os.path.join(dir_temp, "sorted_reads.txt")
        with open(src_reads_path, 'r') as fin, open(temp_unsorted, 'w') as fout:
            valid_idx = 0
            for line in fin:
                parts = line.strip().split()
                if len(parts) < 2: continue
                if valid_idx < len(cluster_map):
                    cid = cluster_map[valid_idx]
                    if cid != -1 and cid in cluster_ref_seqs:
                        fout.write(f"{cid}\t{parts[-1]}\n")
                valid_idx += 1
        
        del cluster_map
        print("\n   [3.4] å¤–éƒ¨æ’åº...")
        subprocess.run(f"sort -n -k1,1 -S 50% {temp_unsorted} -o {temp_sorted}", shell=True, check=True)
        os.remove(temp_unsorted)

        print("   [3.5] å†™å…¥æœ€ç»ˆæ–‡ä»¶...")
        out_read = os.path.join(dir_feddna, "read.txt")
        out_ref = os.path.join(dir_feddna, "ref.txt")
        current_cid = None
        with open(temp_sorted, 'r') as fin, open(out_read, 'w') as fr, open(out_ref, 'w') as ff:
            for line in fin:
                parts = line.strip().split('\t')
                if len(parts) != 2: continue
                cid = int(parts[0])
                if cid != current_cid:
                    if current_cid is not None: fr.write("===============================\n")
                    current_cid = cid
                    ref_seq = cluster_ref_seqs[cid]
                    ff.write(ref_seq + "\n")
                    fr.write(ref_seq + "\n")
                fr.write(parts[1] + "\n")
            if current_cid is not None: fr.write("===============================\n")
        
        if os.path.exists(temp_sorted): os.remove(temp_sorted)
        print(f"\nğŸ‰ å¤§åŠŸå‘Šæˆï¼")

if __name__ == "__main__":
    process_goldman_chunked()