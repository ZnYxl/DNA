import os
import sys
import array
from collections import defaultdict, Counter

# ================= é…ç½®åŒºåŸŸ =================
# 1. åŸå§‹å¸¦æ ‡ç­¾çš„æ•°æ®æ–‡ä»¶ (Ground Truth)
# è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®
GT_FILE = "ç»™å¸ˆå¦¹çš„cloveræ•°æ®é›†/ERR036_tags_reads.txt"

# 2. Clover èšç±»è¾“å‡ºæ–‡ä»¶ (Prediction)
# è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®
CLOVER_OUT_FILE = "./Experiments/ERR036_Real/02_CloverOut/clover_result.txt"

# ===========================================

class CompactGroundTruth:
    def __init__(self):
        self.tag_to_id = {}    # å”¯ä¸€ Tag -> int ID
        self.id_to_tag = []    # int ID -> å”¯ä¸€ Tag
        # ä½¿ç”¨ 'I' (unsigned int) æ•°ç»„å­˜å‚¨ï¼Œæçœå†…å­˜
        self.read_labels = array.array('I') 
        
    def load(self, file_path):
        print(f"ğŸ“– [1/3] æ­£åœ¨åŠ è½½ Ground Truth (å†…å­˜ä¼˜åŒ–æ¨¡å¼)...")
        if not os.path.exists(file_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
            sys.exit(1)

        with open(file_path, 'r') as f:
            for i, line in enumerate(f):
                parts = line.strip().split()
                if not parts:
                    self.read_labels.append(0)
                    continue
                    
                tag = parts[0]
                if tag not in self.tag_to_id:
                    new_id = len(self.id_to_tag) + 1 
                    self.tag_to_id[tag] = new_id
                    self.id_to_tag.append(tag)
                
                self.read_labels.append(self.tag_to_id[tag])
                
                if (i + 1) % 5000000 == 0:
                    print(f"   å·²ç´¢å¼• {i + 1} è¡Œ...")
                        
        print(f"   âœ… GT åŠ è½½å®Œæˆã€‚æ€» Reads: {len(self.read_labels)}")
        # æ‰“å°å†…å­˜å ç”¨
        mem_mb = self.read_labels.buffer_info()[1] * self.read_labels.itemsize / (1024*1024)
        print(f"   - æ ‡ç­¾æ•°ç»„å†…å­˜å ç”¨: {mem_mb:.2f} MB")

    def get_tag_id(self, read_idx):
        if read_idx < 0 or read_idx >= len(self.read_labels):
            return 0
        return self.read_labels[read_idx]


def stream_clover_tokens(file_path, chunk_size=1024*1024):
    """
    æµå¼è¯»å–ç”Ÿæˆå™¨ï¼š
    ä¸ç®¡æ–‡ä»¶æ˜¯ Python List æ ¼å¼ `[(1, 2), (3, 4)]` è¿˜æ˜¯çº¯æ–‡æœ¬
    éƒ½å°†å…¶è§†ä¸ºå­—ç¬¦æµï¼Œå‰”é™¤æ ‡ç‚¹å’Œå¼•å·ï¼Œåªäº§å‡ºæœ‰æ•ˆçš„æ•°æ® token
    """
    with open(file_path, 'r') as f:
        buffer = ""
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            
            # ã€å…³é”®ä¿®æ”¹ã€‘ï¼šå¢åŠ äº†å»å•å¼•å·(')å’ŒåŒå¼•å·(")çš„é€»è¾‘
            cleaned_chunk = chunk.replace('[', ' ').replace(']', ' ')\
                                 .replace('(', ' ').replace(')', ' ')\
                                 .replace(',', ' ')\
                                 .replace("'", " ").replace('"', " ") # <--- æ–°å¢
            
            buffer += cleaned_chunk
            
            # åˆ†å‰² token
            tokens = buffer.split()
            
            if chunk[-1].isspace() or chunk[-1] in "[](),'\"": 
                # å¦‚æœå—çš„æœ«å°¾æ˜¯åˆ†éš”ç¬¦ï¼Œè¯´æ˜ tokens å…¨æ˜¯å®Œæ•´çš„
                for token in tokens:
                    yield token
                buffer = ""
            else:
                # æœ€åä¸€ä¸ª token å¯èƒ½è¢«æˆªæ–­äº†ï¼Œç•™åˆ°ä¸‹ä¸€è½®
                if len(tokens) > 0:
                    for token in tokens[:-1]:
                        yield token
                    buffer = tokens[-1]
                else:
                    pass
        
        if buffer.strip():
            yield buffer.strip()

def evaluate(gt_data, clover_path):
    print(f"\nğŸš€ [2/3] æµå¼å¤„ç†èšç±»ç»“æœ (Token Stream Fix)...")
    
    cluster_stats = defaultdict(Counter)
    total_reads_clustered = 0
    
    # è·å– token æµç”Ÿæˆå™¨
    token_stream = stream_clover_tokens(clover_path)
    
    try:
        # æ¯æ¬¡å–ä¸¤ä¸ª tokenï¼š(index, cluster_id)
        while True:
            try:
                idx_token = next(token_stream)
                cid_token = next(token_stream)
            except StopIteration:
                break
            
            # ç°åœ¨çš„ token ç»å¯¹çº¯å‡€ï¼Œå¯ä»¥ç›´æ¥è½¬ int
            line_idx_1based = int(idx_token)
            cluster_id = cid_token 
            
            # è¿‡æ»¤å™ªå£°
            if str(cluster_id) == '-1':
                continue
                
            # è½¬æ¢ç´¢å¼• (1-based -> 0-based)
            read_idx = line_idx_1based - 1
            
            # è·å–çœŸå® Tag ID
            true_tag_id = gt_data.get_tag_id(read_idx)
            
            if true_tag_id != 0:
                cluster_stats[str(cluster_id)][true_tag_id] += 1
                total_reads_clustered += 1
                
            if total_reads_clustered % 5000000 == 0 and total_reads_clustered > 0:
                print(f"   å·²ç»Ÿè®¡ {total_reads_clustered} ä¸ªèšç±»æˆå‘˜...")

    except ValueError as e:
        print(f"âš ï¸ è§£æè­¦å‘Š: æ•°æ®æ ¼å¼å¼‚å¸¸: {e}")
        # ä¸é€€å‡ºï¼Œå°è¯•ç»§ç»­ï¼Œæˆ–è€…åœ¨è¿™é‡Œåšæ–­ç‚¹è°ƒè¯•

    # === [3/3] ç”ŸæˆæŠ¥å‘Š ===
    print(f"\nğŸ“Š æ­£åœ¨æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯...")
    
    correct_reads_count = 0
    recovered_tag_ids = set()
    cluster_purities = []
    
    for cid, counts in cluster_stats.items():
        if not counts: continue
        
        # æ‰¾å‡ºè¯¥ç°‡çš„ä¸»å¯¼ Tag
        dominant_tag_id, dominant_count = counts.most_common(1)[0]
        total_in_cluster = sum(counts.values())
        
        purity = dominant_count / total_in_cluster
        cluster_purities.append(purity)
        
        correct_reads_count += dominant_count
        recovered_tag_ids.add(dominant_tag_id)

    total_unique_tags = len(gt_data.id_to_tag)
    avg_purity = sum(cluster_purities) / len(cluster_purities) if cluster_purities else 0
    micro_accuracy = correct_reads_count / total_reads_clustered if total_reads_clustered else 0
    recovery_rate = len(recovered_tag_ids) / total_unique_tags if total_unique_tags else 0
    
    print("\n" + "="*40)
    print("       ğŸ“Š CLOVER èšç±»ç»“æœè¯„ä¼°æŠ¥å‘Š (ä¿®å¤ç‰ˆ)")
    print("="*40)
    print(f"1. åŸå§‹ Tag æ¢å¤ç‡ (Recovery Rate):")
    print(f"   {len(recovered_tag_ids)} / {total_unique_tags}  ({recovery_rate*100:.2f}%)")
    
    print(f"\n2. å¹³å‡ç°‡çº¯åº¦ (Average Purity):")
    print(f"   {avg_purity*100:.2f}%")
    
    print(f"\n3. æ•´ä½“å‡†ç¡®ç‡ (Micro Accuracy):")
    print(f"   {micro_accuracy*100:.2f}%")
    
    print(f"\n4. ç»Ÿè®¡æ‘˜è¦:")
    print(f"   æœ‰æ•ˆèšç±» Reads æ•°: {total_reads_clustered}")
    print(f"   ç”Ÿæˆçš„ç°‡æ•°é‡: {len(cluster_stats)}")
    print("="*40)

if __name__ == "__main__":
    if not os.path.exists(CLOVER_OUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ° Clover è¾“å‡ºæ–‡ä»¶: {CLOVER_OUT_FILE}")
        sys.exit(1)

    gt = CompactGroundTruth()
    gt.load(GT_FILE)
    evaluate(gt, CLOVER_OUT_FILE)