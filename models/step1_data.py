# models/step1_data.py
import os
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter


class CloverDataLoader:
    """
    é€‚é…ä½ çš„æ•°æ®æ ¼å¼çš„åŠ è½½å™¨
    âœ… å·²ä¿®æ”¹æ”¯æŒè¿­ä»£é—­ç¯ï¼šå¯ä»¥åŠ è½½ refined_labels.txt
    """

    def __init__(self, experiment_dir: str, labels_path: str = None):
        """
        Args:
            experiment_dir: å®éªŒç›®å½•
            labels_path: (å¯é€‰) ä¸Šä¸€è½®ç”Ÿæˆçš„ refined_labels.txt è·¯å¾„
                         å¦‚æœä¸ä¼ ï¼Œé»˜è®¤åŠ è½½ 03_FedDNA_In/read.txt é‡Œçš„åŸå§‹æ ‡ç­¾
        """
        self.experiment_dir = experiment_dir
        self.raw_dir = os.path.join(experiment_dir, "01_RawData")
        self.feddna_dir = os.path.join(experiment_dir, "03_FedDNA_In")
        self.labels_path = labels_path  # âœ… ä¿å­˜å¤–éƒ¨æ ‡ç­¾è·¯å¾„

        # æ•°æ®å­˜å‚¨
        self.reads: List[str] = []
        self.clover_labels: List[int] = []  # Cloverèšç±»ç»“æœ (ä¼šè¢« refined labels è¦†ç›–)
        self.gt_labels: List[int] = []  # Ground Truthæ ‡ç­¾
        self.gt_cluster_seqs: Dict[int, str] = {}  # GTç°‡çš„å‚è€ƒåºåˆ—

        self._load_all_data()

    def _load_feddna_format(self) -> Tuple[List[str], List[int]]:
        """
        åŠ è½½FedDNAæ ¼å¼æ•°æ® (read.txt)
        æ ¼å¼ï¼šæŒ‰ç°‡åˆ†ç»„ï¼Œç”¨=======åˆ†éš”
        """
        read_path = os.path.join(self.feddna_dir, "read.txt")

        if not os.path.exists(read_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° read.txt: {read_path}")

        reads = []
        labels = []
        current_cluster = -1  # ä»-1å¼€å§‹ï¼Œç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦åå˜æˆ0

        print(f"ğŸ“‚ åŠ è½½FedDNAæ ¼å¼æ•°æ®: {read_path}")

        with open(read_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                # æ£€æµ‹åˆ†éš”ç¬¦
                if line.startswith("====="):
                    current_cluster += 1
                else:
                    # è¿™æ˜¯ä¸€ä¸ªreadåºåˆ—
                    reads.append(line)
                    labels.append(current_cluster)

        print(f"   âœ… åŠ è½½ {len(reads)} æ¡readsï¼Œ{current_cluster + 1} ä¸ªCloverç°‡")
        return reads, labels

    def _load_raw_reads(self) -> Dict[str, str]:
        """
        åŠ è½½åŸå§‹reads: raw_reads.txt
        æ ¼å¼: Read_ID \t Sequence
        """
        raw_path = os.path.join(self.raw_dir, "raw_reads.txt")

        if not os.path.exists(raw_path):
            print(f"   âš ï¸ raw_reads.txt ä¸å­˜åœ¨")
            return {}

        reads_dict = {}
        with open(raw_path, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    reads_dict[parts[0]] = parts[1]

        print(f"   âœ… åŠ è½½ {len(reads_dict)} æ¡åŸå§‹reads")
        return reads_dict

    def _load_read_gt(self) -> Dict[str, Tuple[int, str, str]]:
        """
        åŠ è½½Readçº§åˆ«GT: ground_truth_reads.txt
        æ ¼å¼: Read_ID \t Cluster_ID \t Ref_Seq \t Quality
        """
        gt_path = os.path.join(self.raw_dir, "ground_truth_reads.txt")

        if not os.path.exists(gt_path):
            print(f"   âš ï¸ ground_truth_reads.txt ä¸å­˜åœ¨")
            return {}

        gt_dict = {}
        with open(gt_path, 'r') as f:
            header = f.readline()  # è·³è¿‡è¡¨å¤´
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 4:
                    read_id = parts[0]
                    cluster_id = int(parts[1])
                    ref_seq = parts[2]
                    quality = parts[3]
                    gt_dict[read_id] = (cluster_id, ref_seq, quality)

        print(f"   âœ… åŠ è½½ {len(gt_dict)} æ¡Read-Level GT")
        return gt_dict

    def _load_cluster_gt(self) -> Dict[int, str]:
        """
        åŠ è½½Clusterçº§åˆ«GT: ground_truth_clusters.txt
        æ ¼å¼: Cluster_ID \t Ref_Seq
        """
        gt_path = os.path.join(self.raw_dir, "ground_truth_clusters.txt")

        if not os.path.exists(gt_path):
            print(f"   âš ï¸ ground_truth_clusters.txt ä¸å­˜åœ¨")
            return {}

        gt_dict = {}
        with open(gt_path, 'r') as f:
            header = f.readline()  # è·³è¿‡è¡¨å¤´
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    try:
                        cluster_id = int(parts[0])
                        ref_seq = parts[1]
                        gt_dict[cluster_id] = ref_seq
                    except ValueError:
                        continue

        print(f"   âœ… åŠ è½½ {len(gt_dict)} ä¸ªCluster GTåºåˆ—")
        return gt_dict

    def _build_gt_mapping(self, feddna_reads: List[str],
                          raw_reads: Dict[str, str],
                          read_gt: Dict[str, Tuple[int, str, str]]) -> List[int]:
        """
        å»ºç«‹FedDNA readsåˆ°GTæ ‡ç­¾çš„æ˜ å°„
        é€šè¿‡åºåˆ—åŒ¹é…æ‰¾åˆ°æ¯ä¸ªreadå¯¹åº”çš„GTç°‡ID
        """
        # åå‘æ˜ å°„ï¼šsequence -> read_id
        seq_to_id = {seq: rid for rid, seq in raw_reads.items()}

        gt_labels = []
        matched = 0

        for seq in feddna_reads:
            if seq in seq_to_id:
                read_id = seq_to_id[seq]
                if read_id in read_gt:
                    gt_cluster_id = read_gt[read_id][0]
                    gt_labels.append(gt_cluster_id)
                    matched += 1
                else:
                    gt_labels.append(-1)
            else:
                gt_labels.append(-1)

        print(f"   âœ… GTæ ‡ç­¾åŒ¹é…: {matched}/{len(feddna_reads)} ({matched / len(feddna_reads) * 100:.1f}%)")
        return gt_labels

    def _load_all_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("\n" + "=" * 60)
        print("ğŸ“‚ åŠ è½½å®éªŒæ•°æ®")
        print("=" * 60)

        # 1. åŠ è½½FedDNAæ ¼å¼çš„readså’ŒCloveræ ‡ç­¾ (ä½œä¸ºåŸºç¡€)
        self.reads, initial_labels = self._load_feddna_format()

        # =========================================================
        # âœ… æ ¸å¿ƒä¿®æ”¹ï¼šå°è¯•åŠ è½½ Refined Labels (ç”¨äºè¿­ä»£è®­ç»ƒ)
        # =========================================================
        if self.labels_path and os.path.exists(self.labels_path):
            print(f"\nğŸ”„ [Iterative] æ­£åœ¨åŠ è½½ Refined Labels: {self.labels_path}")
            try:
                # å‡è®¾ refined_labels.txt æ˜¯çº¯æ•°å­—ï¼Œæ¯è¡Œä¸€ä¸ª label
                # ä½¿ç”¨ numpy è¯»å–ï¼Œå› ä¸ºå®ƒæ˜¯æœ€ç¨³å¥çš„
                refined_labels = np.loadtxt(self.labels_path, dtype=int).tolist()

                if len(refined_labels) == len(self.reads):
                    self.clover_labels = refined_labels
                    print(f"   âœ… æˆåŠŸè¦†ç›–æ ‡ç­¾: {len(self.clover_labels)} æ¡")

                    # ç»Ÿè®¡ä¸€ä¸‹å˜åŒ– (ç›‘æ§è¿­ä»£æ•ˆæœ)
                    changes = sum(1 for x, y in zip(initial_labels, refined_labels) if x != y)
                    print(f"   ğŸ“‰ ä¸åˆå§‹Cloverç›¸æ¯”å˜åŒ–æ•°: {changes} ({(changes/len(initial_labels))*100:.1f}%)")
                    
                    # ç»Ÿè®¡å™ªå£°æ¯”ä¾‹
                    noise_count = sum(1 for l in refined_labels if l == -1)
                    print(f"   ğŸ—‘ï¸ å½“å‰å™ªå£°Readsæ•°: {noise_count} ({(noise_count/len(refined_labels))*100:.1f}%)")
                else:
                    print(f"   âŒ æ ‡ç­¾æ•°é‡ä¸åŒ¹é…! Reads: {len(self.reads)}, Labels: {len(refined_labels)}")
                    print("   âš ï¸ å›é€€ä½¿ç”¨åˆå§‹ Clover æ ‡ç­¾")
                    self.clover_labels = initial_labels
            except Exception as e:
                print(f"   âŒ Refined Labels åŠ è½½å¤±è´¥: {e}")
                print("   âš ï¸ å›é€€ä½¿ç”¨åˆå§‹ Clover æ ‡ç­¾")
                self.clover_labels = initial_labels
        else:
            # é»˜è®¤æƒ…å†µï¼šä½¿ç”¨åŸå§‹ Clover æ ‡ç­¾
            self.clover_labels = initial_labels
            if self.labels_path:
                print(f"   âš ï¸ æŒ‡å®šçš„æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {self.labels_path}ï¼Œå·²å›é€€åˆ°é»˜è®¤")

        # 2. åŠ è½½åŸå§‹æ•°æ®å’ŒGT
        raw_reads = self._load_raw_reads()
        read_gt = self._load_read_gt()
        self.gt_cluster_seqs = self._load_cluster_gt()

        # 3. å»ºç«‹GTæ˜ å°„
        if raw_reads and read_gt:
            self.gt_labels = self._build_gt_mapping(self.reads, raw_reads, read_gt)
        else:
            self.gt_labels = [-1] * len(self.reads)
            print(f"   âš ï¸ æ— æ³•å»ºç«‹GTæ˜ å°„ï¼Œä½¿ç”¨-1å¡«å……")

        print(f"\nğŸ“Š æ•°æ®æ‘˜è¦:")
        print(f"   - æ€»reads: {len(self.reads)}")
        print(f"   - å½“å‰ä½¿ç”¨ç°‡æ•°: {len(set(self.clover_labels))}")
        print(f"   - GTç°‡æ•°: {len(self.gt_cluster_seqs)}")
        print(f"   - åºåˆ—é•¿åº¦èŒƒå›´: {min(len(r) for r in self.reads)} - {max(len(r) for r in self.reads)}")


def seq_to_onehot(seq: str, max_len: int = 150) -> torch.Tensor:
    """DNAåºåˆ—è½¬one-hotç¼–ç """
    base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 0}  # Nå½“ä½œAå¤„ç†

    # å¡«å……æˆ–æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
    seq_padded = seq.ljust(max_len, 'N')[:max_len]

    # è½¬æ¢ä¸ºç´¢å¼•
    indices = [base_to_idx.get(base.upper(), 0) for base in seq_padded]

    # è½¬one-hot
    onehot = torch.zeros(max_len, 4)
    for i, idx in enumerate(indices):
        onehot[i, idx] = 1.0

    return onehot


class Step1Dataset(Dataset):
    """
    æ­¥éª¤ä¸€çš„æ•°æ®é›†
    """

    def __init__(self, data_loader: CloverDataLoader, max_len: int = 150):
        self.data_loader = data_loader
        self.max_len = max_len

        # è¿‡æ»¤æ‰å™ªå£°reads (Cloveræ ‡ç­¾ä¸º-1çš„)
        # æ³¨æ„ï¼šè¿™é‡Œçš„ self.data_loader.clover_labels å¯èƒ½å·²ç»æ˜¯ refined è¿‡çš„æ ‡ç­¾äº†
        self.valid_indices = [i for i, label in enumerate(data_loader.clover_labels) if label >= 0]

        print(f"ğŸ“Š Datasetç»Ÿè®¡:")
        print(f"   - æœ‰æ•ˆreads (Label != -1): {len(self.valid_indices)}/{len(data_loader.reads)}")
        print(f"   - å™ªå£°reads (è¢«è¿‡æ»¤): {len(data_loader.reads) - len(self.valid_indices)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]

        seq = self.data_loader.reads[real_idx]
        clover_label = self.data_loader.clover_labels[real_idx]
        gt_label = self.data_loader.gt_labels[real_idx]

        # åºåˆ—ç¼–ç 
        encoding = seq_to_onehot(seq, self.max_len)

        return {
            'encoding': encoding,  # (L, 4)
            'clover_label': clover_label,  # Cloverèšç±»æ ‡ç­¾ (å¯èƒ½æ˜¯ refined çš„)
            'gt_label': gt_label,  # Ground Truthæ ‡ç­¾
            'read_idx': real_idx,  # åŸå§‹ç´¢å¼•
            'sequence': seq  # åŸå§‹åºåˆ—
        }


def create_cluster_balanced_sampler(dataset: Step1Dataset,
                                    batch_size: int = 32,
                                    max_clusters_per_batch: int = 5) -> List[List[int]]:
    """
    åˆ›å»ºç°‡å¹³è¡¡çš„batché‡‡æ ·å™¨
    ç¡®ä¿æ¯ä¸ªbatchåŒ…å«å¤šä¸ªç°‡ï¼Œä½†ä¸ä¼šå¤ªå¤šï¼ˆé¿å…å†…å­˜çˆ†ç‚¸ï¼‰
    """
    # æŒ‰Cloveræ ‡ç­¾åˆ†ç»„
    cluster_to_indices = defaultdict(list)
    for idx in range(len(dataset)):
        item = dataset[idx]
        cluster_label = item['clover_label']
        cluster_to_indices[cluster_label].append(idx)

    print(f"ğŸ“Š ç°‡åˆ†å¸ƒ (Top 10):")
    cluster_sizes = [(cid, len(indices)) for cid, indices in cluster_to_indices.items()]
    cluster_sizes.sort(key=lambda x: x[1], reverse=True)

    for i, (cid, size) in enumerate(cluster_sizes[:10]):  # æ˜¾ç¤ºå‰10ä¸ªæœ€å¤§çš„ç°‡
        print(f"   ç°‡{cid}: {size} reads")
    if len(cluster_sizes) > 10:
        print(f"   ... è¿˜æœ‰ {len(cluster_sizes) - 10} ä¸ªç°‡")

    # ç”Ÿæˆbatch
    batches = []
    cluster_ids = list(cluster_to_indices.keys())
    np.random.shuffle(cluster_ids)

    while cluster_ids:
        # éšæœºé€‰æ‹©å‡ ä¸ªç°‡
        num_clusters = min(max_clusters_per_batch, len(cluster_ids))
        selected_clusters = np.random.choice(cluster_ids, size=num_clusters, replace=False)

        # ä»é€‰ä¸­çš„ç°‡ä¸­é‡‡æ ·reads
        batch_indices = []
        for cluster_id in selected_clusters:
            cluster_indices = cluster_to_indices[cluster_id]

            # æ¯ä¸ªç°‡è´¡çŒ®çš„readsæ•°é‡
            reads_per_cluster = batch_size // num_clusters
            sample_size = min(reads_per_cluster, len(cluster_indices))

            if sample_size > 0:
                sampled = np.random.choice(cluster_indices, size=sample_size, replace=False)
                batch_indices.extend(sampled)

                # ç§»é™¤å·²ä½¿ç”¨çš„indices
                for idx in sampled:
                    cluster_to_indices[cluster_id].remove(idx)

        # ç§»é™¤ç©ºç°‡
        cluster_ids = [cid for cid in cluster_ids if len(cluster_to_indices[cid]) > 0]

        if batch_indices:
            batches.append(batch_indices)

    print(f"ğŸ“¦ ç”Ÿæˆ {len(batches)} ä¸ªbatch")
    return batches