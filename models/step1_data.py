# models/step1_data.py
import os
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter, deque  # âœ… æ–°å¢ deque


class CloverDataLoader:
    """
    é€‚é…ä½ çš„æ•°æ®æ ¼å¼çš„åŠ è½½å™¨
    âœ… å·²ä¿®æ”¹æ”¯æŒè¿­ä»£é—­ç¯ï¼šå¯ä»¥åŠ è½½ refined_labels.txt
    """

    def __init__(self, experiment_dir: str, labels_path: str = None):
        self.experiment_dir = experiment_dir
        self.raw_dir = os.path.join(experiment_dir, "01_RawData")

        feddna_subdir = os.path.join(experiment_dir, "03_FedDNA_In")
        if os.path.exists(feddna_subdir):
            self.feddna_dir = feddna_subdir
        else:
            self.feddna_dir = experiment_dir
            print(f"   â„¹ï¸ ä½¿ç”¨éæ ‡å‡†ç›®å½•ç»“æ„ (read.txt ç›´æ¥åœ¨æ ¹ç›®å½•)")

        self.labels_path = labels_path

        self.reads: List[str] = []
        self.clover_labels: List[int] = []
        self.gt_labels: List[int] = []
        self.gt_cluster_seqs: Dict[int, str] = {}

        self._load_all_data()

    def _load_feddna_format(self) -> Tuple[List[str], List[int]]:
        read_path = os.path.join(self.feddna_dir, "read.txt")

        if not os.path.exists(read_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° read.txt: {read_path}")

        reads = []
        labels = []
        current_cluster = -1

        print(f"ğŸ“‚ åŠ è½½FedDNAæ ¼å¼æ•°æ®: {read_path}")

        with open(read_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                if line.startswith("====="):
                    current_cluster += 1
                else:
                    reads.append(line)
                    labels.append(current_cluster)

        print(f"   âœ… åŠ è½½ {len(reads)} æ¡readsï¼Œ{current_cluster + 1} ä¸ªCloverç°‡")
        return reads, labels

    def _load_raw_reads(self) -> Dict[str, str]:
        raw_path = os.path.join(self.raw_dir, "raw_reads.txt")
        if not os.path.exists(raw_path):
            print(f"   âš ï¸ raw_reads.txt ä¸å­˜åœ¨")
            return {}

        reads_dict = {}
        with open(raw_path, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    reads_dict[parts[0]] = parts[1]

        print(f"   âœ… åŠ è½½ {len(reads_dict)} æ¡åŸå§‹reads")
        return reads_dict

    def _load_read_gt(self) -> Dict[str, Tuple[int, str, str]]:
        gt_path = os.path.join(self.raw_dir, "ground_truth_reads.txt")
        if not os.path.exists(gt_path):
            print(f"   âš ï¸ ground_truth_reads.txt ä¸å­˜åœ¨")
            return {}

        gt_dict = {}
        with open(gt_path, 'r') as f:
            header = f.readline()
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 4:
                    gt_dict[parts[0]] = (int(parts[1]), parts[2], parts[3])

        print(f"   âœ… åŠ è½½ {len(gt_dict)} æ¡Read-Level GT")
        return gt_dict

    def _load_cluster_gt(self) -> Dict[int, str]:
        gt_path = os.path.join(self.raw_dir, "ground_truth_clusters.txt")
        if not os.path.exists(gt_path):
            print(f"   âš ï¸ ground_truth_clusters.txt ä¸å­˜åœ¨")
            return {}

        gt_dict = {}
        with open(gt_path, 'r') as f:
            header = f.readline()
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    try:
                        gt_dict[int(parts[0])] = parts[1]
                    except ValueError:
                        continue

        print(f"   âœ… åŠ è½½ {len(gt_dict)} ä¸ªCluster GTåºåˆ—")
        return gt_dict

    def _build_gt_mapping(self, feddna_reads: List[str],
                          raw_reads: Dict[str, str],
                          read_gt: Dict[str, Tuple[int, str, str]]) -> List[int]:
        seq_to_id = {seq: rid for rid, seq in raw_reads.items()}

        gt_labels = []
        matched = 0

        for seq in feddna_reads:
            if seq in seq_to_id:
                read_id = seq_to_id[seq]
                if read_id in read_gt:
                    gt_labels.append(read_gt[read_id][0])
                    matched += 1
                else:
                    gt_labels.append(-1)
            else:
                gt_labels.append(-1)

        print(f"   âœ… GTæ ‡ç­¾åŒ¹é…: {matched}/{len(feddna_reads)} ({matched / len(feddna_reads) * 100:.1f}%)")
        return gt_labels

    def _load_all_data(self):
        print("\n" + "=" * 60)
        print("ğŸ“‚ åŠ è½½å®éªŒæ•°æ®")
        print("=" * 60)

        self.reads, initial_labels = self._load_feddna_format()

        if self.labels_path and os.path.exists(self.labels_path):
            print(f"\nğŸ”„ [Iterative] æ­£åœ¨åŠ è½½ Refined Labels: {self.labels_path}")
            try:
                refined_labels = np.loadtxt(self.labels_path, dtype=int).tolist()

                if len(refined_labels) == len(self.reads):
                    self.clover_labels = refined_labels
                    print(f"   âœ… æˆåŠŸè¦†ç›–æ ‡ç­¾: {len(self.clover_labels)} æ¡")

                    changes = sum(1 for x, y in zip(initial_labels, refined_labels) if x != y)
                    print(f"   ğŸ“‰ ä¸åˆå§‹Cloverç›¸æ¯”å˜åŒ–æ•°: {changes} ({changes / len(initial_labels) * 100:.1f}%)")

                    noise_count = sum(1 for l in refined_labels if l == -1)
                    print(f"   ğŸ—‘ï¸ å½“å‰å™ªå£°Readsæ•°: {noise_count} ({noise_count / len(refined_labels) * 100:.1f}%)")
                else:
                    print(f"   âŒ æ ‡ç­¾æ•°é‡ä¸åŒ¹é…! Reads: {len(self.reads)}, Labels: {len(refined_labels)}")
                    print("   âš ï¸ å›é€€ä½¿ç”¨åˆå§‹ Clover æ ‡ç­¾")
                    self.clover_labels = initial_labels
            except Exception as e:
                print(f"   âŒ Refined Labels åŠ è½½å¤±è´¥: {e}")
                print("   âš ï¸ å›é€€ä½¿ç”¨åˆå§‹ Clover æ ‡ç­¾")
                self.clover_labels = initial_labels
        else:
            self.clover_labels = initial_labels
            if self.labels_path:
                print(f"   âš ï¸ æŒ‡å®šçš„æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {self.labels_path}ï¼Œå·²å›é€€åˆ°é»˜è®¤")

        raw_reads = self._load_raw_reads()
        read_gt = self._load_read_gt()
        self.gt_cluster_seqs = self._load_cluster_gt()

        if raw_reads and read_gt:
            self.gt_labels = self._build_gt_mapping(self.reads, raw_reads, read_gt)
        else:
            self.gt_labels = [-1] * len(self.reads)
            print(f"   âš ï¸ æ— æ³•å»ºç«‹GTæ˜ å°„ï¼Œä½¿ç”¨-1å¡«å……")

        print(f"\nğŸ“Š æ•°æ®æ‘˜è¦:")
        print(f"   - æ€»reads: {len(self.reads)}")
        print(f"   - å½“å‰ä½¿ç”¨ç°‡æ•°: {len(set(self.clover_labels))}")


def seq_to_onehot(seq: str, max_len: int = 150) -> torch.Tensor:
    base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 0}
    seq_padded = seq.ljust(max_len, 'N')[:max_len]
    indices = [base_to_idx.get(base.upper(), 0) for base in seq_padded]
    onehot = torch.zeros(max_len, 4)
    for i, idx in enumerate(indices):
        onehot[i, idx] = 1.0
    return onehot


class Step1Dataset(Dataset):
    def __init__(self, data_loader: CloverDataLoader, max_len: int = 150):
        self.data_loader = data_loader
        self.max_len = max_len
        self.valid_indices = [i for i, label in enumerate(data_loader.clover_labels) if label >= 0]

        print(f"ğŸ“Š Datasetç»Ÿè®¡:")
        print(f"   - æœ‰æ•ˆreads (Label != -1): {len(self.valid_indices)}/{len(data_loader.reads)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        seq = self.data_loader.reads[real_idx]
        clover_label = self.data_loader.clover_labels[real_idx]
        gt_label = self.data_loader.gt_labels[real_idx]
        encoding = seq_to_onehot(seq, self.max_len)

        return {
            'encoding': encoding,
            'clover_label': clover_label,
            'gt_label': gt_label,
            'read_idx': real_idx,
            'sequence': seq
        }


# ===========================================================================
# ğŸš€ æ€§èƒ½ä¿®å¤ç‰ˆé‡‡æ ·å™¨
# ===========================================================================
def create_cluster_balanced_sampler(dataset: Step1Dataset,
                                    batch_size: int = 32,
                                    max_clusters_per_batch: int = 5) -> List[List[int]]:
    """
    æ€§èƒ½ä¿®å¤ç‰ˆï¼šä½¿ç”¨ deque æ›¿ä»£ list(set)
    è§£å†³å¤§è§„æ¨¡ç°‡æ•°é‡ä¸‹æ­»å¾ªç¯/å¡æ­»é—®é¢˜
    """
    print("   ğŸ”¨ æ­£åœ¨æ„å»ºé‡‡æ ·å™¨ (Queueä¼˜åŒ–ç‰ˆ)...")

    valid_indices = dataset.valid_indices
    all_labels = dataset.data_loader.clover_labels

    cluster_to_indices = defaultdict(list)
    for idx, real_idx in enumerate(valid_indices):
        label = all_labels[real_idx]
        cluster_to_indices[label].append(idx)

    for cid in cluster_to_indices:
        np.random.shuffle(cluster_to_indices[cid])

    cluster_ptrs = {cid: 0 for cid in cluster_to_indices}

    print(f"   ğŸ“Š ç°‡åˆ†å¸ƒ (Top 5):")
    cluster_sizes = [(cid, len(indices)) for cid, indices in cluster_to_indices.items()]
    cluster_sizes.sort(key=lambda x: x[1], reverse=True)
    for i, (cid, size) in enumerate(cluster_sizes[:5]):
        print(f"      ç°‡{cid}: {size}")

    batches = []

    # âœ… ä¼˜åŒ–ç‚¹ 1: åˆå§‹åŒ–ä¸€æ¬¡åˆ—è¡¨å¹¶æ‰“ä¹±
    cluster_ids = list(cluster_to_indices.keys())
    np.random.shuffle(cluster_ids)

    # âœ… ä¼˜åŒ–ç‚¹ 2: ä½¿ç”¨åŒç«¯é˜Ÿåˆ— (Deque)
    active_queue = deque(cluster_ids)

    while active_queue:
        # æ¯æ¬¡ä»é˜Ÿåˆ—å¤´å– max_clusters_per_batch ä¸ª
        num_to_select = min(max_clusters_per_batch, len(active_queue))
        selected_clusters = []
        for _ in range(num_to_select):
            selected_clusters.append(active_queue.popleft())

        batch_indices = []
        reads_per_cluster = max(1, batch_size // num_to_select)

        # è®°å½•è¿˜æœ‰å‰©ä½™æ•°æ®çš„ç°‡ï¼Œç¨åæ”¾å›é˜Ÿåˆ—å°¾éƒ¨
        clusters_to_keep = []

        for cluster_id in selected_clusters:
            indices = cluster_to_indices[cluster_id]
            ptr = cluster_ptrs[cluster_id]
            remaining = len(indices) - ptr
            take = min(reads_per_cluster, remaining)

            if take > 0:
                batch_indices.extend(indices[ptr: ptr + take])
                cluster_ptrs[cluster_id] += take
                # å¦‚æœè¿˜æœ‰å‰©ä½™ï¼ŒåŠ å…¥ä¿ç•™åˆ—è¡¨
                if cluster_ptrs[cluster_id] < len(indices):
                    clusters_to_keep.append(cluster_id)
            else:
                # ç†è®ºä¸Šä¸åº”è¿›å…¥è¿™é‡Œï¼Œä½†ä¸ºäº†ä¿é™©
                pass

        # âœ… å°†æœªæ¶ˆè€—å®Œçš„ç°‡æ”¾å›é˜Ÿåˆ—å°¾éƒ¨ (Round Robin)
        for cid in clusters_to_keep:
            active_queue.append(cid)

        if batch_indices:
            batches.append(batch_indices)

    print(f"   ğŸ“¦ ç”Ÿæˆ {len(batches)} ä¸ªBatchï¼Œå‡†å¤‡å°±ç»ªï¼")
    return batches


def create_dynamic_sampler(dataset: Step1Dataset,
                           batch_size: int = 32,
                           max_clusters_per_batch: int = 5,
                           state_path: str = None,
                           round_idx: int = 1) -> List[List[int]]:
    """
    åŠ¨æ€é‡‡æ ·å™¨ï¼ˆåŒæ ·åº”ç”¨ Queue ä¼˜åŒ–ï¼‰
    """
    # Round 1: æ—  stateï¼Œç›´æ¥å…¨é‡
    if round_idx <= 1 or state_path is None or not os.path.exists(state_path):
        print("   ğŸ“¦ Round 1 / æ—  stateï¼Œä½¿ç”¨å…¨é‡é‡‡æ ·")
        return create_cluster_balanced_sampler(
            dataset, batch_size=batch_size,
            max_clusters_per_batch=max_clusters_per_batch
        )

    # ---- Round 2+: è¯» stateï¼ŒæŒ‰ä¸‰åŒºåˆ¶è¿‡æ»¤ ----
    print(f"   ğŸ“¦ Round {round_idx}: è¯»å– read_state.ptï¼ŒæŒ‰ä¸‰åŒºåˆ¶é‡‡æ ·...")
    state = torch.load(state_path, map_location='cpu')
    zone_ids_full = state['zone_ids']

    valid_indices = dataset.valid_indices
    all_labels = dataset.data_loader.clover_labels

    kept_indices = []

    # å‚æ•°ï¼šZone I æŠ½æ ·ç‡
    ZONE1_SAMPLE_RATE = 0.20

    n_z1, n_z2, n_z3_dropped, n_z1_dropped = 0, 0, 0, 0

    for ds_idx, real_idx in enumerate(valid_indices):
        zone = int(zone_ids_full[real_idx])

        if zone == 3:
            n_z3_dropped += 1
            continue
        elif zone == 1:
            if np.random.random() < ZONE1_SAMPLE_RATE:
                kept_indices.append(ds_idx)
                n_z1 += 1
            else:
                n_z1_dropped += 1
        elif zone == 2:
            kept_indices.append(ds_idx)
            n_z2 += 1
        else:
            continue

    print(f"   ğŸ“Š åŠ¨æ€é‡‡æ ·ç»Ÿè®¡:")
    print(f"      Zone I  ä¿ç•™: {n_z1:>7d}  (ä¸¢å¼ƒ {n_z1_dropped})")
    print(f"      Zone II ä¿ç•™: {n_z2:>7d}")
    print(f"      Zone III ä¸¢å¼ƒ:{n_z3_dropped:>7d}")
    print(f"      æ€»ä¿ç•™:       {len(kept_indices)}")

    if len(kept_indices) == 0:
        print("   âš ï¸ åŠ¨æ€é‡‡æ ·åæ— æ•°æ®ï¼Œå›é€€åˆ°å…¨é‡é‡‡æ ·")
        return create_cluster_balanced_sampler(
            dataset, batch_size=batch_size,
            max_clusters_per_batch=max_clusters_per_batch
        )

    # ---- ç”¨ä¿ç•™çš„ idx æ„å»º cluster-balanced batches (Queue ä¼˜åŒ–ç‰ˆ) ----
    cluster_to_indices = defaultdict(list)
    for ds_idx in kept_indices:
        real_idx = valid_indices[ds_idx]
        label = all_labels[real_idx]
        cluster_to_indices[label].append(ds_idx)

    for cid in cluster_to_indices:
        np.random.shuffle(cluster_to_indices[cid])

    cluster_ptrs = {cid: 0 for cid in cluster_to_indices}

    # âœ… åˆå§‹åŒ–é˜Ÿåˆ—
    cluster_ids = list(cluster_to_indices.keys())
    np.random.shuffle(cluster_ids)
    active_queue = deque(cluster_ids)

    batches = []
    while active_queue:
        num_sel = min(max_clusters_per_batch, len(active_queue))
        selected = []
        for _ in range(num_sel):
            selected.append(active_queue.popleft())

        batch = []
        per_cluster = max(1, batch_size // num_sel)

        clusters_to_keep = []

        for cid in selected:
            indices = cluster_to_indices[cid]
            ptr = cluster_ptrs[cid]
            rem = len(indices) - ptr
            take = min(per_cluster, rem)

            if take > 0:
                batch.extend(indices[ptr: ptr + take])
                cluster_ptrs[cid] += take
                if cluster_ptrs[cid] < len(indices):
                    clusters_to_keep.append(cid)

        for cid in clusters_to_keep:
            active_queue.append(cid)

        if batch:
            batches.append(batch)

    print(f"   ğŸ“¦ ç”Ÿæˆ {len(batches)} ä¸ªBatchï¼ˆåŠ¨æ€é‡‡æ ·ç‰ˆï¼‰")
    return batches