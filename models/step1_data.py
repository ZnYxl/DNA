# models/step1_data.py
"""
ä¿®å¤æ¸…å•:
  [FIX-#1]  æ–°å¢ inference_modeï¼ŒStep2 æ¨ç†ä¸å†å— training_cap é™åˆ¶
  [FIX-#8]  æ¯è½®é‡‡æ ·ç§å­åŠ å…¥ round_idx
  [OPT]     numpy å‘é‡åŒ– one-hot ç¼–ç  (5-10x åŠ é€Ÿ)
  [NEW]     GT æ ‡ç­¾åŠ è½½ (id20 åºåˆ—åŒ¹é…)
  [NEW]     training_cap å¯é€šè¿‡ args é…ç½®
"""
import os
import torch
import numpy as np
from torch.utils.data import Dataset
from typing import Dict, List, Optional
from collections import defaultdict

# ============================================================
# é«˜æ€§èƒ½ One-Hot ç¼–ç  (numpy å‘é‡åŒ–)
# ============================================================
_BASE_LUT = np.zeros(256, dtype=np.int64)
for _c, _i in [('A',0),('C',1),('G',2),('T',3),
               ('a',0),('c',1),('g',2),('t',3),
               ('N',0),('n',0)]:
    _BASE_LUT[ord(_c)] = _i


def seq_to_onehot(seq: str, max_len: int) -> torch.Tensor:
    """numpy å‘é‡åŒ–ç‰ˆæœ¬ï¼Œæ¯”é€å­—ç¬¦å¾ªç¯å¿« 5-10 å€"""
    L = min(len(seq), max_len)
    byte_arr = np.frombuffer(seq[:L].encode('ascii'), dtype=np.uint8)
    indices = _BASE_LUT[byte_arr]
    tensor = torch.zeros(max_len, 4)
    tensor[np.arange(L), indices] = 1.0
    return tensor


# ============================================================
# æ•°æ®åŠ è½½å™¨ (é€šç”¨: Goldman / id20 / ERR036)
# ============================================================
class CloverDataLoader:
    def __init__(self, experiment_dir: str, labels_path: str = None):
        self.experiment_dir = experiment_dir

        feddna_subdir = os.path.join(experiment_dir, "03_FedDNA_In")
        self.feddna_dir = feddna_subdir if os.path.exists(feddna_subdir) else experiment_dir
        self.labels_path = labels_path

        self.reads: List[str] = []
        self.clover_labels: List[int] = []
        self.gt_labels: List[int] = []
        self.gt_cluster_seqs: Dict[int, str] = {}

        self._load_all_data()

    def _load_all_data(self):
        print("\n" + "=" * 60)
        print("ğŸ“‚ [DataLoader] Loading Data...")
        print("=" * 60)

        read_path = os.path.join(self.feddna_dir, "read.txt")
        if not os.path.exists(read_path):
            raise FileNotFoundError(f"Missing: {read_path}")

        with open(read_path, 'r') as f:
            current_cluster = -1
            for line in f:
                line = line.strip()
                if not line:
                    continue
                if line.startswith("====="):
                    current_cluster += 1
                else:
                    self.reads.append(line)
                    self.clover_labels.append(current_cluster)

        print(f"   âœ… Reads Loaded: {len(self.reads)}")
        print(f"   âœ… Clusters:     {current_cluster + 1}")

        # åŠ è½½ Refined Labels (Round 2+)
        if self.labels_path and os.path.exists(self.labels_path):
            print(f"   ğŸ”„ Loading Refined Labels...")
            try:
                refined = np.loadtxt(self.labels_path, dtype=int).tolist()
                if len(refined) == len(self.reads):
                    self.clover_labels = refined
                    noise_cnt = sum(1 for x in refined if x < 0)
                    print(f"   âœ… Labels Updated. Noise: {noise_cnt}")
                else:
                    print(f"   âš ï¸ Length Mismatch ({len(refined)} vs {len(self.reads)}). Keeping original.")
            except Exception as e:
                print(f"   âš ï¸ Failed to load labels: {e}")

        # é»˜è®¤ GT = -1 (Goldman æ¨¡å¼)
        self.gt_labels = [-1] * len(self.reads)
        print(f"   â„¹ï¸ GT Labels initialized to -1 (default)")

    def load_gt_tags(self, gt_tags_file: str):
        """
        ä» GT æ ‡ç­¾æ–‡ä»¶åŠ è½½ (id20 æ ¼å¼: æ¯è¡Œ "tag_id sequence")
        é€šè¿‡åºåˆ—å‰ç¼€åŒ¹é… read.txt ä¸­çš„ reads
        """
        if not gt_tags_file or not os.path.exists(gt_tags_file):
            print(f"   âš ï¸ GT æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸º None")
            return

        print(f"   ğŸ“‹ Loading GT tags: {os.path.basename(gt_tags_file)}")

        PREFIX_LEN = 80
        seq_to_tag: Dict[str, int] = {}
        total_lines = 0
        with open(gt_tags_file, 'r') as f:
            for line in f:
                parts = line.strip().split(maxsplit=1)
                if len(parts) < 2:
                    continue
                tag_id = int(parts[0])
                seq = parts[1].strip()
                key = seq[:PREFIX_LEN]
                seq_to_tag[key] = tag_id
                total_lines += 1

        matched = 0
        for i, read in enumerate(self.reads):
            key = read[:PREFIX_LEN]
            tag = seq_to_tag.get(key, -1)
            self.gt_labels[i] = tag
            if tag >= 0:
                matched += 1

        rate = matched / max(len(self.reads), 1) * 100
        print(f"      GT æ¡ç›®: {total_lines}, åŒ¹é…: {matched}/{len(self.reads)} ({rate:.1f}%)")


# ============================================================
# Dataset (é€šç”¨)
# ============================================================
class Step1Dataset(Dataset):
    def __init__(self, data_loader: CloverDataLoader, max_len: int = 150,
                 training_cap: int = 2000000,
                 inference_mode: bool = False,
                 round_idx: int = 1):
        """
        Args:
            training_cap:   è®­ç»ƒæ—¶çš„æ ·æœ¬ä¸Šé™
            inference_mode: [FIX-#1] True æ—¶å¿½ç•¥ training_capï¼Œä½¿ç”¨å…¨éƒ¨æœ‰æ•ˆæ ·æœ¬
            round_idx:      [FIX-#8] è½®æ¬¡ç´¢å¼•ï¼Œç”¨äºå˜åŒ–é‡‡æ ·ç§å­
        """
        self.data_loader = data_loader
        self.max_len = max_len

        full_valid_indices = [i for i, label in enumerate(data_loader.clover_labels) if label >= 0]
        total_valid = len(full_valid_indices)

        if inference_mode:
            self.valid_indices = full_valid_indices
            print(f"   ğŸ“Š Inference Mode: {len(self.valid_indices)} samples (å…¨é‡)")
        elif total_valid > training_cap:
            seed = 42 + round_idx
            rng = np.random.RandomState(seed)
            self.valid_indices = rng.choice(
                full_valid_indices, training_cap, replace=False
            ).tolist()
            print(f"   âœ‚ï¸ Training: {total_valid} â†’ {training_cap} (seed={seed})")
        else:
            self.valid_indices = full_valid_indices

        print(f"   ğŸ“Š Dataset Size: {len(self.valid_indices)} samples")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        seq = self.data_loader.reads[real_idx]
        encoding = seq_to_onehot(seq, self.max_len)

        return {
            'encoding': encoding,
            'clover_label': self.data_loader.clover_labels[real_idx],
            'gt_label': self.data_loader.gt_labels[real_idx],
            'read_idx': real_idx,
        }


# ============================================================
# é‡‡æ ·å™¨ (é€šç”¨, è´ªå¿ƒå¡«å……ç‰ˆ)
#
# [OPT] id20 æœ‰ 756K ç°‡å¹³å‡ä»… 2.6 reads/ç°‡
#       æ—§ç‰ˆå›ºå®š max_clusters_per_batch=8 â†’ batchâ‰ˆ21 â†’ 79K batches â†’ ææ…¢
#       æ–°ç‰ˆè´ªå¿ƒå¡«å……: ä¸æ–­å–ç°‡ç›´åˆ°å¡«æ»¡ batch_sizeï¼Œå¤§å¹…å‡å°‘ batch æ•°
# ============================================================
def create_cluster_balanced_sampler(dataset: Step1Dataset,
                                    batch_size: int = 256,
                                    max_clusters_per_batch: int = 64):
    """
    è´ªå¿ƒå¡«å……ç­–ç•¥:
      1. æ‰“ä¹±ç°‡é¡ºåº
      2. é€ä¸ªç°‡å€’å…¥å½“å‰ batchï¼Œç›´åˆ° batch_size æ»¡æˆ–è¾¾åˆ° max_clusters ä¸Šé™
      3. å¤§ç°‡(>batch_size)å•ç‹¬åˆ‡ç‰‡
    """
    print("   ğŸ”¨ æ„å»ºé‡‡æ ·å™¨ (è´ªå¿ƒå¡«å……)...")
    valid_indices = dataset.valid_indices
    all_labels = dataset.data_loader.clover_labels

    cluster_to_indices = defaultdict(list)
    for idx, real_idx in enumerate(valid_indices):
        label = all_labels[real_idx]
        cluster_to_indices[label].append(idx)

    # è¿‡æ»¤ç©ºç°‡å’Œå•æ¡ç°‡ (å¯¹æ¯”å­¦ä¹ è‡³å°‘éœ€è¦2æ¡)
    valid_clusters = {cid: idxs for cid, idxs in cluster_to_indices.items()
                      if len(idxs) >= 2}
    singleton_indices = [idxs[0] for idxs in cluster_to_indices.values()
                         if len(idxs) == 1]

    cluster_ids = list(valid_clusters.keys())
    np.random.shuffle(cluster_ids)

    batches = []
    current_batch = []
    current_n_clusters = 0

    for cid in cluster_ids:
        idxs = valid_clusters[cid]

        # å¤§ç°‡: åˆ‡ç‰‡å¤„ç†
        if len(idxs) > batch_size:
            # å…ˆ flush å½“å‰ batch
            if current_batch:
                batches.append(current_batch)
                current_batch = []
                current_n_clusters = 0
            # åˆ‡ç‰‡
            for i in range(0, len(idxs), batch_size):
                batches.append(idxs[i:i + batch_size])
            continue

        # èƒ½æ”¾è¿›å½“å‰ batch å—ï¼Ÿ
        if (len(current_batch) + len(idxs) > batch_size or
                current_n_clusters >= max_clusters_per_batch):
            # flush
            if current_batch:
                batches.append(current_batch)
            current_batch = list(idxs)
            current_n_clusters = 1
        else:
            current_batch.extend(idxs)
            current_n_clusters += 1

    # flush æœ€åä¸€ä¸ª batch
    if current_batch:
        batches.append(current_batch)

    # å•æ¡ reads: åˆå¹¶æˆå¤§ batch (å®ƒä»¬ä¸å‚ä¸å¯¹æ¯”ä½†å‚ä¸é‡å»ºæŸå¤±)
    if singleton_indices:
        for i in range(0, len(singleton_indices), batch_size):
            batches.append(singleton_indices[i:i + batch_size])

    # ç»Ÿè®¡
    sizes = [len(b) for b in batches]
    avg_size = sum(sizes) / max(len(sizes), 1)
    n_valid_c = len(valid_clusters)
    n_single  = len(singleton_indices)

    print(f"   ğŸ“¦ Batches: {len(batches)} (avg {avg_size:.0f} samples/batch)")
    print(f"      æœ‰æ•ˆç°‡(â‰¥2): {n_valid_c}, å•æ¡ç°‡: {n_single}")

    return batches


def create_dynamic_sampler(dataset, batch_size=256, max_clusters_per_batch=64,
                           state_path=None, round_idx=1):
    return create_cluster_balanced_sampler(dataset, batch_size, max_clusters_per_batch)