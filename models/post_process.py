# models/post_process.py
"""
SSI-EC Post-processing: å…¨é‡è·ç¦»åˆ†é…

åœ¨æ‰€æœ‰ T è½®è¿­ä»£ç»“æŸå:
  1. åŠ è½½æœ€ç»ˆæ¨¡å‹, å¯¹æ‰€æœ‰ label==-1 çš„ reads åšæ¨ç†å¾—åˆ° embedding
  2. æ— æ¡ä»¶åˆ†é…åˆ°æœ€è¿‘çš„ç°‡è´¨å¿ƒ (ä¸è®¾ delta é˜ˆå€¼)
  3. è¾“å‡º final_labels.txt (å…¨é‡, æ—  -1)

ç†è®ºä¾æ®:
  - DNA å­˜å‚¨ä¸­æ¯æ¡ read å¿…ç„¶æ¥è‡ªæŸä¸ª reference, ä¸å­˜åœ¨"ä¸å±äºä»»ä½•ç°‡"
  - æœ€åä¸€è½®è´¨å¿ƒæ¥è‡ªè¿­ä»£ä¼˜åŒ–åçš„é«˜è´¨é‡ Zone I reads, æ¯” Clover æ ‘ç´¢å¼•ä¸­å¿ƒæ›´å‡†ç¡®
  - è¿­ä»£è¿‡ç¨‹ä¸­ä¿æŒ -1 ä¸¢å¼ƒ (é¿å…å™ªå£°æ±¡æŸ“è®­ç»ƒ), åªåœ¨æœ€ç»ˆè¯„ä¼°æ—¶å…¨é‡åˆ†é…
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import sys
import time
from torch.utils.data import Dataset, DataLoader

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.step1_model import Step1EvidentialModel
from models.step1_data import CloverDataLoader, seq_to_onehot


# ===========================================================================
# å™ªå£° Reads ä¸“ç”¨ Dataset
# ===========================================================================
class NoiseReadDataset(Dataset):
    """åªåŠ è½½ label == -1 çš„ reads, ç”¨äº post-processing æ¨ç†"""

    def __init__(self, data_loader, noise_indices, max_len=150):
        """
        Args:
            data_loader:   CloverDataLoader å®ä¾‹
            noise_indices: list[int], label == -1 çš„å…¨å±€ read ç´¢å¼•
            max_len:       åºåˆ—æœ€å¤§é•¿åº¦
        """
        self.data_loader = data_loader
        self.noise_indices = noise_indices
        self.max_len = max_len

    def __len__(self):
        return len(self.noise_indices)

    def __getitem__(self, idx):
        real_idx = self.noise_indices[idx]
        seq = self.data_loader.reads[real_idx]
        encoding = seq_to_onehot(seq, self.max_len)
        return {
            'encoding': encoding,
            'read_idx': real_idx,
        }


# ===========================================================================
# æ ¸å¿ƒ: å…¨é‡è·ç¦»åˆ†é…
# ===========================================================================
@torch.no_grad()
def post_process_final_assignment(experiment_dir, final_checkpoint_path,
                                  final_labels_path, centroids_path,
                                  output_dir, device='cuda',
                                  dim=256, max_length=150,
                                  gt_tags_file=None):
    """
    Post-processing: å¯¹æ‰€æœ‰ label==-1 çš„ reads åšæ— æ¡ä»¶æœ€è¿‘é‚»åˆ†é…

    Args:
        experiment_dir:       å®éªŒæ ¹ç›®å½•
        final_checkpoint_path: æœ€ç»ˆè½®æ¬¡çš„ model checkpoint
        final_labels_path:    æœ€ç»ˆè½®æ¬¡çš„ refined_labels.txt
        centroids_path:       æœ€ç»ˆè½®æ¬¡ä¿å­˜çš„ centroids.pt
        output_dir:           è¾“å‡ºç›®å½•
        device:               è®¡ç®—è®¾å¤‡
        dim:                  æ¨¡å‹ç»´åº¦
        max_length:           åºåˆ—æœ€å¤§é•¿åº¦
        gt_tags_file:         GT æ ‡ç­¾æ–‡ä»¶ (å¯é€‰)

    Returns:
        final_labels_path: str, æœ€ç»ˆæ ‡ç­¾æ–‡ä»¶è·¯å¾„
    """
    device = torch.device(device if torch.cuda.is_available() else 'cpu')
    os.makedirs(output_dir, exist_ok=True)

    print(f"\n{'='*70}")
    print(f"ğŸ”§ Post-processing: å…¨é‡è·ç¦»åˆ†é…")
    print(f"{'='*70}")

    # =====================================================================
    # 1. åŠ è½½æ ‡ç­¾, è¯†åˆ« -1 reads
    # =====================================================================
    labels = np.loadtxt(final_labels_path, dtype=int)
    TOTAL_READS = len(labels)
    noise_mask = (labels == -1)
    n_noise = noise_mask.sum()

    print(f"   æ€» reads: {TOTAL_READS:,}")
    print(f"   label==-1 çš„ reads: {n_noise:,} ({n_noise/TOTAL_READS*100:.2f}%)")

    if n_noise == 0:
        print(f"   âœ… æ— éœ€ post-processing, æ‰€æœ‰ reads å·²æœ‰æ ‡ç­¾")
        final_path = os.path.join(output_dir, "final_labels.txt")
        np.savetxt(final_path, labels, fmt='%d')
        return final_path

    # =====================================================================
    # 2. åŠ è½½è´¨å¿ƒ
    # =====================================================================
    centroids_data = torch.load(centroids_path, map_location='cpu')
    centroids = centroids_data['centroids']  # dict {cluster_id: tensor(D,)}
    print(f"   è´¨å¿ƒæ•°: {len(centroids)}")

    # =====================================================================
    # 3. åŠ è½½æ¨¡å‹
    # =====================================================================
    checkpoint = torch.load(final_checkpoint_path, map_location=device)
    step1_args = checkpoint.get('args', {})
    model_dim = step1_args.get('dim', dim)
    model_max_len = step1_args.get('max_length', max_length)

    num_clusters = max(50, len(centroids))
    model = Step1EvidentialModel(
        dim=model_dim, max_length=model_max_len,
        num_clusters=num_clusters, device=str(device)
    ).to(device)

    sd = checkpoint['model_state_dict']
    if 'length_adapter.weight' in sd:
        sh = sd['length_adapter.weight'].shape
        if sh[0] == model_max_len:
            model.length_adapter = nn.Linear(sh[1], sh[0]).to(device)
    model.load_state_dict(sd, strict=False)
    model.eval()
    print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # =====================================================================
    # 4. åŠ è½½æ•°æ®, åˆ›å»ºå™ªå£° Dataset
    # =====================================================================
    data_loader = CloverDataLoader(experiment_dir, labels_path=final_labels_path)
    noise_indices = np.where(noise_mask)[0].tolist()

    noise_dataset = NoiseReadDataset(data_loader, noise_indices, model_max_len)
    noise_loader = DataLoader(
        noise_dataset, batch_size=1024, shuffle=False,
        num_workers=0, pin_memory=True
    )

    print(f"   ğŸ“¦ å™ªå£° reads dataset: {len(noise_dataset)} samples")

    # =====================================================================
    # 5. æ¨ç†å™ªå£° reads çš„ embeddings
    # =====================================================================
    print(f"   ğŸ”® æ¨ç†å™ªå£° reads...")
    t0 = time.time()

    N = len(noise_dataset)
    D = model_dim
    noise_embeddings = torch.zeros(N, D)
    noise_read_indices = torch.zeros(N, dtype=torch.long)
    offset = 0

    for batch in noise_loader:
        reads = batch['encoding'].to(device)
        idxs = batch['read_idx']
        B = reads.shape[0]

        # Padding / Truncation
        if reads.shape[1] != model_max_len:
            if reads.shape[1] < model_max_len:
                reads = F.pad(reads, (0, 0, 0, model_max_len - reads.shape[1]))
            else:
                reads = reads[:, :model_max_len, :]

        _, pooled = model.encode_reads(reads)
        noise_embeddings[offset:offset+B] = pooled.cpu()
        noise_read_indices[offset:offset+B] = idxs
        offset += B

    noise_embeddings = noise_embeddings[:offset]
    noise_read_indices = noise_read_indices[:offset].numpy()

    del model
    torch.cuda.empty_cache()

    t1 = time.time()
    print(f"   âœ… æ¨ç†å®Œæˆ: {offset} reads, {t1-t0:.1f}s")

    # =====================================================================
    # 6. æ— æ¡ä»¶æœ€è¿‘é‚»åˆ†é…
    # =====================================================================
    print(f"   ğŸ“ æœ€è¿‘é‚»åˆ†é…...")
    t0 = time.time()

    # Normalize embeddings
    noise_emb_norm = F.normalize(noise_embeddings, dim=-1)

    # å‡†å¤‡è´¨å¿ƒçŸ©é˜µ
    sorted_cids = sorted(centroids.keys())
    centroid_matrix = torch.stack([centroids[c] for c in sorted_cids])
    # è´¨å¿ƒå·²åœ¨ compute_centroids_weighted ä¸­ normalized

    # åˆ†å—è®¡ç®—æœ€è¿‘é‚» (é¿å… OOM)
    final_labels = labels.copy()
    chunk_size = 5000

    for i in range(0, len(noise_emb_norm), chunk_size):
        batch_emb = noise_emb_norm[i:i+chunk_size]
        batch_indices = noise_read_indices[i:i+chunk_size]

        # Cosine similarity â†’ L2 distance
        sim = batch_emb @ centroid_matrix.T
        dist = torch.sqrt((2.0 - 2.0 * sim).clamp(min=0.0))
        nearest_idx = dist.argmin(dim=1)

        for j in range(len(batch_emb)):
            read_idx = int(batch_indices[j])
            cluster_id = sorted_cids[nearest_idx[j].item()]
            final_labels[read_idx] = cluster_id

    t1 = time.time()
    print(f"   âœ… åˆ†é…å®Œæˆ: {n_noise} reads â†’ {len(sorted_cids)} ä¸ªç°‡, {t1-t0:.1f}s")

    # éªŒè¯: ä¸åº”è¯¥è¿˜æœ‰ -1
    remaining_noise = (final_labels == -1).sum()
    if remaining_noise > 0:
        print(f"   âš ï¸ ä»æœ‰ {remaining_noise} æ¡ -1 (å¯èƒ½æ˜¯åˆå§‹å°±æœªè¢«æ¨ç†çš„ reads)")
    else:
        print(f"   âœ… å…¨é‡åˆ†é…å®Œæˆ, æ— æ®‹ç•™ -1")

    # =====================================================================
    # 7. ä¿å­˜
    # =====================================================================
    final_path = os.path.join(output_dir, "final_labels.txt")
    np.savetxt(final_path, final_labels, fmt='%d')
    print(f"   ğŸ’¾ æœ€ç»ˆæ ‡ç­¾: {final_path}")

    # =====================================================================
    # 8. è¯„ä¼° (å¦‚æœæœ‰ GT)
    # =====================================================================
    if gt_tags_file and os.path.exists(gt_tags_file):
        from models.eval_metrics import compute_all_metrics, save_metrics_report

        # åŠ è½½ GT
        if not hasattr(data_loader, 'gt_labels') or all(g == -1 for g in data_loader.gt_labels):
            data_loader.load_gt_tags(gt_tags_file)

        gt_labels_arr = np.array(data_loader.gt_labels)
        metrics = compute_all_metrics(final_labels, gt_labels_arr, verbose=True)

        report_path = os.path.join(output_dir, "final_eval_report.txt")
        save_metrics_report(metrics, report_path,
                            round_info="Post-processing final assignment")
    else:
        print(f"   â„¹ï¸ æ—  GT æ–‡ä»¶, è·³è¿‡è¯„ä¼°")

    return final_path