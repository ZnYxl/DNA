# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
âœ… ä¿®å¤ç‰ˆ v2ï¼šè§£å†³ "bitwise_and_cuda not implemented for Float" æŠ¥é”™
"""
import torch
import torch.nn.functional as F
import random

def split_confidence_by_percentile(strength, cluster_labels, p=0.2):
    """
    Phase A: ç°‡å†…ç›¸å¯¹è¯æ®ç­›é€‰
    """
    low_conf_mask = torch.zeros_like(cluster_labels, dtype=torch.bool)
    stats = {'processed_clusters': 0, 'skipped_clusters': 0, 'total_low_conf': 0}

    unique_labels = torch.unique(cluster_labels)

    print(f"   ğŸ¯ ç°‡å†…ç›¸å¯¹ç­›é€‰ (p={p:.1%}):")

    for c in unique_labels:
        if c < 0: continue # è·³è¿‡å™ªå£°

        mask = cluster_labels == c
        cluster_size = mask.sum().item()

        if cluster_size < 5:
            stats['skipped_clusters'] += 1
            continue

        s = strength[mask]
        tau = torch.quantile(s, p)

        cluster_low_conf = s <= tau
        low_conf_mask[mask] = cluster_low_conf

        stats['total_low_conf'] += cluster_low_conf.sum().item()
        stats['processed_clusters'] += 1
        
    print(f"\n   ğŸ“Š ç›¸å¯¹ç­›é€‰ç»“æœ:")
    print(f"      å¤„ç†ç°‡æ•°: {stats['processed_clusters']}")
    print(f"      è·³è¿‡ç°‡æ•°: {stats['skipped_clusters']}")
    print(f"      é«˜ç½®ä¿¡åº¦: { (~low_conf_mask).sum() }")
    print(f"      ä½ç½®ä¿¡åº¦: { low_conf_mask.sum() }")

    return low_conf_mask, stats


def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ (è¿”å› CPU å­—å…¸ä»¥èŠ‚çœ GPU æ˜¾å­˜)
    """
    print(f"\n   ğŸ§® æ­£åœ¨å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ (Total Reads: {len(labels)})...")

    device = embeddings.device
    valid_mask = (labels >= 0) & high_conf_mask
    valid_embeddings = embeddings[valid_mask]
    valid_labels = labels[valid_mask]

    if len(valid_labels) == 0:
        return {}, {}

    max_id = int(valid_labels.max().item())
    
    # åœ¨ GPU ä¸Šç´¯åŠ 
    sum_embeddings = torch.zeros(max_id + 1, embeddings.shape[1], device=device)
    count_reads = torch.zeros(max_id + 1, device=device)
    
    sum_embeddings.index_add_(0, valid_labels, valid_embeddings)
    ones = torch.ones_like(valid_labels, dtype=torch.float)
    count_reads.index_add_(0, valid_labels, ones)

    # è½¬å› CPU å¤„ç†å­—å…¸
    centroids = {}
    cluster_sizes = {}
    
    sum_emb_cpu = sum_embeddings.cpu()
    counts_cpu = count_reads.cpu()
    unique_ids_cpu = torch.unique(valid_labels).cpu().numpy()

    for k in unique_ids_cpu:
        count = counts_cpu[k].item()
        if count < 2: continue
        
        # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯ CPU Tensor
        centroids[int(k)] = sum_emb_cpu[k] / count
        cluster_sizes[int(k)] = int(count)

    print(f"   ğŸ“ ç°‡ä¸­å¿ƒè®¡ç®—å®Œæˆ: æœ‰æ•ˆç°‡æ•° {len(centroids)}")
    return centroids, cluster_sizes


def refine_low_confidence_reads(embeddings, labels, low_conf_mask, centroids, delta):
    """
    âœ… [ä¿®å¤+åŠ é€Ÿç‰ˆ] ç°‡ä¿®æ­£
    è§£å†³äº† CPU Centroids ä¸ GPU Embeddings çš„è®¾å¤‡å†²çª
    ä½¿ç”¨çŸ©é˜µè¿ç®—æ›¿ä»£å¾ªç¯
    """
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    low_conf_indices = torch.where(low_conf_mask)[0]
    num_low_conf = len(low_conf_indices)
    
    if num_low_conf == 0:
        return new_labels, noise_mask, {'reassigned': 0, 'marked_noise': 0, 'kept_unchanged': 0}

    print(f"\n   ğŸ”„ æ­£åœ¨æ‰¹é‡ä¿®æ­£ {num_low_conf} ä¸ªä½ç½®ä¿¡åº¦reads (Matrix Mode)...")
    
    # 1. å‡†å¤‡ç°‡ä¸­å¿ƒçŸ©é˜µ (å¹¶ç§»åŠ¨åˆ° GPU!)
    sorted_cluster_ids = sorted(centroids.keys())
    if not sorted_cluster_ids:
        print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç°‡ä¸­å¿ƒï¼Œè·³è¿‡ä¿®æ­£")
        return new_labels, noise_mask, {}

    # å°† CPU çš„ centroids å †å åï¼Œä¸€æ¬¡æ€§æ¬è¿åˆ° GPU
    cluster_matrix = torch.stack([centroids[k] for k in sorted_cluster_ids]).to(embeddings.device) # (K, D)
    cluster_ids_tensor = torch.tensor(sorted_cluster_ids, device=embeddings.device) # (K,)
    
    # 2. å‡†å¤‡æŸ¥è¯¢å‘é‡ (å·²ç»åœ¨ GPU ä¸Š)
    query_embeddings = embeddings[low_conf_indices] # (M, D)
    
    # 3. åˆ†å—è®¡ç®—è·ç¦»çŸ©é˜µ (é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸)
    batch_size = 5000 
    reassigned = 0
    marked_noise = 0
    
    for i in range(0, num_low_conf, batch_size):
        end = min(i + batch_size, num_low_conf)
        batch_queries = query_embeddings[i:end] # (B, D)
        
        # è®¡ç®—è·ç¦» (B, K)
        dists = torch.cdist(batch_queries, cluster_matrix)
        
        # æ‰¾æœ€è¿‘
        min_dists, min_indices = torch.min(dists, dim=1) # (B,)
        best_cluster_ids = cluster_ids_tensor[min_indices]
        
        # å†³ç­–
        valid_mask = min_dists < delta
        
        # å†™å›
        global_indices = low_conf_indices[i:end]
        
        # æœ‰æ•ˆçš„ï¼šé‡æ–°åˆ†é…
        valid_indices = global_indices[valid_mask]
        valid_assignments = best_cluster_ids[valid_mask]
        
        # ç»Ÿè®¡å˜åŒ–
        original_labels = labels[valid_indices]
        reassigned += (original_labels != valid_assignments).sum().item()
        
        new_labels[valid_indices] = valid_assignments
        
        # æ— æ•ˆçš„ï¼šæ ‡è®°å™ªå£°
        noise_indices = global_indices[~valid_mask]
        new_labels[noise_indices] = -1
        noise_mask[noise_indices] = True
        marked_noise += len(noise_indices)

    print(f"   âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned}")
    print(f"      æ ‡è®°å™ªå£°: {marked_noise}")
    print(f"      ä¿æŒä¸å˜: {num_low_conf - reassigned - marked_noise}")

    stats = {
        'reassigned': reassigned,
        'marked_noise': marked_noise,
        'kept_unchanged': num_low_conf - reassigned - marked_noise
    }

    return new_labels, noise_mask, stats


def compute_adaptive_delta(embeddings, centroids, percentile=10):
    """
    âœ… [ä¿®å¤ç‰ˆ] è‡ªé€‚åº”è®¡ç®— Delta
    """
    device = embeddings.device
    print(f"   ğŸ¯ è®¡ç®—è‡ªé€‚åº” Delta (Percentile={percentile})...")
    
    # ä¼˜åŒ–é€»è¾‘ï¼šåªé‡‡æ · 100 ä¸ªç°‡æ¥è®¡ç®—é˜ˆå€¼ï¼Œé¿å… 100 ä¸‡æ¬¡è®¡ç®—å¡æ­»
    sample_dists = []
    sampled_keys = random.sample(list(centroids.keys()), min(100, len(centroids)))
    
    for k in sampled_keys:
        ck_gpu = centroids[k].to(device)
        # éšæœºé‡‡ 100 ä¸ª embedding ç®—ä¸€ä¸‹è·ç¦»åˆ†å¸ƒ
        indices = torch.randint(0, len(embeddings), (100,), device=device)
        dists = torch.norm(embeddings[indices] - ck_gpu.unsqueeze(0), dim=1)
        sample_dists.append(dists)
        
    all_dists = torch.cat(sample_dists)
    delta = torch.quantile(all_dists, percentile / 100.0).item()

    print(f"   ğŸ¯ è‡ªé€‚åº”delta: {delta:.4f}")
    return delta


def merge_similar_clusters(embeddings, labels, centroids, merge_threshold=0.1):
    """
    âœ… [ä¿®å¤ç‰ˆ] å¼ºåŠ›åˆå¹¶
    ä¿®å¤äº† 'bitwise_and_cuda' not implemented for 'Float' æŠ¥é”™
    """
    print(f"\n   ğŸ§² å¼€å§‹æ‰§è¡Œç°‡åˆå¹¶ (é˜ˆå€¼={merge_threshold})...")
    device = embeddings.device
    
    # 1. å‡†å¤‡æ•°æ®
    sorted_ids = sorted(list(centroids.keys()))
    if len(sorted_ids) < 2: return labels, {}
    
    # è½¬ä¸º Tensor çŸ©é˜µ
    center_matrix = torch.stack([centroids[k] for k in sorted_ids]).to(device) # (K, D)
    
    # 2. è®¡ç®—ä¸¤ä¸¤è·ç¦» (K, K)
    dists = torch.cdist(center_matrix, center_matrix)
    
    # æ’é™¤è‡ªèº« (è®¾ä¸ºæ— ç©·å¤§)
    eye_mask = torch.eye(len(sorted_ids), device=device).bool()
    dists.masked_fill_(eye_mask, float('inf'))
    
    # 3. è´ªå©ªåˆå¹¶ç­–ç•¥
    merge_map = {} # old_id -> new_id
    
    # ğŸ”´ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶å°†ä¸Šä¸‰è§’æ©ç è½¬ä¸º bool ç±»å‹
    # åŸä»£ç : torch.triu(torch.ones_like(dists), diagonal=1) -> Float
    # æ–°ä»£ç : torch.triu(torch.ones_like(dists, dtype=torch.bool), diagonal=1) -> Bool
    
    upper_tri_mask = torch.triu(torch.ones_like(dists, dtype=torch.bool), diagonal=1)
    
    # è·å–æ»¡è¶³æ¡ä»¶çš„ç´¢å¼•
    pairs = torch.nonzero((dists < merge_threshold) & upper_tri_mask)
    
    # æŒ‰è·ç¦»ä»å°åˆ°å¤§æ’åºï¼Œä¼˜å…ˆåˆå¹¶æœ€è¿‘çš„
    if len(pairs) > 0:
        pair_dists = dists[pairs[:, 0], pairs[:, 1]]
        sorted_idx = torch.argsort(pair_dists)
        pairs = pairs[sorted_idx]
    
    merge_count = 0
    
    for idx in range(len(pairs)):
        i, j = pairs[idx].tolist()
        id_a, id_b = sorted_ids[i], sorted_ids[j]
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«åˆå¹¶è¿‡
        root_a = id_a
        while root_a in merge_map: root_a = merge_map[root_a]
        
        root_b = id_b
        while root_b in merge_map: root_b = merge_map[root_b]
        
        if root_a != root_b:
            # æ€»æ˜¯æŠŠå¤§çš„ ID åˆå¹¶åˆ°å°çš„ ID (ä¿æŒç¨³å®š)
            target = min(root_a, root_b)
            source = max(root_a, root_b)
            merge_map[source] = target
            merge_count += 1
            
    print(f"      å‘ç° {merge_count} å¯¹ç›¸ä¼¼ç°‡éœ€è¦åˆå¹¶")
    
    if merge_count == 0:
        return labels, {}

    # 4. æ‰§è¡Œåˆå¹¶ (æ›´æ–° Labels)
    new_labels = labels.clone()
    
    # æ‰¹é‡æ›´æ–°
    for src, dst in merge_map.items():
        mask = (labels == src)
        new_labels[mask] = dst
        
    print(f"   âœ… åˆå¹¶å®Œæˆï¼ç°‡æ•°é‡: {len(sorted_ids)} -> {len(sorted_ids) - merge_count}")
    return new_labels, merge_map