# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
âœ… ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ï¼šç°‡å†…æ¯”è¾ƒï¼Œä¸æ˜¯å…¨å±€é˜ˆå€¼
"""
import torch
import torch.nn.functional as F

def split_confidence_by_percentile(strength, cluster_labels, p=0.2):
    """
    âœ… Phase A: ç°‡å†…ç›¸å¯¹è¯æ®ç­›é€‰ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    æ¯ä¸ªç°‡é‡Œï¼Œå–strengthæœ€ä½çš„p%ä½œä¸ºä½ç½®ä¿¡åº¦
    
    Args:
        strength: (N,) evidence strength
        cluster_labels: (N,) ç°‡æ ‡ç­¾
        p: float, ä½ç½®ä¿¡åº¦ç™¾åˆ†æ¯” (0.2 = 20%)
    
    Returns:
        low_conf_mask: (N,) bool, Trueè¡¨ç¤ºä½ç½®ä¿¡åº¦
        stats: dict, ç»Ÿè®¡ä¿¡æ¯
    """
    low_conf_mask = torch.zeros_like(cluster_labels, dtype=torch.bool)
    stats = {'processed_clusters': 0, 'skipped_clusters': 0, 'total_low_conf': 0}
    
    unique_labels = torch.unique(cluster_labels)
    
    print(f"   ğŸ¯ ç°‡å†…ç›¸å¯¹ç­›é€‰ (p={p:.1%}):")
    
    for c in unique_labels:
        if c < 0:  # è·³è¿‡å™ªå£°
            continue
            
        mask = cluster_labels == c
        cluster_size = mask.sum().item()
        
        if cluster_size < 5:  # å¤ªå°çš„ç°‡è·³è¿‡
            print(f"      ç°‡{c}: {cluster_size} reads (å¤ªå°ï¼Œè·³è¿‡)")
            stats['skipped_clusters'] += 1
            continue
        
        # è¯¥ç°‡çš„strength
        s = strength[mask]
        tau = torch.quantile(s, p)  # ç¬¬påˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        
        # æ ‡è®°è¯¥ç°‡å†…çš„ä½ç½®ä¿¡åº¦reads
        cluster_low_conf = s <= tau
        low_conf_mask[mask] = cluster_low_conf
        
        low_count = cluster_low_conf.sum().item()
        stats['total_low_conf'] += low_count
        stats['processed_clusters'] += 1
        
        print(f"      ç°‡{c}: {cluster_size} reads, Ï„={tau:.3f}, ä½ç½®ä¿¡åº¦={low_count} ({low_count/cluster_size:.1%})")
    
    high_conf_mask = ~low_conf_mask
    
    print(f"\n   ğŸ“Š ç›¸å¯¹ç­›é€‰ç»“æœ:")
    print(f"      å¤„ç†ç°‡æ•°: {stats['processed_clusters']}")
    print(f"      è·³è¿‡ç°‡æ•°: {stats['skipped_clusters']}")
    print(f"      é«˜ç½®ä¿¡åº¦: {high_conf_mask.sum()}/{len(strength)} ({high_conf_mask.float().mean():.1%})")
    print(f"      ä½ç½®ä¿¡åº¦: {low_conf_mask.sum()}/{len(strength)} ({low_conf_mask.float().mean():.1%})")
    
    return low_conf_mask, stats

def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    âœ… åªç”¨é«˜ç½®ä¿¡åº¦readsè®¡ç®—ç°‡ä¸­å¿ƒ
    
    Args:
        embeddings: (N, D) Step1çš„embedding
        labels: (N,) å½“å‰ç°‡æ ‡ç­¾
        high_conf_mask: (N,) bool, é«˜ç½®ä¿¡åº¦mask
    
    Returns:
        centroids: dict[label] -> (D,) ç°‡ä¸­å¿ƒ
        cluster_sizes: dict[label] -> int ç°‡å¤§å°
    """
    centroids = {}
    cluster_sizes = {}
    
    unique_labels = torch.unique(labels)
    valid_clusters = 0
    
    for k in unique_labels:
        if k < 0:  # è·³è¿‡å™ªå£°
            continue
        
        # âœ… åªç”¨é«˜ç½®ä¿¡åº¦reads
        mask = (labels == k) & high_conf_mask
        count = mask.sum().item()
        
        if count < 2:  # è‡³å°‘2ä¸ªé«˜ç½®ä¿¡åº¦reads
            print(f"   âš ï¸ ç°‡ {k}: åªæœ‰ {count} ä¸ªé«˜ç½®ä¿¡åº¦readsï¼Œè·³è¿‡")
            continue
        
        centroids[int(k.item())] = embeddings[mask].mean(dim=0)
        cluster_sizes[int(k.item())] = count
        valid_clusters += 1
    
    print(f"\n   ğŸ“ ç°‡ä¸­å¿ƒç»Ÿè®¡:")
    print(f"      æœ‰æ•ˆç°‡æ•°: {valid_clusters}/{len(unique_labels)-1}")  # -1æ’é™¤å™ªå£°
    if cluster_sizes:
        print(f"      å¹³å‡ç°‡å¤§å°: {sum(cluster_sizes.values())/len(cluster_sizes):.1f}")
    
    return centroids, cluster_sizes

def refine_low_confidence_reads(embeddings, labels, low_conf_mask, 
                                centroids, delta):
    """
    âœ… Phase B: ç°‡ä¿®æ­£ï¼ˆåªå¤„ç†ä½ç½®ä¿¡åº¦readsï¼‰
    ä½ç½®ä¿¡åº¦readsé‡æ–°åˆ†é…æˆ–æ ‡è®°ä¸ºå™ªå£°
    
    Args:
        embeddings: (N, D)
        labels: (N,) å½“å‰æ ‡ç­¾
        low_conf_mask: (N,) ä½ç½®ä¿¡åº¦mask
        centroids: dict[label] -> (D,)
        delta: float, è·ç¦»é˜ˆå€¼
    
    Returns:
        new_labels: (N,) ä¿®æ­£åçš„æ ‡ç­¾
        noise_mask: (N,) bool, æ–°å¢å™ªå£°mask
        stats: dict, ç»Ÿè®¡ä¿¡æ¯
    """
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    # ç»Ÿè®¡ä¿¡æ¯
    reassigned = 0
    marked_noise = 0
    
    low_conf_indices = torch.where(low_conf_mask)[0]
    
    print(f"\n   ğŸ”„ åªå¤„ç† {len(low_conf_indices)} ä¸ªä½ç½®ä¿¡åº¦reads...")
    
    for idx in low_conf_indices:
        i = idx.item()
        zi = embeddings[i]
        
        # æ‰¾æœ€è¿‘çš„ç°‡ä¸­å¿ƒ
        best_k = None
        best_dist = float('inf')
        
        for k, ck in centroids.items():
            dist = torch.norm(zi - ck).item()
            if dist < best_dist:
                best_dist = dist
                best_k = k
        
        # å†³ç­–è§„åˆ™
        if best_k is not None and best_dist < delta:
            # âœ… é‡æ–°åˆ†é…åˆ°æœ€è¿‘ç°‡
            if new_labels[i] != best_k:
                reassigned += 1
            new_labels[i] = best_k
        else:
            # âŒ æ ‡è®°ä¸ºå™ªå£°
            new_labels[i] = -1
            noise_mask[i] = True
            marked_noise += 1
    
    print(f"   âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned}")
    print(f"      æ ‡è®°å™ªå£°: {marked_noise}")
    print(f"      ä¿æŒä¸å˜: {len(low_conf_indices) - reassigned - marked_noise}")
    
    stats = {
        'reassigned': reassigned,
        'marked_noise': marked_noise,
        'kept_unchanged': len(low_conf_indices) - reassigned - marked_noise
    }
    
    return new_labels, noise_mask, stats

def compute_adaptive_delta(embeddings, centroids, percentile=10):
    """
    âœ… è‡ªé€‚åº”è®¡ç®—deltaé˜ˆå€¼
    
    Args:
        embeddings: (N, D)
        centroids: dict[label] -> (D,)
        percentile: int, ç™¾åˆ†ä½æ•°ï¼ˆæ¥æ”¶æœ€è¿‘çš„X%ï¼‰
    
    Returns:
        delta: float
    """
    all_distances = []
    
    for k, ck in centroids.items():
        dists = torch.norm(embeddings - ck.unsqueeze(0), dim=1)
        all_distances.append(dists)
    
    all_distances = torch.cat(all_distances)
    delta = torch.quantile(all_distances, percentile / 100.0).item()
    
    print(f"   ğŸ¯ è‡ªé€‚åº”delta: {delta:.4f} (æ¥æ”¶æœ€è¿‘{percentile}%çš„reads)")
    
    return delta