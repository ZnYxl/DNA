# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
âœ… ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ï¼šç°‡å†…æ¯”è¾ƒï¼Œä¸æ˜¯å…¨å±€é˜ˆå€¼
"""
import torch
import torch.nn.functional as F

def split_confidence_by_percentile(strength, cluster_labels, p=0.2):
    """
    âœ… Phase A: ç°‡å†…ç›¸å¯¹è¯æ®ç­›é€‰ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    æ¯ä¸ªç°‡é‡Œï¼Œå–strengthæœ€ä½çš„p%ä½œä¸ºä½ç½®ä¿¡åº¦
    
    Args:
        strength: (N,) evidence strength
        cluster_labels: (N,) ç°‡æ ‡ç­¾
        p: float, ä½ç½®ä¿¡åº¦ç™¾åˆ†æ¯” (0.2 = 20%)
    
    Returns:
        low_conf_mask: (N,) bool, Trueè¡¨ç¤ºä½ç½®ä¿¡åº¦
        stats: dict, ç»Ÿè®¡ä¿¡æ¯
    """
    low_conf_mask = torch.zeros_like(cluster_labels, dtype=torch.bool)
    stats = {'processed_clusters': 0, 'skipped_clusters': 0, 'total_low_conf': 0}
    
    unique_labels = torch.unique(cluster_labels)
    
    print(f"   ğŸ¯ ç°‡å†…ç›¸å¯¹ç­›é€‰ (p={p:.1%}):")
    
    for c in unique_labels:
        if c < 0:  # è·³è¿‡å™ªå£°
            continue
            
        mask = cluster_labels == c
        cluster_size = mask.sum().item()
        
        if cluster_size < 5:  # å¤ªå°çš„ç°‡è·³è¿‡
            print(f"      ç°‡{c}: {cluster_size} reads (å¤ªå°ï¼Œè·³è¿‡)")
            stats['skipped_clusters'] += 1
            continue
        
        # è¯¥ç°‡çš„strength
        s = strength[mask]
        tau = torch.quantile(s, p)  # ç¬¬påˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        
        # æ ‡è®°è¯¥ç°‡å†…çš„ä½ç½®ä¿¡åº¦reads
        cluster_low_conf = s <= tau
        low_conf_mask[mask] = cluster_low_conf
        
        low_count = cluster_low_conf.sum().item()
        stats['total_low_conf'] += low_count
        stats['processed_clusters'] += 1
        
        print(f"      ç°‡{c}: {cluster_size} reads, Ï„={tau:.3f}, ä½ç½®ä¿¡åº¦={low_count} ({low_count/cluster_size:.1%})")
    
    high_conf_mask = ~low_conf_mask
    
    print(f"\n   ğŸ“Š ç›¸å¯¹ç­›é€‰ç»“æœ:")
    print(f"      å¤„ç†ç°‡æ•°: {stats['processed_clusters']}")
    print(f"      è·³è¿‡ç°‡æ•°: {stats['skipped_clusters']}")
    print(f"      é«˜ç½®ä¿¡åº¦: {high_conf_mask.sum()}/{len(strength)} ({high_conf_mask.float().mean():.1%})")
    print(f"      ä½ç½®ä¿¡åº¦: {low_conf_mask.sum()}/{len(strength)} ({low_conf_mask.float().mean():.1%})")
    
    return low_conf_mask, stats

def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    âœ… [æ€§èƒ½ä¼˜åŒ–ç‰ˆ] å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ
    å¤æ‚åº¦é™ä½ä¸º O(N)ï¼Œé€‚é… 100ä¸‡+ æ•°æ®é‡
    """
    print(f"\n   ğŸ§® æ­£åœ¨å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ (Total Reads: {len(labels)})...")
    
    device = embeddings.device
    
    # 1. è¿‡æ»¤ï¼šåªä¿ç•™é«˜ç½®ä¿¡åº¦ä¸”éå™ªå£°çš„reads
    # label >= 0 ä¸” high_conf_mask ä¸º True
    valid_mask = (labels >= 0) & high_conf_mask
    
    valid_embeddings = embeddings[valid_mask] # (M, D)
    valid_labels = labels[valid_mask]         # (M,)
    
    if len(valid_labels) == 0:
        print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„é«˜ç½®ä¿¡åº¦Readsç”¨äºè®¡ç®—ä¸­å¿ƒ")
        return {}, {}

    # 2. è·å–æ‰€æœ‰å‡ºç°çš„ç°‡ID
    unique_cluster_ids = torch.unique(valid_labels)
    max_id = int(valid_labels.max().item())
    
    # 3. åˆå§‹åŒ–ç´¯åŠ å™¨ (ä½¿ç”¨ max_id + 1 å¤§å°çš„å¼ é‡ä½œä¸ºæ•£åˆ—è¡¨)
    # sum_embeddings[k] å­˜å‚¨ç°‡ k çš„å‘é‡å’Œ
    sum_embeddings = torch.zeros(max_id + 1, embeddings.shape[1], device=device)
    # count_reads[k] å­˜å‚¨ç°‡ k çš„æ•°é‡
    count_reads = torch.zeros(max_id + 1, device=device)
    
    # 4. æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ scatter_add æˆ– index_add_ (è¿™é‡Œç”¨ index_add_ æ›´é€šç”¨)
    # å°† valid_embeddings åŠ åˆ°å¯¹åº”çš„ sum_embeddings è¡Œä¸­
    sum_embeddings.index_add_(0, valid_labels, valid_embeddings)
    
    # è®¡ç®—è®¡æ•° (åŠ 1.0)
    ones = torch.ones_like(valid_labels, dtype=torch.float)
    count_reads.index_add_(0, valid_labels, ones)
    
    # 5. è½¬æ¢ä¸ºå­—å…¸è¾“å‡º (ä¿æŒåŸæœ‰æ¥å£å…¼å®¹æ€§)
    centroids = {}
    cluster_sizes = {}
    
    valid_clusters_count = 0
    
    # å°† Tensor è½¬å› CPU å¤„ç†å­—å…¸ (å› ä¸ºæ­¤æ—¶ K åªæœ‰ 10000ï¼Œå¾ªç¯å¾ˆå¿«)
    # é¿å…åœ¨ GPU ä¸Šåšå¤§è§„æ¨¡å­—å…¸æ“ä½œ
    sum_emb_cpu = sum_embeddings.cpu()
    counts_cpu = count_reads.cpu()
    unique_ids_cpu = unique_cluster_ids.cpu().numpy()
    
    for k in unique_ids_cpu:
        count = counts_cpu[k].item()
        if count < 2: # ä¿æŒä½ ä¹‹å‰çš„é€»è¾‘ï¼šè‡³å°‘2ä¸ªreads
            continue
            
        # è®¡ç®—å¹³å‡å€¼
        centroid = sum_emb_cpu[k] / count
        
        centroids[int(k)] = centroid
        cluster_sizes[int(k)] = int(count)
        valid_clusters_count += 1
        
    print(f"   ğŸ“ ç°‡ä¸­å¿ƒè®¡ç®—å®Œæˆ:")
    print(f"      æœ‰æ•ˆç°‡æ•°: {valid_clusters_count}")
    if cluster_sizes:
        avg_size = sum(cluster_sizes.values()) / len(cluster_sizes)
        print(f"      å¹³å‡æœ‰æ•ˆç°‡å¤§å°: {avg_size:.1f}")
        
    return centroids, cluster_sizes

def refine_low_confidence_reads(embeddings, labels, low_conf_mask, 
                                centroids, delta):
    """
    âœ… Phase B: ç°‡ä¿®æ­£ï¼ˆåªå¤„ç†ä½ç½®ä¿¡åº¦readsï¼‰
    ä½ç½®ä¿¡åº¦readsé‡æ–°åˆ†é…æˆ–æ ‡è®°ä¸ºå™ªå£°
    
    Args:
        embeddings: (N, D)
        labels: (N,) å½“å‰æ ‡ç­¾
        low_conf_mask: (N,) ä½ç½®ä¿¡åº¦mask
        centroids: dict[label] -> (D,)
        delta: float, è·ç¦»é˜ˆå€¼
    
    Returns:
        new_labels: (N,) ä¿®æ­£åçš„æ ‡ç­¾
        noise_mask: (N,) bool, æ–°å¢å™ªå£°mask
        stats: dict, ç»Ÿè®¡ä¿¡æ¯
    """
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    # ç»Ÿè®¡ä¿¡æ¯
    reassigned = 0
    marked_noise = 0
    
    low_conf_indices = torch.where(low_conf_mask)[0]
    
    print(f"\n   ğŸ”„ åªå¤„ç† {len(low_conf_indices)} ä¸ªä½ç½®ä¿¡åº¦reads...")
    
    for idx in low_conf_indices:
        i = idx.item()
        zi = embeddings[i]
        
        # æ‰¾æœ€è¿‘çš„ç°‡ä¸­å¿ƒ
        best_k = None
        best_dist = float('inf')
        
        for k, ck in centroids.items():
            dist = torch.norm(zi - ck).item()
            if dist < best_dist:
                best_dist = dist
                best_k = k
        
        # å†³ç­–è§„åˆ™
        if best_k is not None and best_dist < delta:
            # âœ… é‡æ–°åˆ†é…åˆ°æœ€è¿‘ç°‡
            if new_labels[i] != best_k:
                reassigned += 1
            new_labels[i] = best_k
        else:
            # âŒ æ ‡è®°ä¸ºå™ªå£°
            new_labels[i] = -1
            noise_mask[i] = True
            marked_noise += 1
    
    print(f"   âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned}")
    print(f"      æ ‡è®°å™ªå£°: {marked_noise}")
    print(f"      ä¿æŒä¸å˜: {len(low_conf_indices) - reassigned - marked_noise}")
    
    stats = {
        'reassigned': reassigned,
        'marked_noise': marked_noise,
        'kept_unchanged': len(low_conf_indices) - reassigned - marked_noise
    }
    
    return new_labels, noise_mask, stats

def compute_adaptive_delta(embeddings, centroids, percentile=10):
    """
    âœ… è‡ªé€‚åº”è®¡ç®—deltaé˜ˆå€¼
    
    Args:
        embeddings: (N, D)
        centroids: dict[label] -> (D,)
        percentile: int, ç™¾åˆ†ä½æ•°ï¼ˆæ¥æ”¶æœ€è¿‘çš„X%ï¼‰
    
    Returns:
        delta: float
    """
    all_distances = []
    
    for k, ck in centroids.items():
        dists = torch.norm(embeddings - ck.unsqueeze(0), dim=1)
        all_distances.append(dists)
    
    all_distances = torch.cat(all_distances)
    delta = torch.quantile(all_distances, percentile / 100.0).item()
    
    print(f"   ğŸ¯ è‡ªé€‚åº”delta: {delta:.4f} (æ¥æ”¶æœ€è¿‘{percentile}%çš„reads)")
    
    return delta