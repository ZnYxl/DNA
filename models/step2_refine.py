# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
"""
import torch
import torch.nn.functional as F

def select_high_confidence_reads(strength, tau=None, quantile=0.5):
    """
    âœ… Phase A: è¯æ®ç­›é€‰
    æ ¹æ®evidence strengthåŒºåˆ†é«˜/ä½ç½®ä¿¡åº¦reads
    
    Args:
        strength: (N,) æ¯æ¡readçš„evidence strength
        tau: float or None, è‡ªå®šä¹‰é˜ˆå€¼
        quantile: float, åˆ†ä½æ•°ï¼ˆå½“tau=Noneæ—¶ä½¿ç”¨ï¼‰
    
    Returns:
        high_conf_mask: (N,) bool, Trueè¡¨ç¤ºé«˜ç½®ä¿¡åº¦
        tau_used: float, å®é™…ä½¿ç”¨çš„é˜ˆå€¼
    """
    if tau is None:
        tau = torch.quantile(strength, quantile)
    
    high_conf_mask = strength >= tau
    
    print(f"   ğŸ“Š ç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"      é˜ˆå€¼ Ï„: {tau:.4f}")
    print(f"      é«˜ç½®ä¿¡åº¦: {high_conf_mask.sum()}/{len(strength)} ({high_conf_mask.float().mean()*100:.1f}%)")
    print(f"      ä½ç½®ä¿¡åº¦: {(~high_conf_mask).sum()}/{len(strength)} ({(~high_conf_mask).float().mean()*100:.1f}%)")
    
    return high_conf_mask, tau


def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    âœ… åªç”¨é«˜ç½®ä¿¡åº¦readsè®¡ç®—ç°‡ä¸­å¿ƒ
    
    Args:
        embeddings: (N, D) Step1çš„embedding
        labels: (N,) å½“å‰ç°‡æ ‡ç­¾
        high_conf_mask: (N,) bool, é«˜ç½®ä¿¡åº¦mask
    
    Returns:
        centroids: dict[label] -> (D,) ç°‡ä¸­å¿ƒ
        cluster_sizes: dict[label] -> int ç°‡å¤§å°
    """
    centroids = {}
    cluster_sizes = {}
    
    unique_labels = torch.unique(labels)
    valid_clusters = 0
    
    for k in unique_labels:
        if k < 0:  # è·³è¿‡å™ªå£°
            continue
        
        # âœ… åªç”¨é«˜ç½®ä¿¡åº¦reads
        mask = (labels == k) & high_conf_mask
        count = mask.sum().item()
        
        if count < 2:  # è‡³å°‘2ä¸ªé«˜ç½®ä¿¡åº¦reads
            print(f"   âš ï¸ ç°‡ {k}: åªæœ‰ {count} ä¸ªé«˜ç½®ä¿¡åº¦readsï¼Œè·³è¿‡")
            continue
        
        centroids[int(k.item())] = embeddings[mask].mean(dim=0)
        cluster_sizes[int(k.item())] = count
        valid_clusters += 1
    
    print(f"\n   ğŸ“ ç°‡ä¸­å¿ƒç»Ÿè®¡:")
    print(f"      æœ‰æ•ˆç°‡æ•°: {valid_clusters}/{len(unique_labels)-1}")  # -1æ’é™¤å™ªå£°
    print(f"      å¹³å‡ç°‡å¤§å°: {sum(cluster_sizes.values())/len(cluster_sizes):.1f}")
    
    return centroids, cluster_sizes


def refine_low_confidence_reads(embeddings, labels, high_conf_mask, 
                                centroids, delta):
    """
    âœ… Phase B: ç°‡ä¿®æ­£
    ä½ç½®ä¿¡åº¦readsé‡æ–°åˆ†é…æˆ–æ ‡è®°ä¸ºå™ªå£°
    
    Args:
        embeddings: (N, D)
        labels: (N,) å½“å‰æ ‡ç­¾
        high_conf_mask: (N,)
        centroids: dict[label] -> (D,)
        delta: float, è·ç¦»é˜ˆå€¼
    
    Returns:
        new_labels: (N,) ä¿®æ­£åçš„æ ‡ç­¾
        noise_mask: (N,) bool, æ–°å¢å™ªå£°mask
        stats: dict, ç»Ÿè®¡ä¿¡æ¯
    """
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    # ç»Ÿè®¡ä¿¡æ¯
    reassigned = 0
    marked_noise = 0
    
    low_conf_indices = torch.where(~high_conf_mask)[0]
    
    print(f"\n   ğŸ”„ å¤„ç† {len(low_conf_indices)} ä¸ªä½ç½®ä¿¡åº¦reads...")
    
    for idx in low_conf_indices:
        i = idx.item()
        zi = embeddings[i]
        
        # æ‰¾æœ€è¿‘çš„ç°‡ä¸­å¿ƒ
        best_k = None
        best_dist = float('inf')
        
        for k, ck in centroids.items():
            dist = torch.norm(zi - ck).item()
            if dist < best_dist:
                best_dist = dist
                best_k = k
        
        # å†³ç­–è§„åˆ™
        if best_k is not None and best_dist < delta:
            # âœ… é‡æ–°åˆ†é…åˆ°æœ€è¿‘ç°‡
            if new_labels[i] != best_k:
                reassigned += 1
            new_labels[i] = best_k
        else:
            # âŒ æ ‡è®°ä¸ºå™ªå£°
            new_labels[i] = -1
            noise_mask[i] = True
            marked_noise += 1
    
    print(f"   âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned}")
    print(f"      æ ‡è®°å™ªå£°: {marked_noise}")
    print(f"      ä¿æŒä¸å˜: {len(low_conf_indices) - reassigned - marked_noise}")
    
    stats = {
        'reassigned': reassigned,
        'marked_noise': marked_noise,
        'kept_unchanged': len(low_conf_indices) - reassigned - marked_noise
    }
    
    return new_labels, noise_mask, stats


def compute_adaptive_delta(embeddings, centroids, percentile=10):
    """
    âœ… è‡ªé€‚åº”è®¡ç®—deltaé˜ˆå€¼
    
    Args:
        embeddings: (N, D)
        centroids: dict[label] -> (D,)
        percentile: int, ç™¾åˆ†ä½æ•°ï¼ˆæ¥æ”¶æœ€è¿‘çš„X%ï¼‰
    
    Returns:
        delta: float
    """
    all_distances = []
    
    for k, ck in centroids.items():
        dists = torch.norm(embeddings - ck.unsqueeze(0), dim=1)
        all_distances.append(dists)
    
    all_distances = torch.cat(all_distances)
    delta = torch.quantile(all_distances, percentile / 100.0).item()
    
    print(f"   ğŸ¯ è‡ªé€‚åº”delta: {delta:.4f} (æ¥æ”¶æœ€è¿‘{percentile}%çš„reads)")
    
    return delta
