# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
âœ… ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ï¼šç°‡å†…æ¯”è¾ƒï¼Œä¸æ˜¯å…¨å±€é˜ˆå€¼
"""
import torch
import torch.nn.functional as F

def split_confidence_by_percentile(strength, cluster_labels, p=0.2):
    """
    âœ… Phase A: ç°‡å†…ç›¸å¯¹è¯æ®ç­›é€‰ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    æ¯ä¸ªç°‡é‡Œï¼Œå–strengthæœ€ä½çš„p%ä½œä¸ºä½ç½®ä¿¡åº¦
    
    Args:
        strength: (N,) evidence strength
        cluster_labels: (N,) ç°‡æ ‡ç­¾
        p: float, ä½ç½®ä¿¡åº¦ç™¾åˆ†æ¯” (0.2 = 20%)
    
    Returns:
        low_conf_mask: (N,) bool, Trueè¡¨ç¤ºä½ç½®ä¿¡åº¦
        stats: dict, ç»Ÿè®¡ä¿¡æ¯
    """
    low_conf_mask = torch.zeros_like(cluster_labels, dtype=torch.bool)
    stats = {'processed_clusters': 0, 'skipped_clusters': 0, 'total_low_conf': 0}
    
    unique_labels = torch.unique(cluster_labels)
    
    print(f"   ğŸ¯ ç°‡å†…ç›¸å¯¹ç­›é€‰ (p={p:.1%}):")
    
    for c in unique_labels:
        if c < 0:  # è·³è¿‡å™ªå£°
            continue
            
        mask = cluster_labels == c
        cluster_size = mask.sum().item()
        
        if cluster_size < 5:  # å¤ªå°çš„ç°‡è·³è¿‡
            print(f"      ç°‡{c}: {cluster_size} reads (å¤ªå°ï¼Œè·³è¿‡)")
            stats['skipped_clusters'] += 1
            continue
        
        # è¯¥ç°‡çš„strength
        s = strength[mask]
        tau = torch.quantile(s, p)  # ç¬¬påˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        
        # æ ‡è®°è¯¥ç°‡å†…çš„ä½ç½®ä¿¡åº¦reads
        cluster_low_conf = s <= tau
        low_conf_mask[mask] = cluster_low_conf
        
        low_count = cluster_low_conf.sum().item()
        stats['total_low_conf'] += low_count
        stats['processed_clusters'] += 1
        
        print(f"      ç°‡{c}: {cluster_size} reads, Ï„={tau:.3f}, ä½ç½®ä¿¡åº¦={low_count} ({low_count/cluster_size:.1%})")
    
    high_conf_mask = ~low_conf_mask
    
    print(f"\n   ğŸ“Š ç›¸å¯¹ç­›é€‰ç»“æœ:")
    print(f"      å¤„ç†ç°‡æ•°: {stats['processed_clusters']}")
    print(f"      è·³è¿‡ç°‡æ•°: {stats['skipped_clusters']}")
    print(f"      é«˜ç½®ä¿¡åº¦: {high_conf_mask.sum()}/{len(strength)} ({high_conf_mask.float().mean():.1%})")
    print(f"      ä½ç½®ä¿¡åº¦: {low_conf_mask.sum()}/{len(strength)} ({low_conf_mask.float().mean():.1%})")
    
    return low_conf_mask, stats

def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    âœ… [æ€§èƒ½ä¼˜åŒ–ç‰ˆ] å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ
    å¤æ‚åº¦é™ä½ä¸º O(N)ï¼Œé€‚é… 100ä¸‡+ æ•°æ®é‡
    """
    print(f"\n   ğŸ§® æ­£åœ¨å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ (Total Reads: {len(labels)})...")
    
    device = embeddings.device
    
    # 1. è¿‡æ»¤ï¼šåªä¿ç•™é«˜ç½®ä¿¡åº¦ä¸”éå™ªå£°çš„reads
    # label >= 0 ä¸” high_conf_mask ä¸º True
    valid_mask = (labels >= 0) & high_conf_mask
    
    valid_embeddings = embeddings[valid_mask] # (M, D)
    valid_labels = labels[valid_mask]         # (M,)
    
    if len(valid_labels) == 0:
        print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„é«˜ç½®ä¿¡åº¦Readsç”¨äºè®¡ç®—ä¸­å¿ƒ")
        return {}, {}

    # 2. è·å–æ‰€æœ‰å‡ºç°çš„ç°‡ID
    unique_cluster_ids = torch.unique(valid_labels)
    max_id = int(valid_labels.max().item())
    
    # 3. åˆå§‹åŒ–ç´¯åŠ å™¨ (ä½¿ç”¨ max_id + 1 å¤§å°çš„å¼ é‡ä½œä¸ºæ•£åˆ—è¡¨)
    # sum_embeddings[k] å­˜å‚¨ç°‡ k çš„å‘é‡å’Œ
    sum_embeddings = torch.zeros(max_id + 1, embeddings.shape[1], device=device)
    # count_reads[k] å­˜å‚¨ç°‡ k çš„æ•°é‡
    count_reads = torch.zeros(max_id + 1, device=device)
    
    # 4. æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ scatter_add æˆ– index_add_ (è¿™é‡Œç”¨ index_add_ æ›´é€šç”¨)
    # å°† valid_embeddings åŠ åˆ°å¯¹åº”çš„ sum_embeddings è¡Œä¸­
    sum_embeddings.index_add_(0, valid_labels, valid_embeddings)
    
    # è®¡ç®—è®¡æ•° (åŠ 1.0)
    ones = torch.ones_like(valid_labels, dtype=torch.float)
    count_reads.index_add_(0, valid_labels, ones)
    
    # 5. è½¬æ¢ä¸ºå­—å…¸è¾“å‡º (ä¿æŒåŸæœ‰æ¥å£å…¼å®¹æ€§)
    centroids = {}
    cluster_sizes = {}
    
    valid_clusters_count = 0
    
    # å°† Tensor è½¬å› CPU å¤„ç†å­—å…¸ (å› ä¸ºæ­¤æ—¶ K åªæœ‰ 10000ï¼Œå¾ªç¯å¾ˆå¿«)
    # é¿å…åœ¨ GPU ä¸Šåšå¤§è§„æ¨¡å­—å…¸æ“ä½œ
    sum_emb_cpu = sum_embeddings.cpu()
    counts_cpu = count_reads.cpu()
    unique_ids_cpu = unique_cluster_ids.cpu().numpy()
    
    for k in unique_ids_cpu:
        count = counts_cpu[k].item()
        if count < 2: # ä¿æŒä½ ä¹‹å‰çš„é€»è¾‘ï¼šè‡³å°‘2ä¸ªreads
            continue
            
        # è®¡ç®—å¹³å‡å€¼
        centroid = sum_emb_cpu[k] / count
        
        centroids[int(k)] = centroid
        cluster_sizes[int(k)] = int(count)
        valid_clusters_count += 1
        
    print(f"   ğŸ“ ç°‡ä¸­å¿ƒè®¡ç®—å®Œæˆ:")
    print(f"      æœ‰æ•ˆç°‡æ•°: {valid_clusters_count}")
    if cluster_sizes:
        avg_size = sum(cluster_sizes.values()) / len(cluster_sizes)
        print(f"      å¹³å‡æœ‰æ•ˆç°‡å¤§å°: {avg_size:.1f}")
        
    return centroids, cluster_sizes

def refine_low_confidence_reads(embeddings, labels, low_conf_mask, centroids, delta):
    """
    âœ… [å‘é‡åŒ–ä¼˜åŒ–ç‰ˆ] 
    ä½¿ç”¨çŸ©é˜µè¿ç®—æ›¿ä»£åŒé‡å¾ªç¯ï¼Œç§’çº§å®Œæˆ 20ä¸‡ x 1ä¸‡ çš„åŒ¹é…
    """
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    # æå–ä½ç½®ä¿¡åº¦çš„ embeddings
    low_conf_indices = torch.where(low_conf_mask)[0]
    num_low_conf = len(low_conf_indices)
    
    if num_low_conf == 0:
        return new_labels, noise_mask, {'reassigned': 0, 'marked_noise': 0, 'kept_unchanged': 0}

    print(f"\n   ğŸ”„ æ­£åœ¨æ‰¹é‡ä¿®æ­£ {num_low_conf} ä¸ªä½ç½®ä¿¡åº¦reads...")
    
    # 1. å‡†å¤‡ç°‡ä¸­å¿ƒçŸ©é˜µ
    # å°†å­—å…¸è½¬æ¢ä¸º tensor: (K, D)
    sorted_cluster_ids = sorted(centroids.keys())
    cluster_matrix = torch.stack([centroids[k] for k in sorted_cluster_ids]) # (K, D)
    cluster_ids_tensor = torch.tensor(sorted_cluster_ids, device=embeddings.device) # (K,)
    
    # 2. å‡†å¤‡æŸ¥è¯¢å‘é‡
    query_embeddings = embeddings[low_conf_indices] # (M, D)
    
    # 3. è®¡ç®—è·ç¦»çŸ©é˜µ (M, K)
    # ä¸ºäº†é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸ (å¦‚æœ M*K å¾ˆå¤§)ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†å—è®¡ç®—
    # 20ä¸‡ * 1ä¸‡ * 4 bytes â‰ˆ 8GBï¼Œå¦‚æœä½ æ˜¾å­˜æœ‰24Gï¼Œå¯ä»¥ç›´æ¥ç®—ã€‚ä¿é™©èµ·è§åˆ†å—ã€‚
    
    batch_size = 5000 # æ¯æ¬¡å¤„ç† 5000 ä¸ª reads
    reassigned = 0
    marked_noise = 0
    
    for i in range(0, num_low_conf, batch_size):
        end = min(i + batch_size, num_low_conf)
        batch_queries = query_embeddings[i:end] # (B, D)
        
        # è®¡ç®—è¯¥æ‰¹æ¬¡åˆ°æ‰€æœ‰ç°‡ä¸­å¿ƒçš„è·ç¦» (B, K)
        dists = torch.cdist(batch_queries, cluster_matrix)
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç°‡
        min_dists, min_indices = torch.min(dists, dim=1) # (B,)
        
        # è·å–å¯¹åº”çš„ Cluster ID
        best_cluster_ids = cluster_ids_tensor[min_indices]
        
        # å†³ç­–
        # æ»¡è¶³ delta é˜ˆå€¼
        valid_mask = min_dists < delta
        
        # å½“å‰æ‰¹æ¬¡åœ¨å…¨å±€çš„ç´¢å¼•
        global_indices = low_conf_indices[i:end]
        
        # 1. é‡æ–°åˆ†é… (valid_mask ä¸º True çš„éƒ¨åˆ†)
        valid_indices = global_indices[valid_mask]
        new_assignments = best_cluster_ids[valid_mask]
        
        # ç»Ÿè®¡é‡æ–°åˆ†é…çš„æ•°é‡ (æ ‡ç­¾å‘ç”Ÿå˜åŒ–çš„)
        original_labels = labels[valid_indices]
        reassigned += (original_labels != new_assignments).sum().item()
        
        new_labels[valid_indices] = new_assignments
        
        # 2. æ ‡è®°å™ªå£° (valid_mask ä¸º False çš„éƒ¨åˆ†)
        noise_indices = global_indices[~valid_mask]
        new_labels[noise_indices] = -1
        noise_mask[noise_indices] = True
        marked_noise += len(noise_indices)

    print(f"   âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned}")
    print(f"      æ ‡è®°å™ªå£°: {marked_noise}")
    print(f"      ä¿æŒä¸å˜: {num_low_conf - reassigned - marked_noise}")

    stats = {
        'reassigned': reassigned,
        'marked_noise': marked_noise,
        'kept_unchanged': num_low_conf - reassigned - marked_noise
    }

    return new_labels, noise_mask, stats

def compute_adaptive_delta(embeddings, centroids, percentile=10):
    """
    âœ… è‡ªé€‚åº”è®¡ç®—deltaé˜ˆå€¼
    
    Args:
        embeddings: (N, D)
        centroids: dict[label] -> (D,)
        percentile: int, ç™¾åˆ†ä½æ•°ï¼ˆæ¥æ”¶æœ€è¿‘çš„X%ï¼‰
    
    Returns:
        delta: float
    """
    all_distances = []
    
    for k, ck in centroids.items():
        dists = torch.norm(embeddings - ck.unsqueeze(0), dim=1)
        all_distances.append(dists)
    
    all_distances = torch.cat(all_distances)
    delta = torch.quantile(all_distances, percentile / 100.0).item()
    
    print(f"   ğŸ¯ è‡ªé€‚åº”delta: {delta:.4f} (æ¥æ”¶æœ€è¿‘{percentile}%çš„reads)")
    
    return delta