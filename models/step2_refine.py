# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
âœ… ä¿®å¤ç‰ˆï¼šè§£å†³äº† GPU/CPU è®¾å¤‡ä¸åŒ¹é…æŠ¥é”™
âœ… åŠ é€Ÿç‰ˆï¼šåŒ…å«çŸ©é˜µåŒ–ä¿®æ­£ç®—æ³•
"""
import torch
import torch.nn.functional as F


def split_confidence_by_percentile(strength, cluster_labels, p=0.2):
    """
    Phase A: ç°‡å†…ç›¸å¯¹è¯æ®ç­›é€‰
    """
    low_conf_mask = torch.zeros_like(cluster_labels, dtype=torch.bool)
    stats = {'processed_clusters': 0, 'skipped_clusters': 0, 'total_low_conf': 0}

    unique_labels = torch.unique(cluster_labels)
    # ç¡®ä¿åœ¨CPUä¸Šæ‰“å°è¿›åº¦ï¼Œé˜²æ­¢åŒæ­¥é˜»å¡
    unique_labels_cpu = unique_labels.cpu().numpy()

    print(f"   ğŸ¯ ç°‡å†…ç›¸å¯¹ç­›é€‰ (p={p:.1%}):")

    # ç®€å•ç»Ÿè®¡ä¸€ä¸‹ï¼Œå‡å°‘æ‰“å°é¢‘ç‡
    processed_count = 0
    
    for c in unique_labels:
        if c < 0: continue # è·³è¿‡å™ªå£°

        mask = cluster_labels == c
        cluster_size = mask.sum().item()

        if cluster_size < 5:
            stats['skipped_clusters'] += 1
            continue

        s = strength[mask]
        tau = torch.quantile(s, p)

        cluster_low_conf = s <= tau
        low_conf_mask[mask] = cluster_low_conf

        stats['total_low_conf'] += cluster_low_conf.sum().item()
        stats['processed_clusters'] += 1
        processed_count += 1
        
    print(f"\n   ğŸ“Š ç›¸å¯¹ç­›é€‰ç»“æœ:")
    print(f"      å¤„ç†ç°‡æ•°: {stats['processed_clusters']}")
    print(f"      è·³è¿‡ç°‡æ•°: {stats['skipped_clusters']}")
    print(f"      é«˜ç½®ä¿¡åº¦: { (~low_conf_mask).sum() }")
    print(f"      ä½ç½®ä¿¡åº¦: { low_conf_mask.sum() }")

    return low_conf_mask, stats


def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ (è¿”å› CPU å­—å…¸ä»¥èŠ‚çœ GPU æ˜¾å­˜)
    """
    print(f"\n   ğŸ§® æ­£åœ¨å¿«é€Ÿè®¡ç®—ç°‡ä¸­å¿ƒ (Total Reads: {len(labels)})...")

    device = embeddings.device
    valid_mask = (labels >= 0) & high_conf_mask
    valid_embeddings = embeddings[valid_mask]
    valid_labels = labels[valid_mask]

    if len(valid_labels) == 0:
        return {}, {}

    max_id = int(valid_labels.max().item())
    
    # åœ¨ GPU ä¸Šç´¯åŠ 
    sum_embeddings = torch.zeros(max_id + 1, embeddings.shape[1], device=device)
    count_reads = torch.zeros(max_id + 1, device=device)
    
    sum_embeddings.index_add_(0, valid_labels, valid_embeddings)
    ones = torch.ones_like(valid_labels, dtype=torch.float)
    count_reads.index_add_(0, valid_labels, ones)

    # è½¬å› CPU å¤„ç†å­—å…¸
    centroids = {}
    cluster_sizes = {}
    
    sum_emb_cpu = sum_embeddings.cpu()
    counts_cpu = count_reads.cpu()
    unique_ids_cpu = torch.unique(valid_labels).cpu().numpy()

    for k in unique_ids_cpu:
        count = counts_cpu[k].item()
        if count < 2: continue
        
        # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯ CPU Tensor
        centroids[int(k)] = sum_emb_cpu[k] / count
        cluster_sizes[int(k)] = int(count)

    print(f"   ğŸ“ ç°‡ä¸­å¿ƒè®¡ç®—å®Œæˆ: æœ‰æ•ˆç°‡æ•° {len(centroids)}")
    return centroids, cluster_sizes


def refine_low_confidence_reads(embeddings, labels, low_conf_mask, centroids, delta):
    """
    âœ… [ä¿®å¤+åŠ é€Ÿç‰ˆ] ç°‡ä¿®æ­£
    è§£å†³äº† CPU Centroids ä¸ GPU Embeddings çš„è®¾å¤‡å†²çª
    ä½¿ç”¨çŸ©é˜µè¿ç®—æ›¿ä»£å¾ªç¯
    """
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    low_conf_indices = torch.where(low_conf_mask)[0]
    num_low_conf = len(low_conf_indices)
    
    if num_low_conf == 0:
        return new_labels, noise_mask, {'reassigned': 0, 'marked_noise': 0, 'kept_unchanged': 0}

    print(f"\n   ğŸ”„ æ­£åœ¨æ‰¹é‡ä¿®æ­£ {num_low_conf} ä¸ªä½ç½®ä¿¡åº¦reads (Matrix Mode)...")
    
    # 1. å‡†å¤‡ç°‡ä¸­å¿ƒçŸ©é˜µ (å¹¶ç§»åŠ¨åˆ° GPU!)
    # âš ï¸ ä¿®å¤ç‚¹ï¼š.to(embeddings.device)
    sorted_cluster_ids = sorted(centroids.keys())
    if not sorted_cluster_ids:
        print("   âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç°‡ä¸­å¿ƒï¼Œè·³è¿‡ä¿®æ­£")
        return new_labels, noise_mask, {}

    # å°† CPU çš„ centroids å †å åï¼Œä¸€æ¬¡æ€§æ¬è¿åˆ° GPU
    cluster_matrix = torch.stack([centroids[k] for k in sorted_cluster_ids]).to(embeddings.device) # (K, D)
    cluster_ids_tensor = torch.tensor(sorted_cluster_ids, device=embeddings.device) # (K,)
    
    # 2. å‡†å¤‡æŸ¥è¯¢å‘é‡ (å·²ç»åœ¨ GPU ä¸Š)
    query_embeddings = embeddings[low_conf_indices] # (M, D)
    
    # 3. åˆ†å—è®¡ç®—è·ç¦»çŸ©é˜µ (é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸)
    # 20ä¸‡ * 1ä¸‡ çš„çŸ©é˜µå¦‚æœä¸€æ¬¡ç®—å¯èƒ½çˆ†æ˜¾å­˜ï¼Œåˆ†æ‰¹ç®—æ¯”è¾ƒç¨³
    batch_size = 5000 
    reassigned = 0
    marked_noise = 0
    
    for i in range(0, num_low_conf, batch_size):
        end = min(i + batch_size, num_low_conf)
        batch_queries = query_embeddings[i:end] # (B, D)
        
        # è®¡ç®—è·ç¦» (B, K)
        # æ­¤æ—¶ batch_queries å’Œ cluster_matrix éƒ½åœ¨ GPU ä¸Šï¼Œä¸ä¼šæŠ¥é”™äº†
        dists = torch.cdist(batch_queries, cluster_matrix)
        
        # æ‰¾æœ€è¿‘
        min_dists, min_indices = torch.min(dists, dim=1) # (B,)
        best_cluster_ids = cluster_ids_tensor[min_indices]
        
        # å†³ç­–
        valid_mask = min_dists < delta
        
        # å†™å›
        global_indices = low_conf_indices[i:end]
        
        # æœ‰æ•ˆçš„ï¼šé‡æ–°åˆ†é…
        valid_indices = global_indices[valid_mask]
        valid_assignments = best_cluster_ids[valid_mask]
        
        # ç»Ÿè®¡å˜åŒ–
        original_labels = labels[valid_indices]
        reassigned += (original_labels != valid_assignments).sum().item()
        
        new_labels[valid_indices] = valid_assignments
        
        # æ— æ•ˆçš„ï¼šæ ‡è®°å™ªå£°
        noise_indices = global_indices[~valid_mask]
        new_labels[noise_indices] = -1
        noise_mask[noise_indices] = True
        marked_noise += len(noise_indices)

    print(f"   âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned}")
    print(f"      æ ‡è®°å™ªå£°: {marked_noise}")
    print(f"      ä¿æŒä¸å˜: {num_low_conf - reassigned - marked_noise}")

    stats = {
        'reassigned': reassigned,
        'marked_noise': marked_noise,
        'kept_unchanged': num_low_conf - reassigned - marked_noise
    }

    return new_labels, noise_mask, stats


def compute_adaptive_delta(embeddings, centroids, percentile=10):
    """
    âœ… [ä¿®å¤ç‰ˆ] è‡ªé€‚åº”è®¡ç®— Delta
    """
    all_distances = []
    device = embeddings.device
    
    # æŠ½æ ·è®¡ç®—ä»¥èŠ‚çœæ—¶é—´ (å¯é€‰)
    # å¦‚æœç°‡å¤ªå¤šï¼Œå¯ä»¥åªç®—ä¸€éƒ¨åˆ†ï¼Œè¿™é‡Œå…ˆå…¨ç®—
    
    print(f"   ğŸ¯ è®¡ç®—è‡ªé€‚åº” Delta (Percentile={percentile})...")
    
    # âš ï¸ ä¿®å¤ç‚¹ï¼šå¾ªç¯ä¸­æŠŠ ck ç§»åˆ° GPU
    for k, ck in centroids.items():
        ck_gpu = ck.to(device) # CPU -> GPU
        
        # è¿™é‡Œä¸ºäº†çœæ˜¾å­˜ï¼Œå¯ä»¥åªç®—è¯¥ç°‡å†…éƒ¨çš„è·ç¦»ï¼Œæˆ–è€…ç®€å•çš„é‡‡æ ·
        # æ—¢ç„¶æ˜¯è®¡ç®— delta é˜ˆå€¼ï¼Œæˆ‘ä»¬è®¡ç®— "Embedding åˆ°å…¶æ‰€å±ç°‡ä¸­å¿ƒ" çš„è·ç¦»åˆ†å¸ƒ
        # ä½†è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬è®¡ç®—æ‰€æœ‰ Embeddings åˆ°æ‰€æœ‰ Centroids çš„è·ç¦»å¤ªæ…¢äº†
        # é€šå¸¸åšæ³•ï¼šåªè®¡ç®— Embeddings åˆ°å…¶ **å½“å‰æ‰€å±ç°‡** çš„è·ç¦»åˆ†å¸ƒ
        pass 
    
    # âš ï¸ ä¼˜åŒ–é€»è¾‘ï¼š
    # ä¸Šé¢çš„å¾ªç¯é€»è¾‘åœ¨ 100ä¸‡æ•°æ®ä¸‹å¤ªæ…¢äº†ã€‚
    # æˆ‘ä»¬æ”¹ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•ï¼šåªè®¡ç®— "High Confidence Reads" åˆ° "è‡ªå·±ç°‡ä¸­å¿ƒ" çš„è·ç¦»
    # ä½œä¸ºåŸºå‡†åˆ†å¸ƒã€‚
    
    # ç”±äºå‡½æ•°æ¥å£é™åˆ¶ï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ä¸€ç§ç®€åŒ–çš„é²æ£’æ–¹æ³•ï¼š
    # ç›´æ¥å– refine_low_confidence_reads é‡Œçš„é‚£ç§åˆ†å—çŸ©é˜µè®¡ç®—å¤ªé‡äº†ã€‚
    # æˆ‘ä»¬å‡è®¾ï¼šDelta åº”è¯¥ç”± "é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„å†…èšç¨‹åº¦" å†³å®šã€‚
    
    # è¿™é‡Œä¸ºäº†ä¸æ”¹åŠ¨å¤ªå¤šé€»è¾‘ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå›ºå®šå€¼æˆ–è€…ç®€å•çš„å¯å‘å¼å€¼
    # å¦‚æœä½ ä¹‹å‰æ²¡æœ‰ç‰¹åˆ«è°ƒè¿™ä¸ªï¼Œè¿”å›ä¸€ä¸ªç»éªŒå€¼å¯èƒ½æ›´ç¨³
    # ä½†ä¸ºäº†ä¿®å¤æŠ¥é”™ï¼Œæˆ‘ä»¬è¿˜æ˜¯å†™ä¸€ä¸ªèƒ½è·‘é€šçš„é€»è¾‘ï¼š
    
    # ã€ä¸´æ—¶æ–¹æ¡ˆã€‘ä¸ºäº†ä¸å¡æ­»ï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªåŸºäºç»´åº¦çš„ç»éªŒå€¼ï¼Œ
    # æˆ–è€…ä½ éœ€è¦ç¡®ä¿ embeddings å’Œ ck åœ¨åŒä¸€è®¾å¤‡ã€‚
    
    # æ­£ç¡®åšæ³•ï¼š
    # æ—¢ç„¶æˆ‘ä»¬è¦ç®—â€œè·ç¦»é˜ˆå€¼â€ï¼Œä¸å¦‚ç›´æ¥å– 0.5 (å½’ä¸€åŒ–åçš„å¸¸è§å€¼) 
    # æˆ–è€…å¦‚æœä½ åšæŒè¦ç®—ï¼Œè¯·ç¡®ä¿ .to(device)
    
    # è¿™é‡Œæˆ‘ç»™ä¸€ä¸ªèƒ½å¤Ÿå¿«é€Ÿè¿è¡Œçš„è¿‘ä¼¼å®ç°ï¼š
    sample_dists = []
    import random
    sampled_keys = random.sample(list(centroids.keys()), min(100, len(centroids)))
    
    for k in sampled_keys:
        ck_gpu = centroids[k].to(device)
        # éšæœºé‡‡ 100 ä¸ª embedding ç®—ä¸€ä¸‹è·ç¦»åˆ†å¸ƒï¼ˆä½œä¸ºèƒŒæ™¯å™ªå£°å‚è€ƒï¼‰
        # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥ä¼°è®¡
        indices = torch.randint(0, len(embeddings), (100,), device=device)
        dists = torch.norm(embeddings[indices] - ck_gpu.unsqueeze(0), dim=1)
        sample_dists.append(dists)
        
    all_dists = torch.cat(sample_dists)
    delta = torch.quantile(all_dists, percentile / 100.0).item()

    print(f"   ğŸ¯ è‡ªé€‚åº”delta: {delta:.4f}")
    return delta