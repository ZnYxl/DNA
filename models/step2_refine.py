# models/step2_refine.py
"""
Step2: Evidence-Guided Cluster Refinement (Vectorized Optimized Version)
æ ¸å¿ƒï¼šç”¨Step1å­¦åˆ°çš„è¯æ®å¼ºåº¦æ¥ä¿®æ­£ç°‡ç»“æ„
âœ… ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ï¼šç°‡å†…æ¯”è¾ƒ
âœ… å‘é‡åŒ–åŠ é€Ÿï¼šç§»é™¤å…³é”®è·¯å¾„ä¸Šçš„Pythonå¾ªç¯
"""
import torch
import torch.nn.functional as F

def split_confidence_by_percentile(strength, cluster_labels, p=0.2):
    """
    âœ… Phase A: ç°‡å†…ç›¸å¯¹è¯æ®ç­›é€‰
    æ¯ä¸ªç°‡é‡Œï¼Œå–strengthæœ€ä½çš„p%ä½œä¸ºä½ç½®ä¿¡åº¦
    """
    low_conf_mask = torch.zeros_like(cluster_labels, dtype=torch.bool)
    stats = {'processed_clusters': 0, 'skipped_clusters': 0, 'total_low_conf': 0}
    
    unique_labels = torch.unique(cluster_labels)
    
    print(f"    ğŸ¯ ç°‡å†…ç›¸å¯¹ç­›é€‰ (p={p:.1%}):")
    
    # è¿™é‡Œä¿æŒå¾ªç¯æ˜¯OKçš„ï¼Œå› ä¸ºç°‡çš„æ•°é‡è¿œå°äºReadsæ•°é‡ï¼Œä¸”quantileæ“ä½œæ— æ³•ç®€å•çš„å…¨å±€å‘é‡åŒ–ï¼ˆæ¯ä¸ªç°‡é˜ˆå€¼ä¸åŒï¼‰
    for c in unique_labels:
        if c < 0:  # è·³è¿‡å™ªå£°
            continue
            
        mask = cluster_labels == c
        cluster_size = mask.sum().item()
        
        if cluster_size < 5:  # å¤ªå°çš„ç°‡è·³è¿‡ï¼Œç›´æ¥è§†ä¸ºé«˜ç½®ä¿¡åº¦æˆ–ç”±åç»­é€»è¾‘å¤„ç†
            # print(f"      ç°‡{c}: {cluster_size} reads (å¤ªå°ï¼Œä¿ç•™)")
            stats['skipped_clusters'] += 1
            continue
        
        # è¯¥ç°‡çš„strength
        s = strength[mask]
        tau = torch.quantile(s, p)  # ç¬¬påˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        
        # æ ‡è®°è¯¥ç°‡å†…çš„ä½ç½®ä¿¡åº¦reads
        cluster_low_conf = s <= tau
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åˆ©ç”¨maskç´¢å¼•å›å¡«
        # è¿™ç§å†™æ³•åœ¨PyTorchä¸­æ˜¯å®‰å…¨çš„ï¼šlow_conf_mask[mask]çš„shapeç­‰äºcluster_low_conf
        low_conf_mask[mask] = cluster_low_conf
        
        low_count = cluster_low_conf.sum().item()
        stats['total_low_conf'] += low_count
        stats['processed_clusters'] += 1
        
        # åªæœ‰åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰æ‰“å°æ¯ä¸ªç°‡çš„è¯¦æƒ…ï¼Œé˜²æ­¢åˆ·å±
        # print(f"      ç°‡{c}: Ï„={tau:.3f}, ä½ç½®ä¿¡åº¦={low_count}")
    
    high_conf_mask = ~low_conf_mask
    
    print(f"\n    ğŸ“Š ç›¸å¯¹ç­›é€‰ç»“æœ:")
    print(f"      å¤„ç†ç°‡æ•°: {stats['processed_clusters']}")
    print(f"      è·³è¿‡ç°‡æ•°: {stats['skipped_clusters']}")
    print(f"      é«˜ç½®ä¿¡åº¦: {high_conf_mask.sum()}/{len(strength)} ({high_conf_mask.float().mean():.1%})")
    print(f"      ä½ç½®ä¿¡åº¦: {low_conf_mask.sum()}/{len(strength)} ({low_conf_mask.float().mean():.1%})")
    
    return low_conf_mask, stats

def compute_cluster_centroids(embeddings, labels, high_conf_mask):
    """
    âœ… å‘é‡åŒ–è®¡ç®—ç°‡ä¸­å¿ƒ (åªç”¨é«˜ç½®ä¿¡åº¦reads)
    ä½¿ç”¨ scatter/index_add åŠ é€Ÿï¼Œé¿å… Python å¾ªç¯
    """
    # 1. ç­›é€‰æœ‰æ•ˆæ•°æ®
    valid_mask = (labels >= 0) & high_conf_mask
    if valid_mask.sum() == 0:
        return {}, {}
        
    valid_emb = embeddings[valid_mask]
    valid_labels = labels[valid_mask]
    
    # 2. æ˜ å°„ Label åˆ° 0..K-1 çš„è¿ç»­ç´¢å¼•ï¼Œä»¥ä¾¿å‘é‡åŒ–ç´¯åŠ 
    unique_labels, inverse_indices = torch.unique(valid_labels, return_inverse=True)
    num_clusters = len(unique_labels)
    dim = embeddings.shape[1]
    
    # 3. åˆå§‹åŒ–ç´¯åŠ å®¹å™¨
    sum_emb = torch.zeros(num_clusters, dim, device=embeddings.device)
    counts = torch.zeros(num_clusters, device=embeddings.device)
    
    # 4. å‘é‡åŒ–ç´¯åŠ  (Scatter Add / Index Add)
    # counts: ç»Ÿè®¡æ¯ä¸ªç°‡æœ‰å¤šå°‘ä¸ªç‚¹
    counts.index_add_(0, inverse_indices, torch.ones_like(inverse_indices, dtype=torch.float))
    
    # sum_emb: ç´¯åŠ å‘é‡
    # index_add_ éœ€è¦ dim åŒ¹é…ï¼Œæ‰€ä»¥å¯¹ inverse_indices ä¸éœ€è¦åšç‰¹æ®Šå¤„ç†ï¼Œä½† index_add_ æ˜¯æŒ‰è¡Œæ“ä½œçš„
    sum_emb.index_add_(0, inverse_indices, valid_emb)
    
    # 5. è®¡ç®—å‡å€¼
    # clampé¿å…é™¤ä»¥0ï¼ˆè™½ç„¶é€»è¾‘ä¸Šuniqueä¿è¯äº†è‡³å°‘æœ‰1ä¸ªï¼Œä½†å®‰å…¨ç¬¬ä¸€ï¼‰
    means = sum_emb / counts.unsqueeze(1).clamp(min=1)
    
    # 6. è½¬å› Dict æ ¼å¼ (å…¼å®¹åç»­æ¥å£ï¼ŒåŒæ—¶è¿‡æ»¤æå°ç°‡)
    centroids = {}
    cluster_sizes = {}
    valid_clusters_count = 0
    
    # å°† Tensor æ•°æ®è½¬å› CPU å­—å…¸
    means_cpu = means  # ä¿æŒåœ¨åŸè®¾å¤‡æˆ– .cpu() å–å†³äºåç»­éœ€æ±‚ï¼Œè¿™é‡Œå»ºè®®ä¿æŒåŸè®¾å¤‡ç›´åˆ°æœ€å
    counts_cpu = counts
    unique_labels_cpu = unique_labels
    
    for i in range(num_clusters):
        lbl = int(unique_labels_cpu[i].item())
        cnt = int(counts_cpu[i].item())
        
        if cnt >= 2: # è‡³å°‘2ä¸ªé«˜ç½®ä¿¡åº¦readsæ‰ç®—æœ‰æ•ˆä¸­å¿ƒ
            centroids[lbl] = means_cpu[i] # ä¿æŒ Tensor
            cluster_sizes[lbl] = cnt
            valid_clusters_count += 1
    
    print(f"\n    ğŸ“ ç°‡ä¸­å¿ƒç»Ÿè®¡ (Vectorized):")
    print(f"      æœ‰æ•ˆç°‡æ•°: {valid_clusters_count}/{num_clusters}")
    if cluster_sizes:
        avg_size = sum(cluster_sizes.values()) / len(cluster_sizes)
        print(f"      å¹³å‡ç°‡å¤§å°: {avg_size:.1f}")
    
    return centroids, cluster_sizes

def compute_adaptive_delta(embeddings, labels, centroids, high_conf_mask, percentile=95):
    """
    âœ… è‡ªé€‚åº” Delta è®¡ç®—
    é€»è¾‘ï¼šè®¡ç®—ã€é«˜ç½®ä¿¡åº¦ç‚¹ã€‘åˆ°ã€è‡ªèº«ç°‡ä¸­å¿ƒã€‘çš„è·ç¦»åˆ†å¸ƒï¼Œå–ç¬¬ p åˆ†ä½æ•°ã€‚
    è¿™æ„å‘³ç€ï¼šå¦‚æœä¸€ä¸ªç‚¹è·ç¦»ä¸­å¿ƒçš„è·ç¦»è¶…è¿‡äº†95%çš„æ­£å¸¸ç‚¹ï¼Œå®ƒå°±è¢«è§†ä¸ºâ€œå¤ªè¿œâ€ã€‚
    """
    if not centroids:
        return 0.5 # fallback
        
    valid_mask = (labels >= 0) & high_conf_mask
    valid_indices = torch.where(valid_mask)[0]
    
    if len(valid_indices) == 0:
        return 0.5
        
    # ä¸ºäº†å‘é‡åŒ–ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ª "aligned_centroids" çŸ©é˜µ
    # å³ï¼šå¯¹äºç¬¬ i ä¸ª readï¼Œæ‰¾åˆ°å®ƒ label å¯¹åº”çš„ centroid
    
    # 1. å‡†å¤‡æ•°æ®
    curr_embs = embeddings[valid_indices]
    curr_labels = labels[valid_indices]
    
    # 2. æ„é€ ä¸­å¿ƒå¼ é‡
    # å¹¶ä¸æ˜¯æ‰€æœ‰ valid_labels éƒ½åœ¨ centroids å­—å…¸é‡Œ (å› ä¸º centroids è¿‡æ»¤äº† count<2 çš„)
    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç­›é€‰
    keys_tensor = torch.tensor(list(centroids.keys()), device=embeddings.device)
    vals_stack = torch.stack(list(centroids.values()))
    
    # åˆ›å»ºä¸€ä¸ªæŸ¥æ‰¾è¡¨ (Label -> Index in vals_stack)
    # å‡è®¾ Label èŒƒå›´å¯èƒ½å¾ˆå¤§ï¼Œä¸èƒ½ç›´æ¥ç”¨æ•°ç»„ lookupã€‚
    # è¿™é‡Œç”¨ä¸€ä¸ªç®€å•çš„æŠ€å·§ï¼šåªè®¡ç®—é‚£äº› label åœ¨ centroids é‡Œçš„ç‚¹
    
    # å°† dict è½¬ä¸º lookup å¯èƒ½ä¼šæ…¢ï¼Œä¸å¦‚ç›´æ¥éå†è®¡ç®— distances (å¦‚æœç°‡å¾ˆå°‘)
    # æˆ–è€…ï¼Œæ›´ç®€å•çš„æ–¹æ³•ï¼š
    distances = []
    
    # è¿™é‡Œç”¨å¾ªç¯ç°‡çš„æ–¹å¼æ¯”è¾ƒå®‰å…¨ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦â€œç‚¹åˆ°è‡ªèº«ä¸­å¿ƒâ€çš„è·ç¦»
    # ä¸” valid_indices çš„æ•°é‡å¯èƒ½å¾ˆå¤§ï¼Œä½† unique labels ä¸ä¼šå¤ªå¤§
    unique_valid_labels = torch.unique(curr_labels)
    
    for k in unique_valid_labels:
        k_item = int(k.item())
        if k_item not in centroids:
            continue
            
        ck = centroids[k_item] # (D,)
        
        # æ‰¾å‡ºå±äºç°‡ k çš„é«˜ç½®ä¿¡åº¦ç‚¹
        mask_k = (curr_labels == k)
        embs_k = curr_embs[mask_k]
        
        # è®¡ç®—è·ç¦»
        dists_k = torch.norm(embs_k - ck.unsqueeze(0), dim=1)
        distances.append(dists_k)
    
    if not distances:
        print("    âš ï¸ æ— æ³•è®¡ç®— Deltaï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.5")
        return 0.5
        
    all_distances = torch.cat(distances)
    
    # å–åˆ†ä½æ•°
    delta = torch.quantile(all_distances, percentile / 100.0).item()
    
    print(f"    ğŸ¯ è‡ªé€‚åº” Delta: {delta:.4f} (åŸºäºé«˜ç½®ä¿¡åº¦ç‚¹åˆ†å¸ƒçš„ {percentile}%)")
    return delta

def refine_low_confidence_reads(embeddings, labels, low_conf_mask, 
                                centroids, delta):
    """
    âœ… Phase B: ç°‡ä¿®æ­£ (Vectorized Matrix Version)
    ä½¿ç”¨çŸ©é˜µè¿ç®—ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ä½ç½®ä¿¡åº¦ç‚¹åˆ°æ‰€æœ‰ä¸­å¿ƒçš„è·ç¦»
    """
    low_conf_indices = torch.where(low_conf_mask)[0]
    num_low = len(low_conf_indices)
    
    if num_low == 0 or not centroids:
        return labels, torch.zeros_like(labels, dtype=torch.bool), {'reassigned': 0, 'marked_noise': 0}
    
    print(f"\n    ğŸ”„ å‘é‡åŒ–ä¿®æ­£ {num_low} ä¸ªä½ç½®ä¿¡åº¦ reads...")
    
    # 1. å‡†å¤‡ Query æ•°æ® (M, D)
    query_embs = embeddings[low_conf_indices]
    
    # 2. å‡†å¤‡ Reference æ•°æ® (Centroids) -> Matrix (K, D)
    # å¿…é¡»æ’åº keys ä»¥ä¾¿åç»­æ˜ å°„å› label
    sorted_keys = sorted(centroids.keys())
    centroid_labels = torch.tensor(sorted_keys, device=embeddings.device)
    centroid_matrix = torch.stack([centroids[k] for k in sorted_keys])
    
    # 3. çŸ©é˜µè®¡ç®—è·ç¦» (M, K) - æ ¸å¿ƒåŠ é€Ÿç‚¹
    # cdist è®¡ç®— query ä¸­æ¯ä¸€è¡Œåˆ° centroid_matrix ä¸­æ¯ä¸€è¡Œçš„æ¬§æ°è·ç¦»
    dists_matrix = torch.cdist(query_embs, centroid_matrix)
    
    # 4. æ‰¾åˆ°æœ€è¿‘çš„ç°‡
    min_dists, min_indices = torch.min(dists_matrix, dim=1) # (M,) values, (M,) indices
    
    # 5. å†³ç­–
    # æ»¡è¶³è·ç¦»é˜ˆå€¼çš„ mask
    assign_mask = min_dists < delta
    
    # 6. ç”Ÿæˆæ–°æ ‡ç­¾
    new_labels = labels.clone()
    noise_mask = torch.zeros_like(labels, dtype=torch.bool)
    
    # A. é‡æ–°åˆ†é…çš„ç‚¹
    # è·å– centroid_matrix çš„ç´¢å¼• -> æ˜ å°„å› çœŸå® Label ID
    target_centroid_idx = min_indices[assign_mask]
    target_labels = centroid_labels[target_centroid_idx]
    
    # è·å–åŸå§‹ reads çš„å…¨å±€ç´¢å¼•
    reassigned_global_indices = low_conf_indices[assign_mask]
    
    # èµ‹å€¼
    new_labels[reassigned_global_indices] = target_labels
    
    # B. æ ‡è®°ä¸ºå™ªå£°çš„ç‚¹
    noise_global_indices = low_conf_indices[~assign_mask]
    new_labels[noise_global_indices] = -1
    noise_mask[noise_global_indices] = True
    
    # ç»Ÿè®¡
    reassigned_count = len(reassigned_global_indices)
    noise_count = len(noise_global_indices)
    unchanged_count = num_low - reassigned_count - noise_count # é€»è¾‘ä¸Šåº”è¯¥æ˜¯0ï¼Œå› ä¸ºè¦ä¹ˆassignè¦ä¹ˆnoise
    
    print(f"    âœ… ä¿®æ­£å®Œæˆ:")
    print(f"      é‡æ–°åˆ†é…: {reassigned_count}")
    print(f"      æ ‡è®°å™ªå£°: {noise_count}")
    
    stats = {
        'reassigned': reassigned_count,
        'marked_noise': noise_count
    }
    
    return new_labels, noise_mask, stats
