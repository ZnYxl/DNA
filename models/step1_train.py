# models/step1_train.py
"""
Step1 è®­ç»ƒä¸»ç¨‹  â€”  Hot Start æ¥åŠ›ç‰ˆæœ¬

ä¿®æ”¹æ¸…å•å¯¹åº”:
  - Round 1: åŠ è½½ FedDNA é¢„è®­ç»ƒæƒé‡, 20 epoch, lr=1e-4
  - Round 2+: åŠ è½½ä¸Šä¸€è½® checkpoint, 10 epoch, lr=1e-5
  - é‡‡æ ·å™¨åˆ‡æ¢åˆ° create_dynamic_samplerï¼ˆå†…éƒ¨ä¼šæŒ‰ round_idx å†³å®šå…¨é‡/ä¸‰åŒºåˆ¶ï¼‰
  - epoch æ—¥å¿—è¿½åŠ  u_epi_mean / u_ale_mean / queue_countï¼ˆæ¥è‡ª step1_model çš„ outputsï¼‰

çº¢çº¿ä¸åŠ¨:
  - Batch Size = 32
  - grad clip max_norm = 1.0
  - recon_loss æƒé‡ 10.0ï¼ˆåœ¨ model å†…éƒ¨ï¼‰
"""
import torch
import torch.optim as optim
import argparse
import os
import sys
import time
from datetime import datetime
import numpy as np

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir  = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.step1_model import Step1EvidentialModel, load_pretrained_feddna
from models.step1_data  import CloverDataLoader, Step1Dataset, create_dynamic_sampler
from models.step1_visualizer import Step1Visualizer


# ---------------------------------------------------------------------------
# Hot Start è¶…å‚
# ---------------------------------------------------------------------------
ROUND1_EPOCHS = 5
ROUND1_LR     = 1e-4
ROUND2_EPOCHS = 10
ROUND2_LR     = 1e-5


class ListBatchSampler:
    """æŠŠ List[List[int]] åŒ…è£…æˆ DataLoader å¯ç”¨çš„ batch_sampler"""
    def __init__(self, batches):
        self.batches = batches

    def __iter__(self):
        return iter(self.batches)

    def __len__(self):
        return len(self.batches)


def train_step1(args):
    """æ­¥éª¤ä¸€è®­ç»ƒä¸»å‡½æ•°ï¼ˆHot Start æ¥åŠ›ç‰ˆï¼‰"""
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    round_idx  = getattr(args, 'round_idx', 1)
    prev_state = getattr(args, 'prev_state', None)

    # =====================================================================
    # 1. åŠ è½½æ•°æ®
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ•°æ®åŠ è½½")
    print("=" * 60)

    labels_path = getattr(args, 'refined_labels', None)
    data_loader = CloverDataLoader(args.experiment_dir, labels_path=labels_path)
    dataset     = Step1Dataset(data_loader, max_len=args.max_length)

    # =====================================================================
    # 2. åˆ›å»ºæ¨¡å‹
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ§  æ¨¡å‹åˆ›å»º")
    print("=" * 60)

    num_clover_clusters = len(set(data_loader.clover_labels))
    num_gt_clusters     = len(data_loader.gt_cluster_seqs)
    num_clusters        = max(num_clover_clusters, num_gt_clusters, args.min_clusters)

    model = Step1EvidentialModel(
        dim=args.dim,
        max_length=args.max_length,
        num_clusters=num_clusters,
        device=device
    ).to(device)

    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   å½“å‰ç°‡æ•°: {num_clover_clusters}")

    # =====================================================================
    # 3. Hot Start æƒé‡åŠ è½½
    #    Round 1: FedDNA é¢„è®­ç»ƒæƒé‡
    #    Round 2+: ä¸Šä¸€è½®çš„ step1_final_model.pth
    # =====================================================================
    print("\n" + "=" * 60)
    print(f"ğŸ”‹ Hot Start (Round {round_idx})")
    print("=" * 60)

    if round_idx <= 1:
        # Round 1: åŠ è½½ FedDNA é¢„è®­ç»ƒ
        if args.feddna_checkpoint and os.path.exists(args.feddna_checkpoint):
            model = load_pretrained_feddna(model, args.feddna_checkpoint, device)
        else:
            print(f"   ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    else:
        # Round 2+: åŠ è½½ä¸Šä¸€è½® checkpoint
        prev_ckpt = getattr(args, 'prev_checkpoint', None)
        if prev_ckpt and os.path.exists(prev_ckpt):
            try:
                ckpt = torch.load(prev_ckpt, map_location=device)
                sd   = ckpt.get('model_state_dict', ckpt)

                # length_adapter é¢„åŠ è½½ï¼ˆå’Œ step2_runner åŒæ ·çš„ä¿®å¤ï¼‰
                if 'length_adapter.weight' in sd:
                    import torch.nn as nn
                    sh = sd['length_adapter.weight'].shape
                    model.length_adapter = nn.Linear(sh[1], sh[0]).to(device)
                    print(f"   ğŸ”§ é¢„åˆå§‹åŒ– length_adapter: {sh}")

                model.load_state_dict(sd, strict=False)
                print(f"   âœ… æˆåŠŸåŠ è½½ä¸Šä¸€è½®æƒé‡: {prev_ckpt}")
            except Exception as e:
                print(f"   âš ï¸ åŠ è½½ä¸Šä¸€è½®æƒé‡å¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        else:
            print(f"   âš ï¸ æ— ä¸Šä¸€è½® checkpointï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

    # =====================================================================
    # 4. ä¼˜åŒ–å™¨ + è°ƒåº¦å™¨ï¼ˆæŒ‰ Round è‡ªåŠ¨é€‰æ‹©è¶…å‚ï¼‰
    # =====================================================================
    epochs = ROUND1_EPOCHS if round_idx <= 1 else ROUND2_EPOCHS
    lr     = ROUND1_LR     if round_idx <= 1 else ROUND2_LR

    print(f"\n   ğŸ“ è®­ç»ƒè¶…å‚: epochs={epochs}, lr={lr}")

    optimizer  = optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay)
    scheduler  = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    # =====================================================================
    # 5. è®­ç»ƒå¾ªç¯
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("=" * 60)

    model.train()

    training_history = {
        'total_loss': [], 'avg_strength': [], 'high_conf_ratio': [],
        'contrastive_loss': [], 'reconstruction_loss': [], 'kl_loss': [],
        'u_epi_mean': [], 'u_ale_mean': [], 'queue_count': []   # æ–°å¢ç›‘æ§
    }

    for epoch in range(epochs):
        start_time = time.time()

        # åŠ¨æ€é‡‡æ ·ï¼ˆRound 1 å…¨é‡ï¼ŒRound 2+ ä¸‰åŒºåˆ¶ï¼‰
        batch_indices_list = create_dynamic_sampler(
            dataset,
            batch_size=args.batch_size,
            max_clusters_per_batch=args.max_clusters_per_batch,
            state_path=prev_state,
            round_idx=round_idx
        )

        batch_sampler = ListBatchSampler(batch_indices_list)
        train_loader  = torch.utils.data.DataLoader(
            dataset,
            batch_sampler=batch_sampler,
            num_workers=4,
            pin_memory=True
        )

        # epoch ç´¯ç§¯å˜é‡
        epoch_loss     = 0
        epoch_con      = 0
        epoch_rec      = 0
        epoch_kl       = 0
        epoch_str      = 0
        epoch_hc       = 0
        epoch_u_epi    = 0
        epoch_u_ale    = 0
        epoch_qc       = 0
        num_batches    = 0
        total_batches  = len(batch_indices_list)

        print(f"\nğŸ”„ Epoch {epoch + 1}/{epochs} å¼€å§‹... (å…± {total_batches} Batches)")

        for i, batch_data in enumerate(train_loader):
            reads_batch  = batch_data['encoding'].to(device)
            labels_batch = batch_data['clover_label'].to(device)

            # å‰å‘
            loss_dict, outputs = model(reads_batch, labels_batch, epoch=epoch)

            # åå‘
            optimizer.zero_grad()
            loss_dict['total'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # ç´¯ç§¯
            epoch_loss  += loss_dict['total'].item()
            epoch_con   += loss_dict['contrastive'].item()
            epoch_rec   += loss_dict['reconstruction'].item()
            epoch_kl    += loss_dict['kl_divergence'].item()
            epoch_str   += outputs['avg_strength']
            epoch_hc    += outputs['high_conf_ratio']
            epoch_u_epi += outputs.get('u_epi_mean', 0.0)
            epoch_u_ale += outputs.get('u_ale_mean', 0.0)
            epoch_qc    += outputs.get('queue_count', 0)
            num_batches += 1

            if (i + 1) % 10 == 0:
                print(f"   [Batch {i+1}/{total_batches}] "
                      f"Loss: {loss_dict['total'].item():.4f} | "
                      f"Str: {outputs['avg_strength']:.1f} | "
                      f"U_epi: {outputs.get('u_epi_mean',0):.4f}",
                      end='\r')

        # Epoch ç»“æŸ
        scheduler.step()
        epoch_time = time.time() - start_time

        # å¹³å‡å€¼
        avg = lambda x: x / max(num_batches, 1)

        training_history['total_loss'].append(avg(epoch_loss))
        training_history['contrastive_loss'].append(avg(epoch_con))
        training_history['reconstruction_loss'].append(avg(epoch_rec))
        training_history['kl_loss'].append(avg(epoch_kl))
        training_history['avg_strength'].append(avg(epoch_str))
        training_history['high_conf_ratio'].append(avg(epoch_hc))
        training_history['u_epi_mean'].append(avg(epoch_u_epi))
        training_history['u_ale_mean'].append(avg(epoch_u_ale))
        training_history['queue_count'].append(avg(epoch_qc))

        print(f"\n   âœ… Epoch {epoch+1} å®Œæˆ ({epoch_time:.1f}s) | "
              f"Loss: {avg(epoch_loss):.4f} | "
              f"Str: {avg(epoch_str):.1f} | "
              f"U_epi: {avg(epoch_u_epi):.4f} | "
              f"Queue: {avg(epoch_qc):.0f}")

    # =====================================================================
    # 6. ä¿å­˜
    # =====================================================================
    os.makedirs(os.path.join(args.output_dir, "models"), exist_ok=True)
    final_path = os.path.join(args.output_dir, "models", "step1_final_model.pth")

    torch.save({
        'model_state_dict': model.state_dict(),
        'args': vars(args)
    }, final_path)

    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_path}")

    # å¯è§†åŒ–
    try:
        visualizer = Step1Visualizer(args.output_dir)
        visualizer.generate_all_outputs(training_history, model, args)
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆè·³è¿‡: {e}")

    return final_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step 1 Training (Hot Start)')
    parser.add_argument('--experiment_dir',         type=str,   required=True)
    parser.add_argument('--feddna_checkpoint',      type=str,   default=None)
    parser.add_argument('--prev_checkpoint',        type=str,   default=None,
                        help='ä¸Šä¸€è½®çš„ step1_final_model.pthï¼ŒRound 2+ ä½¿ç”¨')
    parser.add_argument('--refined_labels',         type=str,   default=None)
    parser.add_argument('--prev_state',             type=str,   default=None,
                        help='ä¸Šä¸€è½®çš„ read_state.ptï¼Œä¾›åŠ¨æ€é‡‡æ ·å™¨è¯»å–')
    parser.add_argument('--round_idx',              type=int,   default=1)
    parser.add_argument('--dim',                    type=int,   default=256)
    parser.add_argument('--max_length',             type=int,   default=150)
    parser.add_argument('--min_clusters',           type=int,   default=50)
    parser.add_argument('--device',                 type=str,   default='cuda')
    parser.add_argument('--batch_size',             type=int,   default=32)
    parser.add_argument('--max_clusters_per_batch', type=int,   default=5)
    parser.add_argument('--weight_decay',           type=float, default=1e-5)
    parser.add_argument('--output_dir',             type=str,   default='./step1_results')

    args = parser.parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    train_step1(args)
