# models/step1_train.py
import torch
import torch.optim as optim
import argparse
import os
import sys
from datetime import datetime

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from models.step1_model import Step1EvidentialModel, load_pretrained_feddna
from models.step1_data import CloverDataLoader, Step1Dataset, create_cluster_balanced_sampler, seq_to_onehot
from models.step1_visualizer import Step1Visualizer

def evaluate_with_gt(outputs, data_loader, batch_gt_labels, device):
    """
    âœ… GTåªç”¨äºè¯„ä¼°ï¼Œä¸å‚ä¸è®­ç»ƒ
    è®¡ç®—ARIã€NMIç­‰æŒ‡æ ‡
    """
    try:
        from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
        return {
            'gt_available': True,
            'note': 'GT evaluation metrics can be added here'
        }
    except ImportError:
        return {'gt_available': False}

def train_step1(args):
    """æ­¥éª¤ä¸€è®­ç»ƒä¸»å‡½æ•°ï¼ˆä¸¥æ ¼è‡ªç›‘ç£ç‰ˆæœ¬ï¼‰"""
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1ï¸âƒ£ åŠ è½½æ•°æ®
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ•°æ®åŠ è½½")
    print("=" * 60)
    
    data_loader = CloverDataLoader(args.experiment_dir)
    dataset = Step1Dataset(data_loader, max_len=args.max_length)
    
    # 2ï¸âƒ£ åˆ›å»ºæ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ§  æ¨¡å‹åˆ›å»º")
    print("=" * 60)
    
    num_clover_clusters = len(set(data_loader.clover_labels))
    num_gt_clusters = len(data_loader.gt_cluster_seqs)
    
    model = Step1EvidentialModel(
        dim=args.dim,
        max_length=args.max_length,
        num_clusters=max(num_clover_clusters, num_gt_clusters, args.min_clusters),
        device=device
    ).to(device)
    
    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   Cloverç°‡æ•°: {num_clover_clusters}")
    print(f"   GTç°‡æ•°: {num_gt_clusters}")
    
    # 3ï¸âƒ£ åŠ è½½FedDNAé¢„è®­ç»ƒæƒé‡
    if args.feddna_checkpoint and os.path.exists(args.feddna_checkpoint):
        model = load_pretrained_feddna(model, args.feddna_checkpoint, device)
    else:
        print(f"âš ï¸ FedDNAæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {args.feddna_checkpoint}")
        print("   ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    
    # âœ… 4ï¸âƒ£ GTåªç”¨äºè¯„ä¼°ï¼Œä¸å‚ä¸è®­ç»ƒ
    print(f"\nğŸ“Š GTæ•°æ®çŠ¶æ€:")
    print(f"   - GTç°‡æ•°: {len(data_loader.gt_cluster_seqs)}")
    print(f"   - GTç”¨é€”: ä»…ç”¨äºè¯„ä¼°å’Œç›‘æ§")
    print(f"   - â— GTä¸å‚ä¸ä»»ä½•è®­ç»ƒloss")
    
    # 5ï¸âƒ£ ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # 6ï¸âƒ£ è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆä¸¥æ ¼è‡ªç›‘ç£æ¨¡å¼ï¼‰")
    print("=" * 60)
    
    model.train()
    training_history = {
        'total_loss': [],
        'contrastive_loss': [],
        'reconstruction_loss': [],
        'kl_loss': [],
        'strength_incentive_loss': [],
        'avg_strength': [],
        'high_conf_ratio': [],
        'annealing_coef': [] # âœ… æ–°å¢ï¼šè®°å½•é€€ç«ç³»æ•°
    }
    
    for epoch in range(args.epochs):
        # åˆ›å»ºcluster-balanced batch
        batch_indices_list = create_cluster_balanced_sampler(
            dataset, 
            batch_size=args.batch_size,
            max_clusters_per_batch=args.max_clusters_per_batch
        )
        
        epoch_losses = {
            'total': 0, 'contrastive': 0, 
            'reconstruction': 0, 'kl_divergence': 0
        }
        epoch_stats = {
            'avg_strength': 0,
            'high_conf_ratio': 0
        }
        
        current_annealing_coef = 0.0 # ç”¨äºè®°å½•å½“å‰epochå®é™…ä½¿ç”¨çš„ç³»æ•°
        num_batches = 0
        successful_batches = 0
        
        print(f"\nğŸ“¦ ç”Ÿæˆ {len(batch_indices_list)} ä¸ªbatch")
        
        for batch_idx, indices in enumerate(batch_indices_list):
            if len(indices) < 2:
                continue
            
            # æ„å»ºbatchæ•°æ®
            batch_reads = []
            batch_clover_labels = []
            batch_gt_labels = []
            
            for idx in indices:
                item = dataset[idx]
                batch_reads.append(item['encoding'])
                batch_clover_labels.append(item['clover_label'])
                batch_gt_labels.append(item['gt_label'])
            
            # è½¬æ¢ä¸ºå¼ é‡
            reads_batch = torch.stack(batch_reads).to(device)
            clover_labels_batch = torch.tensor(batch_clover_labels, device=device)
            
            # âœ… å‰å‘ä¼ æ’­
            try:
                loss_dict, outputs = model(
                    reads_batch, 
                    clover_labels_batch,
                    epoch=epoch
                )
                
                # âœ… è·å–çœŸå®çš„ Annealing Coef
                current_annealing_coef = loss_dict.get('annealing_coef', 0.0)
                
                # âœ… æ£€æŸ¥lossæœ‰æ•ˆæ€§
                if torch.isnan(loss_dict['total']) or torch.isinf(loss_dict['total']):
                    print(f"   âš ï¸ Batch {batch_idx}: æ£€æµ‹åˆ°å¼‚å¸¸lossï¼Œè·³è¿‡")
                    continue
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss_dict['total'].backward()
                
                # âœ… æ¢¯åº¦è£å‰ª
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    print(f"   âš ï¸ Batch {batch_idx}: æ¢¯åº¦å¼‚å¸¸ï¼Œè·³è¿‡æ›´æ–°")
                    continue
                
                optimizer.step()
                successful_batches += 1
                
            except Exception as e:
                print(f"   âŒ Batch {batch_idx}: è®­ç»ƒå¼‚å¸¸ {e}ï¼Œè·³è¿‡")
                continue
            
            # ç´¯è®¡æŸå¤±å’Œç»Ÿè®¡
            for key in epoch_losses:
                if key in loss_dict:
                    epoch_losses[key] += loss_dict[key].item()
            
            epoch_stats['avg_strength'] += outputs['avg_strength']
            epoch_stats['high_conf_ratio'] += outputs['high_conf_ratio']
            num_batches += 1
            
            # æ‰“å°batchè¿›åº¦
            if batch_idx % 50 == 0:
                print(f"   Batch {batch_idx}/{len(batch_indices_list)}: "
                      f"Loss={loss_dict['total'].item():.4f}, "
                      f"Strength={outputs['avg_strength']:.3f}, "
                      f"KL_Coef={current_annealing_coef:.3f}") # å®æ—¶æ‰“å°ç³»æ•°
        
        # âœ… Scheduler Step
        if successful_batches > 0:
            scheduler.step()
        
        # âœ… è®°å½•å†å²
        if num_batches > 0:
            avg_losses = {k: v/num_batches for k, v in epoch_losses.items()}
            avg_stats = {k: v/num_batches for k, v in epoch_stats.items()}
            
            training_history['total_loss'].append(avg_losses.get('total', 0.0))
            training_history['contrastive_loss'].append(avg_losses.get('contrastive', 0.0))
            training_history['reconstruction_loss'].append(avg_losses.get('reconstruction', 0.0))
            training_history['kl_loss'].append(avg_losses.get('kl_divergence', 0.0))
            training_history['avg_strength'].append(avg_stats.get('avg_strength', 0.0))
            training_history['high_conf_ratio'].append(avg_stats.get('high_conf_ratio', 0.0))
            training_history['annealing_coef'].append(current_annealing_coef) # è®°å½•ç³»æ•°
            
            # âœ… è¯¦ç»†çš„epochæŠ¥å‘Š
            print(f"\nğŸ“Š Epoch {epoch+1}/{args.epochs}:")
            print(f"   ğŸ“‰ æŸå¤±:")
            print(f"      Total: {avg_losses['total']:.4f}")
            print(f"      Contrastive: {avg_losses['contrastive']:.4f}")
            print(f"      Reconstruction: {avg_losses['reconstruction']:.4f}")
            print(f"      KL Divergence: {avg_losses['kl_divergence']:.4f} (Raw)")
            print(f"   ğŸ“Š Evidenceç»Ÿè®¡:")
            print(f"      å¹³å‡Strength: {avg_stats['avg_strength']:.3f}")
            print(f"      é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {avg_stats['high_conf_ratio']*100:.1f}%")
            print(f"   âš™ï¸ è®­ç»ƒçŠ¶æ€:")
            print(f"      Learning Rate: {scheduler.get_last_lr()[0]:.6f}")
            print(f"      Annealing Coef: {current_annealing_coef:.3f}") # âœ… ä½¿ç”¨çœŸå®ç³»æ•°
            print(f"      æˆåŠŸBatch: {successful_batches}/{len(batch_indices_list)}")
            
            # âœ… æ™ºèƒ½çŠ¶æ€æç¤º
            if epoch < 5:
                print(f"   ğŸ”¥ [Phase 1] å¼ºåˆ¶å­¦ä¹ : å¯¹æ¯”å­¦ä¹  Warm-up (æ— ç­›é€‰), KL ç³»æ•°=0")
            elif epoch < 10:
                print(f"   ğŸ’ª [Phase 2] ç§¯ç´¯ä¿¡å¿ƒ: å¯¹æ¯”å­¦ä¹  (å¼€å¯ç­›é€‰), KL ç³»æ•°=0")
            else:
                print(f"   âœ‚ï¸ [Phase 3] è¯æ®ä¿®å‰ª: KL æ­£åˆ™åŒ–ä»‹å…¥ (ç³»æ•°={current_annealing_coef:.3f})")
                
        else:
            print(f"\nâš ï¸ Epoch {epoch+1}: æ²¡æœ‰æˆåŠŸçš„batchï¼Œè·³è¿‡")
            for k in training_history:
                training_history[k].append(0.0)
        
        # ä¿å­˜checkpoint
        if (epoch + 1) % args.save_interval == 0:
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'training_history': training_history,
                'args': vars(args)
            }
            checkpoint_path = os.path.join(args.output_dir, "models", f"step1_epoch_{epoch+1}.pth")
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            torch.save(checkpoint, checkpoint_path)
            print(f"   ğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path}")
    
    # âœ… è®­ç»ƒå®Œæˆåæ‰“å°å†å²è®°å½•ç»Ÿè®¡
    print(f"\nğŸ“Š è®­ç»ƒå†å²è®°å½•ç»Ÿè®¡:")
    for key, values in training_history.items():
        if len(values) > 0:
            print(f"   {key}: {len(values)} æ¡è®°å½•, æœ€ç»ˆå€¼: {values[-1]:.6f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(args.output_dir, "models", "step1_final_model.pth")
    os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
    torch.save({
        'model_state_dict': model.state_dict(),
        'training_history': training_history,
        'args': vars(args)
    }, final_model_path)
    
    # ç”Ÿæˆå¯è§†åŒ–
    print(f"\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆè®­ç»ƒç»“æœä¸å¯è§†åŒ–")
    print("=" * 60)
    
    try:
        visualizer = Step1Visualizer(args.output_dir)
        visualizer.generate_all_outputs(training_history, model, args)
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ (å¯èƒ½ç¼ºå°‘ä¾èµ–): {e}")
    
    print(f"\nğŸ‰ æ­¥éª¤ä¸€è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹: {final_model_path}")
    
    return model, training_history

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æ­¥éª¤ä¸€ï¼šEvidence-drivenè®­ç»ƒï¼ˆä¸¥æ ¼è‡ªç›‘ç£ï¼‰')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--experiment_dir', type=str, required=True,
                       help='å®éªŒç›®å½•è·¯å¾„')
    parser.add_argument('--feddna_checkpoint', type=str, 
                       default='result/FLDNA_I/I_1214234233/model/epoch1_I.pth',
                       help='FedDNAé¢„è®­ç»ƒæƒé‡è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--dim', type=int, default=256, help='ç‰¹å¾ç»´åº¦')
    parser.add_argument('--max_length', type=int, default=150, help='åºåˆ—æœ€å¤§é•¿åº¦')
    parser.add_argument('--min_clusters', type=int, default=50, help='æœ€å°ç°‡æ•°é‡')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_clusters_per_batch', type=int, default=5, help='æ¯ä¸ªbatchæœ€å¤§ç°‡æ•°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='æƒé‡è¡°å‡')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, 
                       default=f'./step1_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--save_interval', type=int, default=10, help='ä¿å­˜é—´éš”')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    model, history = train_step1(args)