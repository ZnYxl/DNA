# models/step1_train.py
import torch
import torch.optim as optim
import argparse
import os
import sys
from datetime import datetime

# æ·»åŠ è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥ models åŒ…
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.step1_model import Step1EvidentialModel, load_pretrained_feddna
from models.step1_data import CloverDataLoader, Step1Dataset, create_cluster_balanced_sampler
from models.step1_visualizer import Step1Visualizer


def train_step1(args):
    """æ­¥éª¤ä¸€è®­ç»ƒä¸»å‡½æ•°ï¼ˆæ”¯æŒè¿­ä»£è®­ç»ƒï¼‰"""
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # 1ï¸âƒ£ åŠ è½½æ•°æ®
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ•°æ®åŠ è½½")
    print("=" * 60)

    # âœ… ä¿®å¤ï¼šå®‰å…¨è·å– refined_labels å‚æ•°
    labels_path = getattr(args, 'refined_labels', None)
    
    data_loader = CloverDataLoader(args.experiment_dir, labels_path=labels_path)
    dataset = Step1Dataset(data_loader, max_len=args.max_length)

    # 2ï¸âƒ£ åˆ›å»ºæ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ§  æ¨¡å‹åˆ›å»º")
    print("=" * 60)

    num_clover_clusters = len(set(data_loader.clover_labels))
    num_gt_clusters = len(data_loader.gt_cluster_seqs)
    
    # ç¡®ä¿ç°‡æ•°é‡è¶³å¤Ÿ
    num_clusters = max(num_clover_clusters, num_gt_clusters, args.min_clusters)

    model = Step1EvidentialModel(
        dim=args.dim,
        max_length=args.max_length,
        num_clusters=num_clusters,
        device=device
    ).to(device)

    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   å½“å‰ç°‡æ•°: {num_clover_clusters}")

    # 3ï¸âƒ£ åŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.feddna_checkpoint and os.path.exists(args.feddna_checkpoint):
        model = load_pretrained_feddna(model, args.feddna_checkpoint, device)
    else:
        print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨æˆ–æœªæŒ‡å®š: {getattr(args, 'feddna_checkpoint', 'None')}")
        print("   ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")

    # 4ï¸âƒ£ ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    # 5ï¸âƒ£ è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("=" * 60)

    model.train()
    training_history = {'total_loss': [], 'avg_strength': [], 'high_conf_ratio': []}

    for epoch in range(args.epochs):
        batch_indices_list = create_cluster_balanced_sampler(
            dataset,
            batch_size=args.batch_size,
            max_clusters_per_batch=args.max_clusters_per_batch
        )

        epoch_loss = 0
        epoch_strength = 0
        epoch_high_conf = 0
        num_batches = 0

        for indices in batch_indices_list:
            if len(indices) < 2: continue

            # æ„å»ºbatch
            batch_reads = [dataset[idx]['encoding'] for idx in indices]
            batch_labels = [dataset[idx]['clover_label'] for idx in indices]

            reads_batch = torch.stack(batch_reads).to(device)
            labels_batch = torch.tensor(batch_labels, device=device)

            # å‰å‘ä¼ æ’­ (ä¼ å…¥ reads ç”¨äºé‡å»ºè¾“å…¥)
            loss_dict, outputs = model(reads_batch, labels_batch, epoch=epoch)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss_dict['total'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # è®°å½•
            epoch_loss += loss_dict['total'].item()
            epoch_strength += outputs['avg_strength']
            epoch_high_conf += outputs['high_conf_ratio']
            num_batches += 1

        if num_batches > 0:
            scheduler.step()
            avg_loss = epoch_loss / num_batches
            avg_strength = epoch_strength / num_batches
            avg_high_conf = epoch_high_conf / num_batches

            training_history['total_loss'].append(avg_loss)
            training_history['avg_strength'].append(avg_strength)
            training_history['high_conf_ratio'].append(avg_high_conf)

            print(f"   Epoch {epoch + 1}/{args.epochs} | Loss: {avg_loss:.4f} | Strength: {avg_strength:.2f} | HighConf: {avg_high_conf:.1%}")

    # 6ï¸âƒ£ ä¿å­˜æœ€ç»ˆæ¨¡å‹
    os.makedirs(os.path.join(args.output_dir, "models"), exist_ok=True)
    final_model_path = os.path.join(args.output_dir, "models", "step1_final_model.pth")
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'args': vars(args)
    }, final_model_path)

    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    try:
        visualizer = Step1Visualizer(args.output_dir)
        visualizer.generate_all_outputs(training_history, model, args)
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆè·³è¿‡: {e}")

    # âœ… è¿”å›æ¨¡å‹è·¯å¾„ï¼Œä¾›å¤–éƒ¨è°ƒç”¨
    return final_model_path


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step 1 Training')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--experiment_dir', type=str, required=True, help='å®éªŒç›®å½•')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--feddna_checkpoint', type=str, default=None, help='é¢„è®­ç»ƒæƒé‡')
    # âœ… ä¿®å¤ï¼šæ·»åŠ  refined_labels å‚æ•°å®šä¹‰
    parser.add_argument('--refined_labels', type=str, default=None, help='è¿­ä»£ä¿®æ­£åçš„æ ‡ç­¾æ–‡ä»¶')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--dim', type=int, default=256)
    parser.add_argument('--max_length', type=int, default=150)
    parser.add_argument('--min_clusters', type=int, default=50)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--max_clusters_per_batch', type=int, default=5)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=1e-5)
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='./step1_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--save_interval', type=int, default=10)

    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    train_step1(args)