import torch
import torch.optim as optim
import argparse
import os
import sys
import time  # æ–°å¢
from datetime import datetime
import numpy as np

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.step1_model import Step1EvidentialModel, load_pretrained_feddna
from models.step1_data import CloverDataLoader, Step1Dataset, create_cluster_balanced_sampler
from models.step1_visualizer import Step1Visualizer

# å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰ Sampler é€‚é… DataLoader
class ListBatchSampler:
    def __init__(self, batches):
        self.batches = batches
    def __iter__(self):
        return iter(self.batches)
    def __len__(self):
        return len(self.batches)

def train_step1(args):
    """æ­¥éª¤ä¸€è®­ç»ƒä¸»å‡½æ•°ï¼ˆé«˜æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # 1ï¸âƒ£ åŠ è½½æ•°æ®
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ•°æ®åŠ è½½")
    print("=" * 60)
    
    labels_path = getattr(args, 'refined_labels', None)
    data_loader = CloverDataLoader(args.experiment_dir, labels_path=labels_path)
    dataset = Step1Dataset(data_loader, max_len=args.max_length)

    # 2ï¸âƒ£ åˆ›å»ºæ¨¡å‹
    print("\n" + "=" * 60)
    print("ğŸ§  æ¨¡å‹åˆ›å»º")
    print("=" * 60)

    num_clover_clusters = len(set(data_loader.clover_labels))
    num_gt_clusters = len(data_loader.gt_cluster_seqs)
    num_clusters = max(num_clover_clusters, num_gt_clusters, args.min_clusters)

    model = Step1EvidentialModel(
        dim=args.dim,
        max_length=args.max_length,
        num_clusters=num_clusters,
        device=device
    ).to(device)

    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   å½“å‰ç°‡æ•°: {num_clover_clusters}")

    # 3ï¸âƒ£ åŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.feddna_checkpoint and os.path.exists(args.feddna_checkpoint):
        model = load_pretrained_feddna(model, args.feddna_checkpoint, device)
    else:
        print(f"   ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")

    # 4ï¸âƒ£ ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)

    # 5ï¸âƒ£ è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ (å·²å¼€å¯å¤šè¿›ç¨‹åŠ é€Ÿä¸å®æ—¶æ—¥å¿—)")
    print("=" * 60)

    model.train()

    training_history = {
        'total_loss': [], 'avg_strength': [], 'high_conf_ratio': [],
        'contrastive_loss': [], 'reconstruction_loss': [], 'kl_loss': []
    }

    for epoch in range(args.epochs):
        start_time = time.time()
        
        # 1. ç”Ÿæˆ Batch ç´¢å¼• (ä½ ä¹‹å‰çš„ä¼˜åŒ–æˆæœï¼Œéå¸¸å¿«)
        batch_indices_list = create_cluster_balanced_sampler(
            dataset,
            batch_size=args.batch_size,
            max_clusters_per_batch=args.max_clusters_per_batch
        )
        
        # 2. ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ DataLoader è¿›è¡Œå¤šè¿›ç¨‹åŠ è½½
        # è¿™å°†è§£å†³ "CPUå•æ ¸å¤„ç†100ä¸‡æ¡æ•°æ®å¤ªæ…¢" çš„é—®é¢˜
        # num_workers=8 è¡¨ç¤ºå¼€å¯8ä¸ªè¿›ç¨‹å¹¶è¡Œè¯»å–æ•°æ®
        batch_sampler = ListBatchSampler(batch_indices_list)
        train_loader = torch.utils.data.DataLoader(
            dataset, 
            batch_sampler=batch_sampler, 
            num_workers=8,  # ä½ çš„æœåŠ¡å™¨æœ‰64æ ¸ï¼Œå¼€16ä¸ªéå¸¸ç¨³
            pin_memory=True
        )

        epoch_loss = 0
        epoch_con_loss = 0
        epoch_rec_loss = 0
        epoch_kl_loss = 0
        epoch_strength = 0
        epoch_high_conf = 0
        num_batches = 0
        
        total_batches = len(batch_indices_list)
        print(f"\nğŸ”„ Epoch {epoch + 1}/{args.epochs} å¼€å§‹... (å…± {total_batches} Batches)")

        # 3. è®­ç»ƒå¾ªç¯ (å¸¦è¿›åº¦æ‰“å°)
        for i, batch_data in enumerate(train_loader):
            # è·å–æ•°æ® (DataLoader è‡ªåŠ¨å¸®æˆ‘ä»¬æ•´ç†å¥½äº†)
            reads_batch = batch_data['encoding'].to(device)
            labels_batch = batch_data['clover_label'].to(device)

            # å‰å‘ä¼ æ’­
            loss_dict, outputs = model(reads_batch, labels_batch, epoch=epoch)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss_dict['total'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # è®°å½•æ•°æ®
            loss_val = loss_dict['total'].item()
            epoch_loss += loss_val
            epoch_con_loss += loss_dict['contrastive'].item()
            epoch_rec_loss += loss_dict['reconstruction'].item()
            epoch_kl_loss += loss_dict['kl_divergence'].item()
            epoch_strength += outputs['avg_strength']
            epoch_high_conf += outputs['high_conf_ratio']
            num_batches += 1
            
            # âœ… å®æ—¶æ—¥å¿—ï¼šæ¯ 10 ä¸ª Batch æ‰“å°ä¸€æ¬¡ï¼Œè®©ä½ çŸ¥é“å®ƒåœ¨åŠ¨ï¼
            if (i + 1) % 10 == 0:
                print(f"   [Batch {i+1}/{total_batches}] Loss: {loss_val:.4f} | "
                      f"Str: {outputs['avg_strength']:.1f}", end='\r')

        # Epoch ç»“æŸå¤„ç†
        scheduler.step()
        epoch_time = time.time() - start_time
        
        # è®¡ç®—å¹³å‡å€¼
        avg_loss = epoch_loss / num_batches
        avg_con = epoch_con_loss / num_batches
        avg_rec = epoch_rec_loss / num_batches
        avg_kl = epoch_kl_loss / num_batches
        avg_strength = epoch_strength / num_batches
        avg_high_conf = epoch_high_conf / num_batches

        # å­˜å…¥å†å²
        training_history['total_loss'].append(avg_loss)
        training_history['contrastive_loss'].append(avg_con)
        training_history['reconstruction_loss'].append(avg_rec)
        training_history['kl_loss'].append(avg_kl)
        training_history['avg_strength'].append(avg_strength)
        training_history['high_conf_ratio'].append(avg_high_conf)

        # æ‰“å° Epoch æ€»ç»“ (æ¢è¡Œ)
        print(f"\n   âœ… Epoch {epoch + 1} å®Œæˆ ({epoch_time:.1f}s) | "
              f"Avg Loss: {avg_loss:.4f} | Avg Str: {avg_strength:.1f}")

    # 6ï¸âƒ£ ä¿å­˜
    os.makedirs(os.path.join(args.output_dir, "models"), exist_ok=True)
    final_model_path = os.path.join(args.output_dir, "models", "step1_final_model.pth")
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'args': vars(args)
    }, final_model_path)
    
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {final_model_path}")

    # å¯è§†åŒ–
    try:
        visualizer = Step1Visualizer(args.output_dir)
        visualizer.generate_all_outputs(training_history, model, args)
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆè·³è¿‡: {e}")

    return final_model_path

if __name__ == "__main__":
    # å‚æ•°è§£æéƒ¨åˆ†ä¿æŒä¸å˜ï¼Œå¤åˆ¶ä½ åŸæ¥çš„å³å¯
    parser = argparse.ArgumentParser(description='Step 1 Training')
    parser.add_argument('--experiment_dir', type=str, required=True)
    parser.add_argument('--feddna_checkpoint', type=str, default=None)
    parser.add_argument('--refined_labels', type=str, default=None)
    parser.add_argument('--dim', type=int, default=256)
    parser.add_argument('--max_length', type=int, default=150)
    parser.add_argument('--min_clusters', type=int, default=50)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--max_clusters_per_batch', type=int, default=5)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--weight_decay', type=float, default=1e-5)
    parser.add_argument('--output_dir', type=str, default='./step1_results')
    parser.add_argument('--save_interval', type=int, default=10)

    args = parser.parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    train_step1(args)