# models/step1_model.py - å®Œæ•´ä¿®å¤ç‰ˆ
import torch
import torch.nn as nn
import torch.nn.functional as F
from models.Model import Encoder, RNNBlock
from utils.Loss import CEBayesRiskLoss, KLDivergenceLoss
import numpy as np

class Step1EvidentialModel(nn.Module):
    """
    æ­¥éª¤ä¸€ï¼šEvidence-drivenè®­ç»ƒæ¨¡å‹ï¼ˆä¸¥æ ¼è‡ªç›‘ç£ç‰ˆæœ¬ï¼‰
    â— GTåªç”¨äºè¯„ä¼°ï¼Œä¸å‚ä¸è®­ç»ƒloss
    """
    def __init__(self, 
                 dim=256, 
                 max_length=150,
                 num_clusters=50,
                 device='cuda'):
        super().__init__()
        
        # ===== FedDNA æ ¸å¿ƒç»„ä»¶ =====
        self.encoder = Encoder(dim=dim)
        
        # âœ… ä¿®å¤ï¼šåŠ¨æ€é€‚é…length_adapter
        self.length_adapter = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        self.rnnblock = RNNBlock(in_channels=dim, lstm_hidden_dim=256, rnn_dropout_p=0.1)
        
        # ===== å¯¹æ¯”å­¦ä¹ ç»„ä»¶ =====
        self.projection_head = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(dim, 128)
        )
        
        self.dim = dim
        self.max_length = max_length
        self.num_clusters = num_clusters
        self.device = device
    
    def _init_length_adapter_if_needed(self, seq_len):
        """å»¶è¿Ÿåˆå§‹åŒ–length_adapterï¼Œé¿å…å½¢çŠ¶ä¸åŒ¹é…"""
        if self.length_adapter is None:
            self.length_adapter = nn.Linear(seq_len, self.max_length).to(self.device)
            print(f"   ğŸ”§ åŠ¨æ€åˆå§‹åŒ–length_adapter: {seq_len} -> {self.max_length}")
    
    def encode_reads(self, reads):
        """
        âœ… ä½¿ç”¨FedDNA Encoderç¼–ç reads
        Args:
            reads: (B, L, 4) æ‰¹æ¬¡ä¸­çš„reads
        Returns:
            embeddings: (B, L, dim) ç¼–ç åçš„ç‰¹å¾
            pooled_emb: (B, dim) æ± åŒ–åçš„å…¨å±€ç‰¹å¾
        """
        B, L, D = reads.shape
        
        # FedDNA Encoder: Conv2d + ConMamba
        embeddings = self.encoder(reads)  # (B, L, dim)
        
        # å…¨å±€æ± åŒ–ç”¨äºå¯¹æ¯”å­¦ä¹ 
        pooled_emb = embeddings.mean(dim=1)  # (B, dim)
        
        return embeddings, pooled_emb
    
    def decode_to_evidence(self, embeddings):
        """
        âœ… ä½¿ç”¨FedDNA Decoderç”Ÿæˆevidence
        Args:
            embeddings: (B, L, dim)
        Returns:
            evidence: (B, L, 4) æ¯ä¸ªä½ç½®çš„ACGT evidence
            strength: (B, L) è¯æ®å¼ºåº¦
            alpha: (B, L, 4) Dirichletå‚æ•°
        """
        B, L, D = embeddings.shape
        
        # åŠ¨æ€åˆå§‹åŒ–length_adapter
        self._init_length_adapter_if_needed(L)
        
        # é•¿åº¦é€‚é…
        if L != self.max_length:
            adapted = embeddings.permute(0, 2, 1)  # (B, dim, L)
            adapted = self.length_adapter(adapted)  # (B, dim, max_length)
            adapted = adapted.permute(0, 2, 1)     # (B, max_length, dim)
        else:
            adapted = embeddings
        
        # FedDNA RNN Decoder
        evidence = self.rnnblock(adapted)  # (B, L, 4)
        
        # âœ… æ•°å€¼ç¨³å®šçš„evidenceå¤„ç†
        evidence = torch.clamp(evidence, min=1e-8, max=1e8)  # é˜²æ­¢æå€¼
        
        # æ­£ç¡®çš„evidence strengthè®¡ç®—
        K = evidence.size(-1)  # 4
        alpha = evidence + 1.0
        strength = torch.sum(alpha, dim=-1)  # (B, L)
        
        return evidence, strength, alpha
    
    def contrastive_learning_with_evidence_filter(self, pooled_emb, cluster_labels, strength, 
                                                 temperature=0.1, epoch=0, warmup_epochs=5):
        """âœ… ä¿®å¤ï¼šæ•°å€¼ç¨³å®šçš„å¯¹æ¯”å­¦ä¹ """
        if epoch < warmup_epochs:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        if pooled_emb.size(0) < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
        proj_emb = self.projection_head(pooled_emb)
        proj_emb = F.normalize(proj_emb, dim=-1)
        
        # âœ… ä¿®å¤ï¼šæ•°å€¼ç¨³å®šçš„confidenceè®¡ç®—
        confidence = strength.mean(dim=1)  # (B,)
        
        # æ£€æŸ¥NaN
        if torch.isnan(confidence).any():
            print(f"   âš ï¸ æ£€æµ‹åˆ°confidence NaNï¼Œè·³è¿‡å¯¹æ¯”å­¦ä¹ ")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # âœ… æ”¹è¿›ï¼šä½¿ç”¨åˆ†ä½æ•°è€Œä¸æ˜¯å‡å€¼ä½œä¸ºé˜ˆå€¼
        conf_threshold = torch.quantile(confidence, 0.6)  # æ›´ç¨³å®šçš„é˜ˆå€¼
        conf_mask = confidence > conf_threshold
        
        if conf_mask.sum() < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(proj_emb, proj_emb.T) / temperature
        
        # æ„å»ºæ­£æ ·æœ¬mask
        labels_expanded = cluster_labels.unsqueeze(1)
        clover_positive_mask = (labels_expanded == labels_expanded.T).float()
        evidence_positive_mask = (conf_mask.unsqueeze(1) & conf_mask.unsqueeze(0)).float()
        
        positive_mask = clover_positive_mask * evidence_positive_mask
        positive_mask.fill_diagonal_(0)
        
        # âœ… æ•°å€¼ç¨³å®šçš„InfoNCE
        exp_sim = torch.exp(torch.clamp(sim_matrix, max=10))  # é˜²æ­¢expçˆ†ç‚¸
        
        numerator = torch.sum(exp_sim * positive_mask, dim=1) + 1e-8
        denominator = torch.sum(exp_sim, dim=1) - torch.diag(exp_sim) + 1e-8
        
        loss = -torch.log(numerator / denominator)
        
        # åªè®¡ç®—æœ‰æ­£æ ·æœ¬çš„loss
        valid_mask = (torch.sum(positive_mask, dim=1) > 0)
        if valid_mask.sum() > 0:
            final_loss = loss[valid_mask].mean()
            # æ£€æŸ¥æœ€ç»ˆloss
            if torch.isnan(final_loss) or torch.isinf(final_loss):
                print(f"   âš ï¸ å¯¹æ¯”å­¦ä¹ losså¼‚å¸¸ï¼Œè¿”å›0")
                return torch.tensor(0.0, device=self.device, requires_grad=True)
            return final_loss
        else:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def self_reconstruction_loss(self, evidence, alpha, cluster_labels):
        """âœ… ä¿®å¤ï¼šæ•°å€¼ç¨³å®šçš„é‡å»ºæŸå¤±"""
        bayes_risk = CEBayesRiskLoss().to(self.device)
        kld_loss_fn = KLDivergenceLoss().to(self.device)
        
        total_recon_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        total_kl_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        fused_consensus = {}
        
        unique_labels = torch.unique(cluster_labels)
        processed_clusters = 0
        
        for label in unique_labels:
            if label < 0:
                continue
            
            cluster_mask = (cluster_labels == label)
            cluster_count = cluster_mask.sum()
            
            if cluster_count < 2:
                continue
            
            cluster_evidence = evidence[cluster_mask]
            cluster_alpha = alpha[cluster_mask]
            cluster_strength = torch.sum(cluster_alpha, dim=-1)
            
            # âœ… æ£€æŸ¥NaN
            if torch.isnan(cluster_strength).any():
                print(f"   âš ï¸ ç°‡{label}çš„strengthåŒ…å«NaNï¼Œè·³è¿‡")
                continue
            
            # Evidence-weightedèåˆ
            weights = F.softmax(cluster_strength.mean(dim=1), dim=0)
            weights = weights.unsqueeze(1).unsqueeze(2)
            
            fused_evidence = torch.sum(cluster_evidence * weights, dim=0, keepdim=True)
            fused_consensus[label.item()] = fused_evidence
            
            # è‡ªé‡å»ºæŸå¤±
            cluster_recon_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            cluster_kl_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            
            for i in range(cluster_count):
                read_evidence = cluster_evidence[i:i+1]
                
                try:
                    recon_loss_i = bayes_risk(fused_evidence, read_evidence)
                    kl_loss_i = kld_loss_fn(fused_evidence, read_evidence)
                    
                    # æ£€æŸ¥lossæœ‰æ•ˆæ€§
                    if not (torch.isnan(recon_loss_i) or torch.isinf(recon_loss_i)):
                        cluster_recon_loss = cluster_recon_loss + recon_loss_i
                    if not (torch.isnan(kl_loss_i) or torch.isinf(kl_loss_i)):
                        cluster_kl_loss = cluster_kl_loss + kl_loss_i
                        
                except Exception as e:
                    print(f"   âš ï¸ ç°‡{label}ç¬¬{i}ä¸ªreadè®¡ç®—losså¤±è´¥: {e}")
                    continue
            
            total_recon_loss = total_recon_loss + cluster_recon_loss
            total_kl_loss = total_kl_loss + cluster_kl_loss
            processed_clusters += 1
        
        # å½’ä¸€åŒ–
        if processed_clusters > 0:
            total_recon_loss = total_recon_loss / processed_clusters
            total_kl_loss = total_kl_loss / processed_clusters
        
        return total_recon_loss, total_kl_loss, fused_consensus
    
    def forward(self, reads, cluster_labels, epoch=0):
        """
        âœ… å®Œæ•´çš„å‰å‘ä¼ æ’­
        Args:
            reads: (B, L, 4) mini-batch reads
            cluster_labels: (B,) Cloveræ ‡ç­¾ï¼ˆä»…ç”¨äºç»„ç»‡å¯¹æ¯”å­¦ä¹ ï¼‰
            epoch: å½“å‰epoch
        """
        # 1ï¸âƒ£ FedDNA Encoder
        embeddings, pooled_emb = self.encode_reads(reads)
        
        # 2ï¸âƒ£ FedDNA Decoder
        evidence, strength, alpha = self.decode_to_evidence(embeddings)
        
        # 3ï¸âƒ£ Evidence-filteredå¯¹æ¯”å­¦ä¹ 
        contrastive_loss = self.contrastive_learning_with_evidence_filter(
            pooled_emb, cluster_labels, strength, epoch=epoch
        )
        
        # 4ï¸âƒ£ è‡ªé‡å»ºæŸå¤±ï¼ˆä¸ä½¿ç”¨GTï¼‰
        recon_loss, kl_loss, fused_consensus = self.self_reconstruction_loss(
            evidence, alpha, cluster_labels
        )
        
        # 5ï¸âƒ£ æ€»æŸå¤±ï¼ˆå¸¦annealingï¼‰
        annealing_coef = min(1.0, epoch / 10)
        total_loss = contrastive_loss + recon_loss + annealing_coef * kl_loss
        
        # 6ï¸âƒ£ ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¿½ï¿½
        avg_strength = strength.mean().item()
        high_conf_ratio = (strength.mean(dim=1) > strength.mean()).float().mean().item()
        
        loss_dict = {
            'total': total_loss,
            'contrastive': contrastive_loss,
            'reconstruction': recon_loss,
            'kl_divergence': kl_loss,
            'annealing_coef': annealing_coef
        }
        
        outputs = {
            'embeddings': embeddings,
            'evidence': evidence,
            'strength': strength,
            'alpha': alpha,
            'fused_consensus': fused_consensus,
            'avg_strength': avg_strength,
            'high_conf_ratio': high_conf_ratio
        }
        
        return loss_dict, outputs

def load_pretrained_feddna(model, checkpoint_path, device):
    """âœ… ä¿®å¤ï¼šæ›´æ™ºèƒ½çš„æƒé‡åŠ è½½"""
    print(f"ğŸ”„ åŠ è½½FedDNAé¢„è®­ç»ƒæƒé‡: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                pretrained_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                pretrained_dict = checkpoint['state_dict']
            else:
                pretrained_dict = checkpoint
        else:
            pretrained_dict = checkpoint
        
        model_dict = model.state_dict()
        
        # æ™ºèƒ½åŠ è½½æƒé‡
        filtered_dict = {}
        skipped_keys = []
        
        for k, v in pretrained_dict.items():
            if k in model_dict:
                if model_dict[k].shape == v.shape:
                    filtered_dict[k] = v
                    print(f"   âœ… åŠ è½½: {k} {v.shape}")
                else:
                    skipped_keys.append(f"{k} (å½¢çŠ¶ä¸åŒ¹é…: æ¨¡å‹{model_dict[k].shape} vs æƒé‡{v.shape})")
            else:
                # âœ… è·³è¿‡length_adapterç›¸å…³æƒé‡ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåŠ¨æ€åˆå§‹åŒ–
                if 'length_adapter' in k:
                    print(f"   ğŸ”§ è·³è¿‡length_adapteræƒé‡ï¼ˆå°†åŠ¨æ€åˆå§‹åŒ–ï¼‰: {k}")
                else:
                    skipped_keys.append(f"{k} (æ¨¡å‹ä¸­ä¸å­˜åœ¨)")
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        if filtered_dict:
            model_dict.update(filtered_dict)
            model.load_state_dict(model_dict, strict=False)  # å…è®¸éƒ¨åˆ†åŠ è½½
            print(f"âœ… æˆåŠŸåŠ è½½ {len(filtered_dict)}/{len(pretrained_dict)} ä¸ªå‚æ•°")
        
        if skipped_keys:
            print(f"âš ï¸ è·³è¿‡çš„æƒé‡:")
            for key in skipped_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     {key}")
            if len(skipped_keys) > 5:
                print(f"     ... è¿˜æœ‰ {len(skipped_keys) - 5} ä¸ª")
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print("   ç»§ç»­ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    
    return model
