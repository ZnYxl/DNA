# models/eval_metrics.py
"""
SSI-EC èšç±»è¯„ä¼°æŒ‡æ ‡ä½“ç³»

åŒ…å«:
  - ARI (Adjusted Rand Index)
  - NMI (Normalized Mutual Information)
  - Purity
  - Recovery Rate
  - Micro Accuracy
  - Recall@cluster
  - åˆ†å±‚åˆ†æ (æŒ‰ç°‡å¤§å°)
"""
import numpy as np
from collections import Counter, defaultdict


def compute_ari(pred_labels, gt_labels):
    """Adjusted Rand Index â€” æ ¡æ­£éšæœºä¸€è‡´æ€§, [-1, 1]"""
    try:
        from sklearn.metrics import adjusted_rand_score
        return adjusted_rand_score(gt_labels, pred_labels)
    except ImportError:
        print("   âš ï¸ sklearn æœªå®‰è£…, ARI è·³è¿‡")
        return None


def compute_nmi(pred_labels, gt_labels):
    """Normalized Mutual Information â€” ä¿¡æ¯è®ºè§†è§’, [0, 1]"""
    try:
        from sklearn.metrics import normalized_mutual_info_score
        return normalized_mutual_info_score(gt_labels, pred_labels)
    except ImportError:
        print("   âš ï¸ sklearn æœªå®‰è£…, NMI è·³è¿‡")
        return None


def compute_purity(pred_labels, gt_labels):
    """
    Purity: æ¯ä¸ªé¢„æµ‹ç°‡ä¸­æœ€å¤šçš„GTç±»åˆ«å æ¯”, readsæ•°åŠ æƒå¹³å‡
    Clover åŸå§‹æŒ‡æ ‡
    """
    cluster_gt = defaultdict(list)
    for p, g in zip(pred_labels, gt_labels):
        if p >= 0 and g >= 0:
            cluster_gt[p].append(g)

    if not cluster_gt:
        return 0.0

    total_correct = 0
    total_count = 0
    for cid, gt_list in cluster_gt.items():
        counter = Counter(gt_list)
        majority = counter.most_common(1)[0][1]
        total_correct += majority
        total_count += len(gt_list)

    return total_correct / max(total_count, 1)


def compute_recovery_rate(pred_labels, gt_labels):
    """
    Recovery Rate: è¢«è‡³å°‘ä¸€ä¸ªé¢„æµ‹ç°‡è¦†ç›–çš„GTç°‡æ¯”ä¾‹
    Clover åŸå§‹æŒ‡æ ‡
    """
    # æ‰¾å‡ºæ‰€æœ‰ GT ç°‡
    gt_clusters = set(g for g in gt_labels if g >= 0)
    if not gt_clusters:
        return 0.0

    # å¯¹æ¯ä¸ªé¢„æµ‹ç°‡, æ‰¾åˆ°å…¶ majority GT
    cluster_gt = defaultdict(list)
    for p, g in zip(pred_labels, gt_labels):
        if p >= 0 and g >= 0:
            cluster_gt[p].append(g)

    recovered_gt = set()
    for cid, gt_list in cluster_gt.items():
        counter = Counter(gt_list)
        majority_gt = counter.most_common(1)[0][0]
        recovered_gt.add(majority_gt)

    return len(recovered_gt) / len(gt_clusters)


def compute_micro_accuracy(pred_labels, gt_labels):
    """
    Micro Accuracy: è¢«æ­£ç¡®åˆ†é…çš„ reads æ€»æ•° / å‚ä¸èšç±»çš„ reads æ€»æ•°
    "æ­£ç¡®åˆ†é…" å®šä¹‰: read çš„ GT æ ‡ç­¾ == è¯¥ read æ‰€åœ¨é¢„æµ‹ç°‡çš„ majority GT æ ‡ç­¾
    Clover åŸå§‹æŒ‡æ ‡
    """
    # å…ˆæ‰¾æ¯ä¸ªé¢„æµ‹ç°‡çš„ majority GT
    cluster_gt = defaultdict(list)
    read_assignments = []  # (pred_cluster, gt_label)

    for p, g in zip(pred_labels, gt_labels):
        if p >= 0 and g >= 0:
            cluster_gt[p].append(g)
            read_assignments.append((p, g))

    cluster_majority = {}
    for cid, gt_list in cluster_gt.items():
        counter = Counter(gt_list)
        cluster_majority[cid] = counter.most_common(1)[0][0]

    correct = sum(1 for p, g in read_assignments if cluster_majority.get(p) == g)
    return correct / max(len(read_assignments), 1)


def compute_recall_at_cluster(pred_labels, gt_labels):
    """
    Recall@cluster: æ¯ä¸ª GT ç°‡è¢«æ­£ç¡®å¬å›çš„ reads æ¯”ä¾‹, å–å¹³å‡
    å±•ç¤ºæ–¹æ³•åœ¨æ¯ä¸ª GT ç°‡ä¸Šçš„å¬å›èƒ½åŠ›
    """
    # å»ºç«‹ GT ç°‡ â†’ reads æ˜ å°„
    gt_to_reads = defaultdict(list)
    for i, g in enumerate(gt_labels):
        if g >= 0:
            gt_to_reads[g].append(i)

    if not gt_to_reads:
        return 0.0

    # å»ºç«‹ pred ç°‡ â†’ majority GT æ˜ å°„
    cluster_gt_lists = defaultdict(list)
    for i, (p, g) in enumerate(zip(pred_labels, gt_labels)):
        if p >= 0 and g >= 0:
            cluster_gt_lists[p].append(g)

    cluster_majority = {}
    for cid, gt_list in cluster_gt_lists.items():
        counter = Counter(gt_list)
        cluster_majority[cid] = counter.most_common(1)[0][0]

    # å¯¹æ¯ä¸ª GT ç°‡, è®¡ç®—è¢«æ­£ç¡®å¬å›çš„æ¯”ä¾‹
    recalls = []
    for gt_id, read_indices in gt_to_reads.items():
        total = len(read_indices)
        correct = 0
        for idx in read_indices:
            p = pred_labels[idx]
            if p >= 0 and cluster_majority.get(p) == gt_id:
                correct += 1
        recalls.append(correct / total)

    return np.mean(recalls)


def compute_stratified_analysis(pred_labels, gt_labels, initial_cluster_sizes):
    """
    åˆ†å±‚åˆ†æ: æŒ‰ Clover åˆå§‹ç°‡å¤§å°åˆ†å±‚æŠ¥å‘ŠæŒ‡æ ‡
    initial_cluster_sizes: dict {cluster_id: size}

    åˆ†å±‚:
      Singleton (1 read)
      Small (2-5 reads)
      Medium (6-50 reads)
      Large (>50 reads)
    """
    # å°† reads æŒ‰å…¶æ‰€åœ¨åˆå§‹ç°‡çš„å¤§å°åˆ†å±‚
    strata = {
        'singleton': {'pred': [], 'gt': []},
        'small':     {'pred': [], 'gt': []},
        'medium':    {'pred': [], 'gt': []},
        'large':     {'pred': [], 'gt': []},
    }

    for i, (p, g) in enumerate(zip(pred_labels, gt_labels)):
        if p < 0 or g < 0:
            continue
        # è¿™ä¸ª read çš„åˆå§‹ç°‡å¤§å°
        size = initial_cluster_sizes.get(p, 0)
        if size <= 1:
            key = 'singleton'
        elif size <= 5:
            key = 'small'
        elif size <= 50:
            key = 'medium'
        else:
            key = 'large'
        strata[key]['pred'].append(p)
        strata[key]['gt'].append(g)

    results = {}
    for stratum, data in strata.items():
        if len(data['pred']) > 0:
            results[stratum] = {
                'count': len(data['pred']),
                'purity': compute_purity(data['pred'], data['gt']),
                'micro_acc': compute_micro_accuracy(data['pred'], data['gt']),
            }
        else:
            results[stratum] = {'count': 0, 'purity': 0.0, 'micro_acc': 0.0}

    return results


def compute_all_metrics(pred_labels, gt_labels, verbose=True):
    """
    è®¡ç®—å…¨å¥—è¯„ä¼°æŒ‡æ ‡

    Args:
        pred_labels: np.array, é¢„æµ‹ç°‡æ ‡ç­¾ (å…¨é‡, æ—  -1)
        gt_labels:   np.array, GT æ ‡ç­¾ (å…¨é‡, -1 è¡¨ç¤ºæ—  GT)
        verbose:     æ˜¯å¦æ‰“å°

    Returns:
        dict of metrics
    """
    # è¿‡æ»¤: åªä¿ç•™ pred >= 0 ä¸” gt >= 0 çš„ reads
    valid_mask = (pred_labels >= 0) & (gt_labels >= 0)
    pred_valid = pred_labels[valid_mask]
    gt_valid = gt_labels[valid_mask]

    if len(pred_valid) == 0:
        print("   âš ï¸ æ— æœ‰æ•ˆè¯„ä¼°æ ·æœ¬")
        return {}

    metrics = {}

    # AI ç¤¾åŒºæ ‡å‡†æŒ‡æ ‡
    metrics['ARI'] = compute_ari(pred_valid, gt_valid)
    metrics['NMI'] = compute_nmi(pred_valid, gt_valid)

    # Clover åŸå§‹æŒ‡æ ‡
    metrics['Purity'] = compute_purity(pred_valid, gt_valid)
    metrics['Recovery_Rate'] = compute_recovery_rate(pred_valid, gt_valid)
    metrics['Micro_Accuracy'] = compute_micro_accuracy(pred_valid, gt_valid)

    # å±•ç¤ºæ–¹æ³•ä¼˜åŠ¿
    metrics['Recall_at_cluster'] = compute_recall_at_cluster(pred_valid, gt_valid)

    # ç»Ÿè®¡ä¿¡æ¯
    n_pred_clusters = len(set(pred_valid))
    n_gt_clusters = len(set(gt_valid))
    metrics['n_pred_clusters'] = n_pred_clusters
    metrics['n_gt_clusters'] = n_gt_clusters
    metrics['n_evaluated_reads'] = len(pred_valid)

    if verbose:
        print(f"\n   {'='*60}")
        print(f"   ğŸ“Š èšç±»è¯„ä¼°ç»“æœ")
        print(f"   {'='*60}")
        print(f"   è¯„ä¼° reads: {metrics['n_evaluated_reads']:,}")
        print(f"   é¢„æµ‹ç°‡æ•°:   {n_pred_clusters:,}")
        print(f"   GT ç°‡æ•°:    {n_gt_clusters:,}")
        print(f"   {'â”€'*60}")
        print(f"   AI æ ‡å‡†æŒ‡æ ‡:")
        if metrics['ARI'] is not None:
            print(f"      ARI:              {metrics['ARI']:.4f}")
        if metrics['NMI'] is not None:
            print(f"      NMI:              {metrics['NMI']:.4f}")
        print(f"   Clover å¯¹æ¯”æŒ‡æ ‡:")
        print(f"      Purity:           {metrics['Purity']:.4f}  ({metrics['Purity']*100:.2f}%)")
        print(f"      Recovery Rate:    {metrics['Recovery_Rate']:.4f}  ({metrics['Recovery_Rate']*100:.2f}%)")
        print(f"      Micro Accuracy:   {metrics['Micro_Accuracy']:.4f}  ({metrics['Micro_Accuracy']*100:.2f}%)")
        print(f"   è¡¥å……æŒ‡æ ‡:")
        print(f"      Recall@cluster:   {metrics['Recall_at_cluster']:.4f}  ({metrics['Recall_at_cluster']*100:.2f}%)")
        print(f"   {'='*60}")

    return metrics


def save_metrics_report(metrics, output_path, round_info=""):
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    import json
    import os
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # æ¸…ç† None å€¼
    clean_metrics = {k: v for k, v in metrics.items() if v is not None}

    with open(output_path, 'w') as f:
        f.write(f"SSI-EC Clustering Evaluation Report\n")
        f.write(f"{'='*60}\n")
        if round_info:
            f.write(f"Info: {round_info}\n")
        f.write(f"\n")
        for k, v in clean_metrics.items():
            if isinstance(v, float):
                f.write(f"{k:25s}: {v:.6f}\n")
            else:
                f.write(f"{k:25s}: {v}\n")

    # åŒæ—¶ä¿å­˜ JSON æ ¼å¼
    json_path = output_path.replace('.txt', '.json')
    with open(json_path, 'w') as f:
        json.dump(clean_metrics, f, indent=2, default=str)

    print(f"   ğŸ’¾ è¯„ä¼°æŠ¥å‘Š: {output_path}")