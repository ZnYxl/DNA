# models/step1_visualizer.py - ä¿®å¤ç‰ˆæœ¬
"""
Step1è®­ç»ƒç»“æœå¯è§†åŒ–ä¸æŠ¥å‘Šç”Ÿæˆ
åŠŸèƒ½ï¼šç”Ÿæˆè®­ç»ƒæ›²çº¿ã€ç»Ÿè®¡å›¾è¡¨ã€è®­ç»ƒæŠ¥å‘Šç­‰
"""
import matplotlib.pyplot as plt
import matplotlib.style as mplstyle
import seaborn as sns
import json
import os
from datetime import datetime
import torch
import numpy as np

# è®¾ç½®ç»˜å›¾æ ·å¼
plt.rcParams['font.size'] = 12
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_palette("husl")

class Step1Visualizer:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        self.plots_dir = os.path.join(output_dir, "plots")
        self.logs_dir = os.path.join(output_dir, "logs")
        self.reports_dir = os.path.join(output_dir, "reports")
        self.models_dir = os.path.join(output_dir, "models")
        
        # åˆ›å»ºå­ç›®å½•
        for dir_path in [self.plots_dir, self.logs_dir, self.reports_dir, self.models_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def _check_history_data(self, history):
        """æ£€æŸ¥å¹¶æ¸…ç†å†å²æ•°æ®"""
        cleaned_history = {}
        
        for key, values in history.items():
            if isinstance(values, list) and len(values) > 0:
                # è¿‡æ»¤æ‰NaNå’Œinfå€¼
                clean_values = []
                for v in values:
                    if isinstance(v, (int, float)) and not (np.isnan(v) or np.isinf(v)):
                        clean_values.append(v)
                    else:
                        clean_values.append(0.0)  # ç”¨0æ›¿æ¢å¼‚å¸¸å€¼
                
                cleaned_history[key] = clean_values
            else:
                # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤å€¼
                cleaned_history[key] = [0.0]
                print(f"   âš ï¸ å†å²è®°å½• '{key}' ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼")
        
        return cleaned_history
    
    def plot_training_losses(self, history):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
        # âœ… æ£€æŸ¥æ•°æ®
        history = self._check_history_data(history)
        
        if len(history.get('total_loss', [])) == 0:
            print(f"   âš ï¸ æ²¡æœ‰æŸå¤±æ•°æ®ï¼Œè·³è¿‡æŸå¤±æ›²çº¿ç»˜åˆ¶")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Step1 Training Losses', fontsize=16, fontweight='bold')
        
        epochs = range(1, len(history['total_loss']) + 1)
        
        # æ€»æŸå¤±
        if 'total_loss' in history:
            axes[0, 0].plot(epochs, history['total_loss'], 'b-', linewidth=2, label='Total Loss')
            axes[0, 0].set_title('Total Loss', fontweight='bold')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].legend()
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        if 'contrastive_loss' in history:
            axes[0, 1].plot(epochs, history['contrastive_loss'], 'r-', linewidth=2, label='Contrastive Loss')
            axes[0, 1].set_title('Contrastive Loss', fontweight='bold')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].legend()
        
        # é‡å»ºæŸå¤±
        if 'reconstruction_loss' in history:
            axes[1, 0].plot(epochs, history['reconstruction_loss'], 'g-', linewidth=2, label='Reconstruction Loss')
            axes[1, 0].set_title('Reconstruction Loss', fontweight='bold')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].legend()
        
        # KLæ•£åº¦
        if 'kl_loss' in history:
            axes[1, 1].plot(epochs, history['kl_loss'], 'm-', linewidth=2, label='KL Divergence')
            axes[1, 1].set_title('KL Divergence', fontweight='bold')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Loss')
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        loss_plot_path = os.path.join(self.plots_dir, "training_losses.png")
        plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“Š æŸå¤±æ›²çº¿å·²ä¿å­˜: {loss_plot_path}")
        return loss_plot_path
    
    def plot_evidence_stats(self, history):
        """ç»˜åˆ¶Evidenceç»Ÿè®¡å›¾"""
        # âœ… æ£€æŸ¥æ•°æ®
        history = self._check_history_data(history)
        
        if len(history.get('avg_strength', [])) == 0:
            print(f"   âš ï¸ æ²¡æœ‰Evidenceæ•°æ®ï¼Œè·³è¿‡Evidenceç»Ÿè®¡å›¾ç»˜åˆ¶")
            return None
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('Evidence Statistics', fontsize=16, fontweight='bold')
        
        epochs = range(1, len(history['avg_strength']) + 1)
        
        # å¹³å‡Evidenceå¼ºåº¦
        if 'avg_strength' in history:
            axes[0].plot(epochs, history['avg_strength'], 'orange', linewidth=3, label='Average Strength')
            axes[0].set_title('Average Evidence Strength', fontweight='bold')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Strength')
            axes[0].grid(True, alpha=0.3)
            axes[0].legend()
        
        # é«˜ç½®ä¿¡åº¦æ¯”ä¾‹
        if 'high_conf_ratio' in history:
            high_conf_percent = [x * 100 for x in history['high_conf_ratio']]
            axes[1].plot(epochs, high_conf_percent, 'purple', linewidth=3, label='High Confidence %')
            axes[1].set_title('High Confidence Ratio', fontweight='bold')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Percentage (%)')
            axes[1].grid(True, alpha=0.3)
            axes[1].legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        evidence_plot_path = os.path.join(self.plots_dir, "evidence_stats.png")
        plt.savefig(evidence_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“ˆ Evidenceç»Ÿè®¡å›¾å·²ä¿å­˜: {evidence_plot_path}")
        return evidence_plot_path
    
    def plot_learning_curves(self, history):
        """ç»˜åˆ¶ç»¼åˆå­¦ä¹ æ›²çº¿"""
        # âœ… æ£€æŸ¥æ•°æ®
        history = self._check_history_data(history)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        required_keys = ['total_loss', 'avg_strength', 'high_conf_ratio']
        available_keys = [k for k in required_keys if k in history and len(history[k]) > 0]
        
        if len(available_keys) == 0:
            print(f"   âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼Œè·³è¿‡")
            return None
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # ä½¿ç”¨æœ€é•¿çš„åºåˆ—ä½œä¸ºepochåŸºå‡†
        max_length = max(len(history[k]) for k in available_keys)
        epochs = range(1, max_length + 1)
        
        # âœ… å®‰å…¨çš„å½’ä¸€åŒ–å‡½æ•°
        def safe_normalize(data):
            data = np.array(data)
            if len(data) == 0:
                return np.array([])
            
            data_min = np.min(data)
            data_max = np.max(data)
            
            if data_max == data_min:
                return np.zeros_like(data)  # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œè¿”å›0
            else:
                return (data - data_min) / (data_max - data_min)
        
        # ç»˜åˆ¶å¯ç”¨çš„æ›²çº¿
        if 'total_loss' in available_keys:
            normalized_loss = safe_normalize(history['total_loss'])
            if len(normalized_loss) > 0:
                loss_epochs = range(1, len(normalized_loss) + 1)
                ax.plot(loss_epochs, normalized_loss, 'b-', linewidth=2, label='Total Loss (norm)')
        
        if 'avg_strength' in available_keys:
            normalized_strength = safe_normalize(history['avg_strength'])
            if len(normalized_strength) > 0:
                strength_epochs = range(1, len(normalized_strength) + 1)
                ax.plot(strength_epochs, normalized_strength, 'orange', linewidth=2, label='Avg Strength (norm)')
        
        if 'high_conf_ratio' in available_keys:
            conf_ratio = history['high_conf_ratio']
            if len(conf_ratio) > 0:
                conf_epochs = range(1, len(conf_ratio) + 1)
                ax.plot(conf_epochs, conf_ratio, 'purple', linewidth=2, label='High Conf Ratio')
        
        ax.set_title('Learning Curves Overview', fontsize=16, fontweight='bold')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Normalized Value')
        ax.grid(True, alpha=0.3)
        ax.legend()
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        curves_plot_path = os.path.join(self.plots_dir, "learning_curves.png")
        plt.savefig(curves_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“‰ å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {curves_plot_path}")
        return curves_plot_path
    
    def save_config(self, args):
        """ä¿å­˜è®­ç»ƒé…ç½®"""
        config = {
            'experiment_info': {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'experiment_dir': args.experiment_dir,
                'output_dir': args.output_dir
            },
            'model_config': {
                'dim': args.dim,
                'max_length': args.max_length,
                'min_clusters': args.min_clusters,
                'device': args.device
            },
            'training_config': {
                'epochs': args.epochs,
                'batch_size': args.batch_size,
                'max_clusters_per_batch': args.max_clusters_per_batch,
                'lr': args.lr,
                'weight_decay': args.weight_decay,
                'save_interval': args.save_interval
            },
            'data_config': {
                'feddna_checkpoint': args.feddna_checkpoint
            }
        }
        
        config_path = os.path.join(self.logs_dir, "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"   âš™ï¸ é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
        return config_path
    
    def save_training_summary(self, history, model, args):
        """ç”Ÿæˆè®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
        # âœ… æ£€æŸ¥æ•°æ®
        history = self._check_history_data(history)
        
        summary_path = os.path.join(self.reports_dir, "training_summary.txt")
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("Step1 Evidence-driven Training Summary\n")
            f.write("=" * 80 + "\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write("ğŸ“‹ å®éªŒä¿¡æ¯:\n")
            f.write(f"   æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"   å®éªŒç›®å½•: {args.experiment_dir}\n")
            f.write(f"   è¾“å‡ºç›®å½•: {args.output_dir}\n\n")
            
            # æ¨¡å‹é…ç½®
            f.write("ğŸ§  æ¨¡å‹é…ç½®:\n")
            f.write(f"   ç‰¹å¾ç»´åº¦: {args.dim}\n")
            f.write(f"   åºåˆ—é•¿åº¦: {args.max_length}\n")
            f.write(f"   æœ€å°ç°‡æ•°: {args.min_clusters}\n")
            f.write(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\n\n")
            
            # è®­ç»ƒé…ç½®
            f.write("ğŸš€ è®­ç»ƒé…ç½®:\n")
            f.write(f"   è®­ç»ƒè½®æ•°: {args.epochs}\n")
            f.write(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}\n")
            f.write(f"   å­¦ä¹ ç‡: {args.lr}\n")
            f.write(f"   æƒé‡è¡°å‡: {args.weight_decay}\n\n")
            
            # âœ… å®‰å…¨çš„è®­ç»ƒç»“æœè®°å½•
            f.write("ğŸ“Š è®­ç»ƒç»“æœ:\n")
            if len(history.get('total_loss', [])) > 0:
                f.write(f"   æœ€ç»ˆæ€»æŸå¤±: {history['total_loss'][-1]:.6f}\n")
                f.write(f"   æœ€ç»ˆå¯¹æ¯”æŸå¤±: {history.get('contrastive_loss', [0])[-1]:.6f}\n")
                f.write(f"   æœ€ç»ˆé‡å»ºæŸå¤±: {history.get('reconstruction_loss', [0])[-1]:.6f}\n")
                f.write(f"   æœ€ç»ˆKLæ•£åº¦: {history.get('kl_loss', [0])[-1]:.6f}\n")
                f.write(f"   æœ€ç»ˆå¹³å‡å¼ºåº¦: {history.get('avg_strength', [0])[-1]:.4f}\n")
                f.write(f"   æœ€ç»ˆé«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {history.get('high_conf_ratio', [0])[-1]*100:.2f}%\n\n")
                
                # è®­ç»ƒè¶‹åŠ¿
                f.write("ğŸ“ˆ è®­ç»ƒè¶‹åŠ¿:\n")
                if len(history['total_loss']) > 1:
                    initial_loss = history['total_loss'][0]
                    final_loss = history['total_loss'][-1]
                    if initial_loss > 0:
                        loss_reduction = (initial_loss - final_loss) / initial_loss * 100
                        f.write(f"   æŸå¤±ä¸‹é™: {loss_reduction:.2f}%\n")
                
                if len(history.get('avg_strength', [])) > 1:
                    initial_strength = history['avg_strength'][0]
                    final_strength = history['avg_strength'][-1]
                    if initial_strength > 0:
                        strength_change = (final_strength - initial_strength) / initial_strength * 100
                        f.write(f"   å¼ºåº¦å˜åŒ–: {strength_change:+.2f}%\n")
                
                if len(history.get('high_conf_ratio', [])) > 1:
                    initial_conf = history['high_conf_ratio'][0]
                    final_conf = history['high_conf_ratio'][-1]
                    conf_change = (final_conf - initial_conf) * 100
                    f.write(f"   ç½®ä¿¡åº¦å˜åŒ–: {conf_change:+.2f}ä¸ªç™¾åˆ†ç‚¹\n\n")
            else:
                f.write("   âš ï¸ è®­ç»ƒæ•°æ®ä¸å®Œæ•´æˆ–è®­ç»ƒæœªæˆåŠŸå®Œæˆ\n\n")
            
            # æ–¹æ³•è®ºæ£€æŸ¥
            f.write("âœ… æ–¹æ³•è®ºéªŒè¯:\n")
            f.write("   - Evidence-drivenå­¦ä¹ : âœ“\n")
            f.write("   - ä¸¥æ ¼è‡ªç›‘ç£è®­ç»ƒ: âœ“\n")
            f.write("   - GTä»…ç”¨äºè¯„ä¼°: âœ“\n")
            f.write("   - æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤: âœ“\n")
            f.write("   - Warm-upæœºåˆ¶: âœ“\n\n")
            
            # æ–‡ä»¶æ¸…å•
            f.write("ğŸ“ è¾“å‡ºæ–‡ä»¶:\n")
            f.write("   models/\n")
            f.write("   â”œâ”€â”€ step1_final_model.pth (æœ€ç»ˆæ¨¡å‹)\n")
            f.write("   â””â”€â”€ step1_epoch_*.pth (æ£€æŸ¥ç‚¹)\n")
            f.write("   plots/\n")
            f.write("   â”œâ”€â”€ training_losses.png (æŸå¤±æ›²çº¿)\n")
            f.write("   â”œâ”€â”€ evidence_stats.png (Evidenceç»Ÿè®¡)\n")
            f.write("   â””â”€â”€ learning_curves.png (å­¦ä¹ æ›²çº¿)\n")
            f.write("   logs/\n")
            f.write("   â””â”€â”€ config.json (é…ç½®æ–‡ä»¶)\n")
            f.write("   reports/\n")
            f.write("   â””â”€â”€ training_summary.txt (æœ¬æŠ¥å‘Š)\n")
        
        print(f"   ğŸ“„ è®­ç»ƒæ€»ç»“å·²ä¿å­˜: {summary_path}")
        return summary_path
    
    def save_model_info(self, model):
        """ä¿å­˜æ¨¡å‹ç»“æ„ä¿¡æ¯"""
        model_info_path = os.path.join(self.reports_dir, "model_info.txt")
        
        with open(model_info_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("Step1 Model Architecture\n")
            f.write("=" * 80 + "\n\n")
            
            # æ¨¡å‹ç»“æ„
            f.write("ğŸ—ï¸ æ¨¡å‹ç»“æ„:\n")
            f.write(str(model))
            f.write("\n\n")
            
            # å‚æ•°ç»Ÿè®¡
            f.write("ğŸ“Š å‚æ•°ç»Ÿè®¡:\n")
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            f.write(f"   æ€»å‚æ•°: {total_params:,}\n")
            f.write(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\n")
            f.write(f"   å†»ç»“å‚æ•°: {total_params - trainable_params:,}\n\n")
            
            # å„å±‚å‚æ•°
            f.write("ğŸ” å„å±‚å‚æ•°è¯¦æƒ…:\n")
            for name, param in model.named_parameters():
                f.write(f"   {name}: {param.shape} ({param.numel():,} params)\n")
        
        print(f"   ğŸ—ï¸ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {model_info_path}")
        return model_info_path
    
    def generate_all_outputs(self, history, model, args):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        print(f"\nğŸ“Š ç”Ÿæˆè®­ç»ƒç»“æœæ–‡ä»¶...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # âœ… æ£€æŸ¥å†å²æ•°æ®çŠ¶æ€
        print(f"ğŸ“‹ å†å²æ•°æ®æ£€æŸ¥:")
        for key, values in history.items():
            if isinstance(values, list):
                print(f"   {key}: {len(values)} æ¡è®°å½•")
            else:
                print(f"   {key}: {type(values)}")
        
        # ç”Ÿæˆå›¾è¡¨ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
        try:
            self.plot_training_losses(history)
        except Exception as e:
            print(f"   âŒ æŸå¤±æ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")
        
        try:
            self.plot_evidence_stats(history)
        except Exception as e:
            print(f"   âŒ Evidenceç»Ÿè®¡å›¾ç”Ÿæˆå¤±è´¥: {e}")
        
        try:
            self.plot_learning_curves(history)
        except Exception as e:
            print(f"   âŒ å­¦ä¹ ï¿½ï¿½ï¿½çº¿ç”Ÿæˆå¤±è´¥: {e}")
        
        # ä¿å­˜é…ç½®å’ŒæŠ¥å‘Š
        try:
            self.save_config(args)
            self.save_training_summary(history, model, args)
            self.save_model_info(model)
        except Exception as e:
            print(f"   âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        
        print(f"\nâœ… è¾“å‡ºæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“‚ æŸ¥çœ‹ç»“æœ: {self.output_dir}")
