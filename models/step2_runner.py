# models/step2_runner.py
"""
Step2 ä¸»å…¥å£ï¼šEvidence-Guided Refinement & Decoding
å…³é”®ï¼šä¸è®­ç»ƒï¼Œåªæ¨ç†+å†³ç­–
"""
import torch
import os
import sys
import argparse
from datetime import datetime

# âœ… æ·»åŠ è·¯å¾„å¤„ç†ï¼ˆä¸step1_train.pyç›¸åŒï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# ç°åœ¨å¯ä»¥æ­£å¸¸å¯¼å…¥
from models.step1_model import Step1EvidentialModel, load_pretrained_feddna
from models.step1_data import CloverDataLoader, Step1Dataset
from models.step2_refine import (
    select_high_confidence_reads,
    compute_cluster_centroids,
    refine_low_confidence_reads,
    compute_adaptive_delta
)
from models.step2_decode import (
    decode_cluster_consensus,
    save_consensus_sequences,
    compute_consensus_quality_metrics
)

# models/step2_runner.py - ä¿®å¤æ¨¡å‹åŠ è½½éƒ¨åˆ†

@torch.no_grad()
def run_step2(args):
    """
    Step2ä¸»æµç¨‹ï¼š
    1. åŠ è½½Step1æ¨¡å‹ï¼ˆfreezeï¼‰
    2. æ¨ç†å¾—åˆ°embeddings + evidence
    3. è¯æ®ç­›é€‰
    4. ç°‡ä¿®æ­£
    5. consensusè§£ç 
    """
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ========== 1ï¸âƒ£ åŠ è½½Step1æ¨¡å‹ ==========
    print("\n" + "=" * 60)
    print("ğŸ“¦ åŠ è½½Step1è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("=" * 60)
    
    checkpoint = torch.load(args.step1_checkpoint, map_location=device)
    
    # âœ… ä½¿ç”¨Step1è®­ç»ƒæ—¶ä¿å­˜çš„å‚æ•°
    if 'args' in checkpoint:
        step1_args = checkpoint['args']
        print(f"   ğŸ“‹ Step1è®­ç»ƒå‚æ•°:")
        print(f"      dim: {step1_args.get('dim', args.dim)}")
        print(f"      max_length: {step1_args.get('max_length', args.max_length)}")
        
        # ä½¿ç”¨Step1çš„å‚æ•°
        model_dim = step1_args.get('dim', args.dim)
        model_max_length = step1_args.get('max_length', args.max_length)
    else:
        print(f"   âš ï¸ checkpointä¸­æ²¡æœ‰ä¿å­˜argsï¼Œä½¿ç”¨å½“å‰å‚æ•°")
        model_dim = args.dim
        model_max_length = args.max_length
    
    # é‡å»ºæ•°æ®åŠ è½½å™¨
    data_loader = CloverDataLoader(args.experiment_dir)
    num_clusters = len(set(data_loader.clover_labels))
    
    # âœ… ä½¿ç”¨Step1ç›¸åŒçš„å‚æ•°é‡å»ºæ¨¡å‹
    model = Step1EvidentialModel(
        dim=model_dim,
        max_length=model_max_length,
        num_clusters=num_clusters,
        device=device
    ).to(device)
    
    # âœ… å°è¯•åŠ è½½ï¼Œå¦‚æœæœ‰ä¸åŒ¹é…çš„å‚æ•°å°±å¿½ç•¥
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=True)
        print(f"   âœ… æ¨¡å‹å®Œå…¨åŒ¹é…åŠ è½½")
    except RuntimeError as e:
        print(f"   âš ï¸ æ¨¡å‹ç»“æ„ä¸å®Œå…¨åŒ¹é…: {e}")
        print(f"   ğŸ”„ å°è¯•å¿½ç•¥ä¸åŒ¹é…çš„å‚æ•°...")
        
        # è·å–å½“å‰æ¨¡å‹çš„å‚æ•°å
        model_keys = set(model.state_dict().keys())
        checkpoint_keys = set(checkpoint['model_state_dict'].keys())
        
        missing_keys = model_keys - checkpoint_keys
        unexpected_keys = checkpoint_keys - model_keys
        
        print(f"      ç¼ºå¤±å‚æ•°: {missing_keys}")
        print(f"      å¤šä½™å‚æ•°: {unexpected_keys}")
        
        # åªåŠ è½½åŒ¹é…çš„å‚æ•°
        filtered_state_dict = {
            k: v for k, v in checkpoint['model_state_dict'].items() 
            if k in model_keys
        }
        
        model.load_state_dict(filtered_state_dict, strict=False)
        print(f"   âœ… å·²åŠ è½½åŒ¹é…çš„å‚æ•°ï¼Œå¿½ç•¥ä¸åŒ¹é…éƒ¨åˆ†")
    
    model.eval()  # âœ… è¯„ä¼°æ¨¡å¼ï¼Œfreezeå‚æ•°
    
    print(f"   ğŸ“Š æœ€ç»ˆæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")    
    print(f"   âœ… æ¨¡å‹å·²åŠ è½½: {args.step1_checkpoint}")
    print(f"   ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # ========== 2ï¸âƒ£ æ¨ç†å…¨éƒ¨æ•°æ® ==========
    print("\n" + "=" * 60)
    print("ğŸ”® Step1æ¨¡å‹æ¨ç†ï¼ˆæå–embeddings + evidenceï¼‰")
    print("=" * 60)
    
    dataset = Step1Dataset(data_loader, max_len=args.max_length)
    
    all_embeddings = []
    all_strength = []
    all_alpha = []
    all_evidence = []
    all_labels = []
    all_read_ids = []
    
    print(f"   å¤„ç† {len(dataset)} æ¡reads...")
    
    for idx in range(len(dataset)):
        item = dataset[idx]
        reads = item['encoding'].unsqueeze(0).to(device)  # (1, L, 4)
        
        # Step1æ¨ç†
        embeddings, pooled_emb = model.encode_reads(reads)
        evidence, strength, alpha = model.decode_to_evidence(embeddings)
        
        all_embeddings.append(pooled_emb.squeeze(0))
        all_strength.append(strength.mean())  # å¹³å‡strength
        all_alpha.append(alpha.squeeze(0))
        all_evidence.append(evidence.squeeze(0))
        all_labels.append(item['clover_label'])
        all_read_ids.append(idx)
        
        if (idx + 1) % 1000 == 0:
            print(f"      å·²å¤„ç†: {idx+1}/{len(dataset)}")
    
    # è½¬æ¢ä¸ºå¼ é‡
    embeddings = torch.stack(all_embeddings)  # (N, D)
    strength = torch.stack(all_strength)      # (N,)
    alpha = torch.stack(all_alpha)            # (N, L, 4)
    evidence = torch.stack(all_evidence)      # (N, L, 4)
    labels = torch.tensor(all_labels, device=device)  # (N,)
    
    print(f"   âœ… æ¨ç†å®Œæˆ:")
    print(f"      Embeddings: {embeddings.shape}")
    print(f"      Evidence: {evidence.shape}")
    print(f"      å¹³å‡strength: {strength.mean():.4f}")
    
    # ========== 3ï¸âƒ£ Phase A: è¯æ®ç­›é€‰ ==========
    print("\n" + "=" * 60)
    print("ğŸ” Phase A: è¯æ®ç­›é€‰")
    print("=" * 60)
    
    high_conf_mask, tau_used = select_high_confidence_reads(
        strength, 
        tau=args.tau,
        quantile=args.quantile
    )
    
    # ========== 4ï¸âƒ£ Phase B: ç°‡ä¿®æ­£ ==========
    print("\n" + "=" * 60)
    print("ğŸ”„ Phase B: ç°‡ä¿®æ­£")
    print("=" * 60)
    
    # è®¡ç®—ç°‡ä¸­å¿ƒï¼ˆåªç”¨é«˜ç½®ä¿¡åº¦ï¼‰
    centroids, cluster_sizes = compute_cluster_centroids(
        embeddings, labels, high_conf_mask
    )
    
    # è‡ªé€‚åº”è®¡ç®—delta
    if args.delta is None:
        delta = compute_adaptive_delta(
            embeddings, centroids, percentile=args.delta_percentile
        )
    else:
        delta = args.delta
        print(f"   ğŸ¯ ä½¿ç”¨å›ºå®šdelta: {delta:.4f}")
    
    # ä¿®æ­£ä½ç½®ä¿¡åº¦reads
    new_labels, noise_mask, refine_stats = refine_low_confidence_reads(
        embeddings, labels, high_conf_mask, centroids, delta
    )
    
    # ========== 5ï¸âƒ£ Phase C: Consensusè§£ç  ==========
    print("\n" + "=" * 60)
    print("ğŸ§¬ Phase C: Consensusè§£ç ")
    print("=" * 60)
    
    consensus_dict = decode_cluster_consensus(
        evidence, alpha, new_labels, strength
    )
    
    # ä¿å­˜å…±è¯†åºåˆ—
    os.makedirs(args.output_dir, exist_ok=True)
    consensus_path = os.path.join(args.output_dir, "consensus_sequences.fasta")
    save_consensus_sequences(consensus_dict, consensus_path)
    
    # ========== 6ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š ==========
    print("\n" + "=" * 60)
    print("ğŸ“Š Step2 æœ€ç»ˆç»Ÿè®¡")
    print("=" * 60)
    
    print(f"\n   ğŸ“ˆ ç°‡ä¿®æ­£æ•ˆæœ:")
    print(f"      åŸå§‹ç°‡æ•°: {len(torch.unique(labels))}")
    print(f"      ä¿®æ­£åç°‡æ•°: {len(consensus_dict)}")
    print(f"      æ€»å™ªå£°reads: {noise_mask.sum()}/{len(labels)} ({noise_mask.float().mean()*100:.1f}%)")
    print(f"      é‡æ–°åˆ†é…: {refine_stats['reassigned']}")
    print(f"      æ–°å¢å™ªå£°: {refine_stats['marked_noise']}")
    
    print(f"\n   ğŸ§¬ Consensusè´¨é‡:")
    for label in sorted(list(consensus_dict.keys())[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
        info = consensus_dict[label]
        print(f"      ç°‡{label}: {info['num_reads']} reads, "
              f"strength={info['avg_strength']:.3f}, "
              f"len={len(info['consensus_seq'])}")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    results = {
        'new_labels': new_labels.cpu(),
        'noise_mask': noise_mask.cpu(),
        'high_conf_mask': high_conf_mask.cpu(),
        'strength': strength.cpu(),
        'consensus_dict': consensus_dict,
        'refine_stats': refine_stats,
        'args': vars(args)
    }
    
    results_path = os.path.join(args.output_dir, "step2_results.pth")
    torch.save(results, results_path)
    print(f"\n   ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {results_path}")
    
    print(f"\nğŸ‰ Step2å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step2: Evidence-Guided Refinement & Decoding')
    
    # è¾“å…¥å‚æ•°
    parser.add_argument('--experiment_dir', type=str, required=True,
                       help='å®éªŒç›®å½•ï¼ˆä¸Step1ç›¸åŒï¼‰')
    parser.add_argument('--step1_checkpoint', type=str, required=True,
                       help='Step1è®­ç»ƒå¥½çš„æ¨¡å‹checkpoint')
    
    # æ¨¡å‹å‚æ•°ï¼ˆéœ€ä¸Step1ä¸€è‡´ï¼‰
    parser.add_argument('--dim', type=int, default=256)
    parser.add_argument('--max_length', type=int, default=150)
    parser.add_argument('--device', type=str, default='cuda')
    
    # Step2å‚æ•°
    parser.add_argument('--tau', type=float, default=None,
                       help='ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆNone=è‡ªåŠ¨ï¼‰')
    parser.add_argument('--quantile', type=float, default=0.5,
                       help='tauçš„åˆ†ä½æ•°ï¼ˆå½“tau=Noneæ—¶ï¼‰')
    parser.add_argument('--delta', type=float, default=None,
                       help='è·ç¦»é˜ˆå€¼ï¼ˆNone=è‡ªé€‚åº”ï¼‰')
    parser.add_argument('--delta_percentile', type=int, default=10,
                       help='deltaçš„ç™¾åˆ†ä½æ•°ï¼ˆæ¥æ”¶æœ€è¿‘X%ï¼‰')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str,
                       default=f'./step2_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è¿è¡ŒStep2
    results = run_step2(args)
