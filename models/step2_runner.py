# models/step2_runner.py
"""
Step2 ä¸»å…¥å£ï¼šEvidence-Guided Refinement & Decoding
å…³é”®ï¼šä¸è®­ç»ƒï¼Œåªæ¨ç†+å†³ç­–
âœ… ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ï¼šç°‡å†…æ¯”è¾ƒï¼Œåå‘ç¡®å®šæ€§
âœ… ä¿®å¤ç‰ˆ v3 (æœ€ç»ˆç‰ˆ)ï¼š
   1. ğŸŒŸ æ•°æ®å¯¹é½ä¿®å¤ï¼šè¾“å‡ºæ ‡ç­¾æ•°é‡ä¸¥æ ¼ç­‰äºåŸå§‹ Reads æ•°é‡ (è§£å†³ Mismatch æŠ¥é”™)
   2. åŒ…å« length_adapter æƒé‡é¢„åŠ è½½ä¿®å¤
   3. åŒ…å« å¼ºåˆ¶ Padding é€»è¾‘
   4. åŒ…å« DataLoader å¤šè¿›ç¨‹æ‰¹é‡æ¨ç†åŠ é€Ÿ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import argparse
import numpy as np
from datetime import datetime

# âœ… æ·»åŠ è·¯å¾„å¤„ç†
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from models.step1_model import Step1EvidentialModel
from models.step1_data import CloverDataLoader, Step1Dataset
from models.step2_refine import (
    split_confidence_by_percentile,
    compute_cluster_centroids,
    refine_low_confidence_reads,
    compute_adaptive_delta
)
from models.step2_decode import (
    decode_cluster_consensus,
    save_consensus_sequences
)


@torch.no_grad()
def run_step2(args):
    """
    Step2ä¸»æµç¨‹ï¼šæ¨ç† -> ä¿®æ­£ -> è§£ç  -> å¯¹é½ä¿å­˜
    """
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # ========== 1ï¸âƒ£ åŠ è½½Step1æ¨¡å‹ ==========
    print("\n" + "=" * 60)
    print("ğŸ“¦ åŠ è½½Step1è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("=" * 60)

    try:
        checkpoint = torch.load(args.step1_checkpoint, map_location=device)
        print(f"   âœ… checkpointåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ checkpointåŠ è½½å¤±è´¥: {e}")
        return None

    # âœ… è·å–æ¨¡å‹å‚æ•°
    if 'args' in checkpoint:
        step1_args = checkpoint['args']
        model_dim = step1_args.get('dim', args.dim)
        model_max_length = step1_args.get('max_length', args.max_length)
    else:
        model_dim = args.dim
        model_max_length = args.max_length

    # âœ… é‡å»ºæ•°æ®åŠ è½½å™¨ & è·å–æ€»æ•° (å…³é”®ä¿®å¤)
    try:
        data_loader = CloverDataLoader(args.experiment_dir)
        TOTAL_READS_COUNT = len(data_loader.reads)  # ğŸŒŸ å¿…é¡»è·å–åŸå§‹æ€»æ•°
        num_clusters = len(set(data_loader.clover_labels))
        print(f"   ğŸ“Š æ•°æ®ç»Ÿè®¡: {TOTAL_READS_COUNT} æ€»Reads, {num_clusters} ç°‡")
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # âœ… é‡å»ºæ¨¡å‹
    try:
        model = Step1EvidentialModel(
            dim=model_dim,
            max_length=model_max_length,
            num_clusters=num_clusters,
            device=device
        ).to(device)
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

    # âœ… ä¿®å¤: æƒé‡é¢„åŠ è½½ (length_adapter)
    try:
        state_dict = checkpoint['model_state_dict']
        if 'length_adapter.weight' in state_dict:
            weight_shape = state_dict['length_adapter.weight'].shape
            model.length_adapter = nn.Linear(weight_shape[1], weight_shape[0]).to(device)
            print(f"   ğŸ”§ é¢„åˆå§‹åŒ– length_adapter: {weight_shape}")
    except Exception:
        pass

    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    model.eval()

    # ========== 2ï¸âƒ£ æ‰¹é‡æ¨ç† (å¸¦ç´¢å¼•è®°å½•) ==========
    print("\n" + "=" * 60)
    print("ğŸ”® Step1æ¨¡å‹æ‰¹é‡æ¨ç†")
    print("=" * 60)

    try:
        # æ³¨æ„ï¼šStep1Dataset å¯èƒ½ä¼šè¿‡æ»¤æ‰ -1 çš„æ•°æ®ï¼Œæ‰€ä»¥ len(dataset) <= TOTAL_READS_COUNT
        dataset = Step1Dataset(data_loader, max_len=model_max_length)
        
        # ä½¿ç”¨ DataLoader åŠ é€Ÿ
        inference_loader = torch.utils.data.DataLoader(
            dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True
        )
        print(f"   ğŸ“Š æœ‰æ•ˆæ¨ç†æ•°æ®: {len(dataset)} (Batch Size: 1024)")
    except Exception as e:
        print(f"   âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return None

    all_embeddings = []
    all_strength = []
    all_alpha = []
    all_evidence = []
    all_labels = []
    all_real_indices = []  # ğŸŒŸ è®°å½•çœŸå®ç´¢å¼•

    print(f"   ğŸ”„ å¼€å§‹æ¨ç†...")

    for batch_idx, batch_data in enumerate(inference_loader):
        reads = batch_data['encoding'].to(device) # (B, L, 4)
        labels = batch_data['clover_label']
        read_indices = batch_data['read_idx']     # (B,) è·å–åŸå§‹ç´¢å¼•

        # ğŸŒŸ å¼ºåˆ¶ Padding é€»è¾‘
        curr_len = reads.shape[1]
        target_len = model_max_length
        if curr_len > target_len:
            reads = reads[:, :target_len, :]
        elif curr_len < target_len:
            pad_len = target_len - curr_len
            reads = F.pad(reads, (0, 0, 0, pad_len), "constant", 0)

        # æ¨ç†
        with torch.no_grad():
            embeddings, pooled_emb = model.encode_reads(reads)
            evidence, strength, alpha = model.decode_to_evidence(embeddings)

        # æ”¶é›†ç»“æœ
        all_embeddings.append(pooled_emb.cpu())
        all_strength.append(strength.mean(dim=1).cpu())
        all_alpha.append(alpha.cpu())
        all_evidence.append(evidence.cpu())
        all_real_indices.append(read_indices) # è®°å½•ç´¢å¼•

        if isinstance(labels, torch.Tensor):
            all_labels.extend(labels.tolist())
        else:
            all_labels.extend(labels)

        if (batch_idx + 1) % 50 == 0:
            print(f"      å·²å¤„ç† Batch: {batch_idx + 1}/{len(inference_loader)}", end='\r')

    if len(all_embeddings) == 0:
        print(f"\n   âŒ æ²¡æœ‰æˆåŠŸæ¨ç†çš„readsï¼")
        return None

    # æ‹¼æ¥å¼ é‡
    embeddings = torch.cat(all_embeddings, dim=0).to(device)
    strength = torch.cat(all_strength, dim=0).to(device)
    alpha = torch.cat(all_alpha, dim=0).to(device)
    evidence = torch.cat(all_evidence, dim=0).to(device)
    labels = torch.tensor(all_labels, device=device)
    
    # ğŸŒŸ æ‹¼æ¥æ‰€æœ‰ç´¢å¼•
    flat_real_indices = torch.cat(all_real_indices).numpy()

    print(f"\n   âœ… æ¨ç†å®Œæˆ. å¼ é‡å½¢çŠ¶: {embeddings.shape}")

    # ========== 3ï¸âƒ£ Phase A: ç›¸å¯¹è¯æ®ç­›é€‰ ==========
    print("\n" + "=" * 60)
    print("ğŸ” Phase A: ç›¸å¯¹è¯æ®ç­›é€‰")
    print("=" * 60)

    low_conf_mask, conf_stats = split_confidence_by_percentile(
        strength, labels, p=args.uncertainty_percentile
    )
    high_conf_mask = ~low_conf_mask

    # ========== 4ï¸âƒ£ Phase B: ç°‡ä¿®æ­£ ==========
    print("\n" + "=" * 60)
    print("ğŸ”„ Phase B: ç°‡ä¿®æ­£")
    print("=" * 60)

    centroids, cluster_sizes = compute_cluster_centroids(
        embeddings, labels, high_conf_mask
    )

    if args.delta is None:
        delta = compute_adaptive_delta(
            embeddings, centroids, percentile=args.delta_percentile
        )
    else:
        delta = args.delta
    
    # new_labels çš„é•¿åº¦ = len(dataset) (å³æœ‰æ•ˆreadsçš„æ•°é‡)
    new_labels, noise_mask, refine_stats = refine_low_confidence_reads(
        embeddings, labels, low_conf_mask, centroids, delta
    )

    # ========== 5ï¸âƒ£ Phase C: Consensus ==========
    print("\n" + "=" * 60)
    print("ğŸ§¬ Phase C: Consensusè§£ç ")
    print("=" * 60)

    consensus_dict = decode_cluster_consensus(
        evidence, alpha, new_labels, strength, high_conf_mask
    )

    os.makedirs(args.output_dir, exist_ok=True)
    consensus_path = os.path.join(args.output_dir, "consensus_sequences.fasta")
    save_consensus_sequences(consensus_dict, consensus_path)

    # ========== 6ï¸âƒ£ å‡†å¤‡ä¸‹ä¸€è½®è¿­ä»£çš„æ•°æ® (ğŸŒŸæ ¸å¿ƒä¿®å¤) ==========
    print("\n" + "=" * 60)
    print("ğŸ”„ å‡†å¤‡ä¸‹ä¸€è½® (Next Round) æ•°æ® - å¯¹é½ä¿®å¤")
    print("=" * 60)

    next_round_dir = os.path.join(args.experiment_dir, "04_Iterative_Labels")
    os.makedirs(next_round_dir, exist_ok=True)
    timestamp_id = datetime.now().strftime("%H%M%S")
    label_save_path = os.path.join(next_round_dir, f"refined_labels_{timestamp_id}.txt")

    # ğŸŒŸ å…³é”®é€»è¾‘ï¼šè¿˜åŸåˆ°å…¨é•¿æ•°ç»„
    # 1. åˆ›å»ºå…¨é•¿æ•°ç»„ï¼Œé»˜è®¤å¡« -1 (å™ªå£°)
    full_refined_labels = np.full(TOTAL_READS_COUNT, -1, dtype=int)
    
    # 2. å°†ä¿®æ­£åçš„ new_labels å¡«å…¥å¯¹åº”çš„åŸå§‹ä½ç½®
    # new_labels æ˜¯ Tensor (N_valid,), flat_real_indices æ˜¯ numpy (N_valid,)
    current_refined_labels = new_labels.cpu().numpy()
    
    # å®‰å…¨æ£€æŸ¥
    if len(flat_real_indices) != len(current_refined_labels):
        print(f"   âŒ ä¸¥é‡é”™è¯¯: ç´¢å¼•æ•°é‡ä¸æ ‡ç­¾æ•°é‡ä¸ä¸€è‡´!")
        return None
        
    full_refined_labels[flat_real_indices] = current_refined_labels
    
    # 3. ä¿å­˜å…¨é•¿æ–‡ä»¶
    np.savetxt(label_save_path, full_refined_labels, fmt='%d')
    
    print(f"   ğŸ“ ä¿®æ­£æ ‡ç­¾å·²ä¿å­˜: {label_save_path}")
    print(f"      - åŸå§‹Readsæ€»æ•°: {TOTAL_READS_COUNT}")
    print(f"      - ä¿å­˜æ ‡ç­¾æ€»æ•°: {len(full_refined_labels)} (å¿…é¡»ä¸€è‡´)")
    print(f"      - æœ‰æ•ˆä¿®æ­£æ•°: {len(current_refined_labels)}")
    print(f"      - è‡ªåŠ¨æ ‡è®°å™ªå£°(-1): {TOTAL_READS_COUNT - len(current_refined_labels)}")

    # ä¿å­˜ç»“æœ dict
    try:
        results = {
            'new_labels': new_labels.cpu(),
            'noise_mask': noise_mask.cpu(),
            'strength': strength.cpu(),
            'consensus_dict': consensus_dict,
            'next_round_files': {
                'labels': label_save_path,
                'reference': consensus_path
            },
            'args': vars(args)
        }
        results_path = os.path.join(args.output_dir, "step2_results.pth")
        torch.save(results, results_path)
        print(f"\n   ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {results_path}")
    except Exception as e:
        print(f"   âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {e}")
        results = None

    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step2: Evidence-Guided Refinement & Decoding')
    parser.add_argument('--experiment_dir', type=str, required=True)
    parser.add_argument('--step1_checkpoint', type=str, required=True)
    parser.add_argument('--dim', type=int, default=256)
    parser.add_argument('--max_length', type=int, default=150)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--uncertainty_percentile', type=float, default=0.2)
    parser.add_argument('--delta', type=float, default=None)
    parser.add_argument('--delta_percentile', type=int, default=10)
    parser.add_argument('--output_dir', type=str, default=f'./step2_results_{datetime.now().strftime("%H%M%S")}')

    args = parser.parse_args()

    try:
        run_step2(args)
    except Exception as e:
        print(f"âŒ Step2æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)