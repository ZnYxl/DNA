# models/step2_runner.py
"""
Step2 ä¸»å…¥å£ï¼šEvidence-Guided Refinement & Decoding (é¡¶åˆŠå¢å¼ºç‰ˆ)
åŠŸèƒ½ï¼š
1. å™ªå£°å¤æ´» (Resurrection): é€šè¿‡å“¨å…µæ ‡ç­¾æŠ€æœ¯å¬å›ä¸Šä¸€è½®è¯¯åˆ çš„ reads
2. çŠ¶æ€ä¼ é€’: ä¿å­˜ strength, u_epi, u_ale ä¾›ä¸‹ä¸€è½®åŠ¨é‡æ›´æ–°å’Œé‡‡æ ·
3. èšç±»çº é”™: åŸºäºè¯æ®çš„è·ç¦»åˆ¤å†³ï¼Œè§£å†³ Clover è¿‡èšç±»é—®é¢˜
4. é¡¶åˆŠæ•°æ®ç›‘æ§: å®æ—¶è®¡ç®—ç°‡åˆå¹¶æ•°é‡ä¸ Micro Accuracy
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import argparse
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir  = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.step1_model  import Step1EvidentialModel, decompose_uncertainty
from models.step1_data   import CloverDataLoader, Step1Dataset
from models.step2_refine import (
    split_confidence_by_zone,
    compute_centroids_weighted,
    compute_global_delta,
    refine_reads
)
from models.step2_decode import (
    decode_cluster_consensus,
    save_consensus_sequences
)
from models.step1_visualizer import Step1Visualizer

# ---------------------------------------------------------------------------
# å…¨å±€è¶…å‚
# ---------------------------------------------------------------------------
MOMENTUM_CURR = 0.7          # å½“å‰è½® strength æƒé‡
MOMENTUM_PREV = 0.3          # ä¸Šä¸€è½® strength æƒé‡
RESURRECTION_SENTINEL = 999999 # å“¨å…µå€¼ï¼šè®© Step1Dataset æ”¾è¡Œï¼Œä½† Step1 è®­ç»ƒåº”å¿½ç•¥

@torch.no_grad()
def run_step2(args):
    """
    Step 2 æ‰§è¡Œæµç¨‹ï¼š
    1. åŠ è½½æ¨¡å‹ä¸æ•°æ®
    2. æ³¨å…¥å“¨å…µæ ‡ç­¾ï¼Œæ¿€æ´»å™ªå£°å¤æ´»æœºåˆ¶
    3. æ‰¹é‡æ¨ç†è·å– Evidence (Alpha)
    4. åŠ¨é‡æ›´æ–° Evidence Strength
    5. ä¸‰åŒºåˆ¶åˆ’åˆ† (Zone Splitting)
    6. è®¡ç®—åŠ æƒè´¨å¿ƒä¸è‡ªé€‚åº” Delta
    7. Zone-aware ä¿®æ­£ (æ ¸å¿ƒçº é”™)
    8. é¡¶åˆŠæŒ‡æ ‡ç»Ÿè®¡ (ç°‡æ•°é‡ã€å‡†ç¡®ç‡)
    9. Consensus è§£ç ä¸çŠ¶æ€ä¿å­˜
    """
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  Step2 å¯åŠ¨ | è®¾å¤‡: {device} | è½®æ¬¡: {args.round_idx}")

    # =====================================================================
    # 1. åŠ è½½ Step1 æ¨¡å‹ä¸æ•°æ®
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ“¦ åŠ è½½æ¨¡å‹ä¸æ•°æ®")
    print("=" * 60)

    # åŠ è½½ Checkpoint
    try:
        checkpoint = torch.load(args.step1_checkpoint, map_location=device)
        step1_args = checkpoint.get('args', {})
        # ä¼˜å…ˆä½¿ç”¨ checkpoint ä¸­çš„é…ç½®ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…
        model_dim = step1_args.get('dim', args.dim)
        model_max_len = step1_args.get('max_length', args.max_length)
        print(f"   âœ… æ¨¡å‹å‚æ•°å·²åŠ è½½: Dim={model_dim}, MaxLen={model_max_len}")
    except Exception as e:
        print(f"   âŒ Checkpoint åŠ è½½å¤±è´¥: {e}")
        return None

    # åŠ è½½æ•°æ® (ä¼ å…¥ä¸Šä¸€è½®çš„ refined_labels ä»¥ç»§æ‰¿çŠ¶æ€)
    try:
        labels_path = getattr(args, 'refined_labels', None)
        data_loader = CloverDataLoader(args.experiment_dir, labels_path=labels_path)
        TOTAL_READS = len(data_loader.reads)
        
        # ç¡®å®šç°‡æ•°é‡ (ç”¨äºåˆå§‹åŒ–åˆ†ç±»å¤´ï¼Œè™½ç„¶ Step2 ä¸ç”¨åˆ†ç±»å¤´ï¼Œä½†æ¨¡å‹ç»“æ„éœ€è¦å¯¹é½)
        current_clusters = set(data_loader.clover_labels)
        if -1 in current_clusters: current_clusters.remove(-1)
        if RESURRECTION_SENTINEL in current_clusters: current_clusters.remove(RESURRECTION_SENTINEL)
        num_clusters = max(50, len(current_clusters))
        
        print(f"   ğŸ“Š æ•°æ®ç»Ÿè®¡: {TOTAL_READS} Reads, {len(current_clusters)} åˆå§‹æœ‰æ•ˆç°‡")
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # åˆå§‹åŒ–æ¨¡å‹
    model = Step1EvidentialModel(
        dim=model_dim, 
        max_length=model_max_len, 
        num_clusters=num_clusters, 
        device=device
    ).to(device)
    
    # é¢„åŠ è½½ length_adapter (å¤„ç†ç»´åº¦ä¸åŒ¹é…é—®é¢˜)
    sd = checkpoint['model_state_dict']
    if 'length_adapter.weight' in sd:
        sh = sd['length_adapter.weight'].shape
        model.length_adapter = nn.Linear(sh[1], sh[0]).to(device)
    
    model.load_state_dict(sd, strict=False)
    model.eval()

    # =====================================================================
    # 2. å™ªå£°å¤æ´»é¢„å¤„ç† (Sentinel Injection)
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ”„ å™ªå£°å¤æ´»æ£€æµ‹")
    print("=" * 60)

    original_labels = list(data_loader.clover_labels) # å¤‡ä»½
    labels_np = np.array(original_labels)
    resurrection_mask = (labels_np == -1)
    n_resurrect = resurrection_mask.sum()

    if n_resurrect > 0:
        print(f"   ğŸ”™ å‘ç° {n_resurrect} æ¡å™ªå£° Readsï¼Œå°è¯•å¤æ´»...")
        # å°† -1 ä¿®æ”¹ä¸ºå“¨å…µå€¼ï¼Œä½¿å…¶é€šè¿‡ Step1Dataset çš„ label >= 0 è¿‡æ»¤
        indices_to_resurrect = np.where(resurrection_mask)[0]
        for idx in indices_to_resurrect:
            data_loader.clover_labels[idx] = RESURRECTION_SENTINEL
    else:
        print("   âœ… æ— å™ªå£° Reads éœ€è¦å¤æ´»")

    # =====================================================================
    # 3. æ‰¹é‡æ¨ç† (Inference)
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ”® å…¨é‡æ¨ç† (æå– Evidence)")
    print("=" * 60)

    dataset = Step1Dataset(data_loader, max_len=model_max_len)
    inference_loader = torch.utils.data.DataLoader(
        dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True
    )

    # å­˜å‚¨å®¹å™¨
    all_embeddings, all_strength, all_evidence, all_alpha = [], [], [], []
    all_u_epi, all_u_ale = [], []
    all_labels, all_indices = [], []

    for batch_idx, batch in enumerate(inference_loader):
        reads = batch['encoding'].to(device)
        lbls = batch['clover_label']
        idxs = batch['read_idx']

        # å¼ºåˆ¶ Padding å¯¹é½
        if reads.shape[1] != model_max_len:
            if reads.shape[1] < model_max_len:
                reads = F.pad(reads, (0, 0, 0, model_max_len - reads.shape[1]))
            else:
                reads = reads[:, :model_max_len, :]

        # Forward
        emb, pooled = model.encode_reads(reads)
        evid, stre, alph = model.decode_to_evidence(emb)
        epi, ale = decompose_uncertainty(alph) #

        all_embeddings.append(pooled.cpu())
        all_strength.append(stre.mean(dim=1).cpu())
        all_alpha.append(alph.cpu())
        all_evidence.append(evid.cpu())
        all_u_epi.append(epi.cpu())
        all_u_ale.append(ale.cpu())
        all_indices.append(idxs)
        
        if isinstance(lbls, torch.Tensor):
            all_labels.extend(lbls.tolist())
        else:
            all_labels.extend(lbls)
            
        if (batch_idx + 1) % 50 == 0:
            print(f"      å¤„ç†è¿›åº¦: {batch_idx + 1}/{len(inference_loader)}", end='\r')

    # æ‹¼æ¥å…¨é‡æ•°æ®
    embeddings = torch.cat(all_embeddings).to(device)
    strength = torch.cat(all_strength).to(device)
    alpha = torch.cat(all_alpha).to(device)
    evidence = torch.cat(all_evidence).to(device)
    u_epi = torch.cat(all_u_epi).to(device)
    u_ale = torch.cat(all_u_ale).to(device)
    labels = torch.tensor(all_labels, device=device)
    flat_real_indices = torch.cat(all_indices).numpy()

    # æ¢å¤æ•°æ®åŠ è½½å™¨çŠ¶æ€ (ç§»é™¤å“¨å…µ)
    data_loader.clover_labels = original_labels

    # è¯†åˆ«å“¨å…µä½ç½®
    sentinel_tensor_mask = (labels == RESURRECTION_SENTINEL)
    labels[sentinel_tensor_mask] = -1 # æ¢å¤ä¸º -1 ä»¥è¿›è¡Œæ­£å¸¸çš„è´¨å¿ƒè®¡ç®—è¿‡æ»¤

    print(f"\n   âœ… æ¨ç†å®Œæˆï¼Œå¤„ç†æ ·æœ¬æ•°: {len(labels)}")

    # =====================================================================
    # 4. åŠ¨é‡æ›´æ–° (Momentum Update)
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š åŠ¨é‡æ›´æ–° Strength")
    print("=" * 60)

    if getattr(args, 'prev_state', None) and os.path.exists(args.prev_state):
        try:
            prev_state = torch.load(args.prev_state, map_location='cpu')
            prev_str_full = prev_state['strength']
            # æ˜ å°„å›å½“å‰æ¨ç†çš„å­é›†
            prev_str_sub = torch.tensor(prev_str_full[flat_real_indices], device=device)
            
            strength = MOMENTUM_CURR * strength + MOMENTUM_PREV * prev_str_sub
            print(f"   âœ… åŠ¨é‡èåˆå®Œæˆ (Curr: {MOMENTUM_CURR}, Prev: {MOMENTUM_PREV})")
        except Exception as e:
            print(f"   âš ï¸ åŠ¨é‡æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨å½“å‰ Strength: {e}")
    else:
        print("   â„¹ï¸ æ— ä¸Šä¸€è½®çŠ¶æ€ï¼Œè·³è¿‡åŠ¨é‡æ›´æ–°")

    # =====================================================================
    # 5. ä¸‰åŒºåˆ¶åˆ’åˆ† & è´¨å¿ƒè®¡ç®—
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ” ä¸‰åŒºåˆ¶åˆ’åˆ†ä¸è´¨å¿ƒè®¡ç®—")
    print("=" * 60)

    # ä¸º Zone åˆ’åˆ†å‡†å¤‡æ ‡ç­¾ (å“¨å…µè®¾ä¸º 0 ä»¥å‚ä¸è®¡ç®—ï¼Œä½†ä¸å½±å“è´¨å¿ƒ)
    labels_for_zone = labels.clone()
    labels_for_zone[sentinel_tensor_mask] = 0 
    
    zone_ids, zone_stats = split_confidence_by_zone(u_epi, u_ale, labels_for_zone)
    
    # [æ ¸å¿ƒ] å¤æ´» Reads å¼ºåˆ¶è¿›å…¥ Zone II (Hard)ï¼Œå¿…é¡»ç»è¿‡è·ç¦»åˆ¤å†³æ‰èƒ½ç”Ÿå­˜
    zone_ids[sentinel_tensor_mask] = 2
    
    # è®¡ç®—è´¨å¿ƒ (ä»…ä½¿ç”¨éè´Ÿæ ‡ç­¾)
    centroids, cluster_sizes = compute_centroids_weighted(embeddings, labels, strength, zone_ids)
    
    # è®¡ç®—è‡ªé€‚åº” Delta
    delta = compute_global_delta(embeddings, labels, zone_ids, centroids)

    # =====================================================================
    # 6. Zone-aware ä¿®æ­£ (æ ¸å¿ƒçº é”™)
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ”„ Zone-aware èšç±»ä¿®æ­£")
    print("=" * 60)

    new_labels, noise_mask, refine_stats = refine_reads(
        embeddings, labels, zone_ids, centroids, delta, round_idx=args.round_idx
    )

    # =====================================================================
    # 7. é¡¶åˆŠæ•°æ®ç›‘æ§ (æ ¸å¿ƒï¼šè¿‡èšç±»çº é”™ä¸ Micro Accuracy)
    # =====================================================================
    print("\n" + "ğŸ“Š" * 20)
    print("ğŸ“ˆ SSI-EC é¡¶åˆŠæ•°æ®ç›‘æ§ (ERR036 éªŒè¯)")
    
    # A. ç°‡æ•°é‡æ¼”å˜
    initial_valid_mask = (labels >= 0)
    final_valid_mask = (new_labels >= 0)
    
    initial_clusters_cnt = len(torch.unique(labels[initial_valid_mask]))
    final_clusters_cnt = len(torch.unique(new_labels[final_valid_mask]))
    
    print(f"   ğŸ”¹ ç°‡æ•°é‡æ¼”å˜: {initial_clusters_cnt} -> {final_clusters_cnt}")
    print(f"      (Cloveråˆå§‹è¿‡èšç±» -> SSI-ECåˆå¹¶ä¿®æ­£ï¼Œç›®æ ‡å€¼: ~72000)")

    # B. å¤æ´»ç»Ÿè®¡
    resurrected_cnt = (sentinel_tensor_mask & (new_labels >= 0)).sum().item()
    print(f"   ğŸ”¹ å™ªå£°å¤æ´»æ•°: {resurrected_cnt} / {sentinel_tensor_mask.sum().item()}")

    # C. Micro Accuracy
    if hasattr(data_loader, 'gt_labels') and len(data_loader.gt_labels) > 0:
        # æ˜ å°„ GT åˆ°å½“å‰æ¨ç†æ ·æœ¬
        gt_full = np.array(data_loader.gt_labels)
        # ç¡®ä¿ç´¢å¼•ä¸è¿‡ç•Œ
        valid_map_mask = flat_real_indices < len(gt_full)
        
        if valid_map_mask.all():
            gt_subset = gt_full[flat_real_indices]
            pred_subset = new_labels.cpu().numpy()
            
            # åªè¯„ä¼°éå™ªå£°é¢„æµ‹çš„å‡†ç¡®æ€§
            eval_mask = (pred_subset >= 0)
            if eval_mask.any():
                correct = (gt_subset[eval_mask] == pred_subset[eval_mask]).sum()
                total_eval = eval_mask.sum()
                acc = correct / total_eval
                print(f"   ğŸ”¹ ä¿®æ­£å Micro Accuracy: {acc:.4%}")
                print(f"      (åŸºäº {total_eval} æ¡æœ‰æ•ˆé¢„æµ‹)")
            else:
                print("   âš ï¸ æ— æœ‰æ•ˆé¢„æµ‹ï¼Œæ— æ³•è®¡ç®— Accuracy")
        else:
            print("   âš ï¸ ç´¢å¼•è¶Šç•Œï¼Œæ— æ³•å¯¹é½ GT Labels")
    
    print("ğŸ“Š" * 20)

    # è°ƒç”¨ Visualizer ç”»ä¸ç¡®å®šæ€§åˆ†å¸ƒå›¾
    viz = Step1Visualizer(args.output_dir)
    viz.plot_uncertainty_distribution(u_epi, u_ale, zone_ids)

    # =====================================================================
    # 8. è§£ç ä¸ä¿å­˜
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ’¾ è§£ç ä¸çŠ¶æ€ä¿å­˜")
    print("=" * 60)

    # Consensus è§£ç 
    high_conf_mask = (zone_ids == 1)
    consensus_dict = decode_cluster_consensus(
        all_evidence, # ä½¿ç”¨ list ä»¥é¿å…ä¸å¿…è¦çš„ cat å¼€é”€ï¼Œå¦‚æœ decode æ”¯æŒ list
        alpha,        # è¿™é‡Œ alpha å·²ç»æ˜¯ cat è¿‡çš„ tensor
        new_labels, 
        strength, 
        high_conf_mask
    )
    
    # ä¿®æ­£ï¼šdecode_cluster_consensus å†…éƒ¨éœ€è¦ tensor ç±»å‹çš„ evidence
    # ä¸Šé¢å·²ç» cat æˆäº† evidence å˜é‡ï¼Œç›´æ¥ä¼  evidence
    consensus_dict = decode_cluster_consensus(
        evidence, 
        alpha, 
        new_labels, 
        strength, 
        high_conf_mask
    )

    save_consensus_sequences(consensus_dict, os.path.join(args.output_dir, "consensus_sequences.fasta"))

    # ä¿å­˜å…¨é•¿çŠ¶æ€ (ç”¨äºä¸‹ä¸€è½®)
    next_round_dir = os.path.join(args.experiment_dir, "04_Iterative_Labels")
    os.makedirs(next_round_dir, exist_ok=True)
    ts = datetime.now().strftime("%H%M%S")

    # ä¿å­˜ Labels
    full_labels = np.full(TOTAL_READS, -1, dtype=int)
    full_labels[flat_real_indices] = new_labels.cpu().numpy()
    label_path = os.path.join(next_round_dir, f"refined_labels_{ts}.txt")
    np.savetxt(label_path, full_labels, fmt='%d')

    # ä¿å­˜ State
    full_u_epi = np.zeros(TOTAL_READS, dtype=np.float32)
    full_u_ale = np.zeros(TOTAL_READS, dtype=np.float32)
    full_strength = np.zeros(TOTAL_READS, dtype=np.float32)
    full_zone_ids = np.zeros(TOTAL_READS, dtype=np.int64)

    full_u_epi[flat_real_indices] = u_epi.cpu().numpy()
    full_u_ale[flat_real_indices] = u_ale.cpu().numpy()
    full_strength[flat_real_indices] = strength.cpu().numpy()
    full_zone_ids[flat_real_indices] = zone_ids.cpu().numpy()

    state_path = os.path.join(next_round_dir, f"read_state_{ts}.pt")
    torch.save({
        'u_epi': full_u_epi,
        'u_ale': full_u_ale,
        'strength': full_strength,
        'zone_ids': full_zone_ids,
        'round_idx': args.round_idx
    }, state_path)

    print(f"   âœ… çŠ¶æ€å·²ä¿å­˜: {state_path}")

    return {
        'next_round_files': {
            'labels': label_path,
            'state': state_path,
            'reference': os.path.join(args.output_dir, "consensus_sequences.fasta")
        }
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step2: Refinement & Decoding')
    parser.add_argument('--experiment_dir',   type=str, required=True)
    parser.add_argument('--step1_checkpoint', type=str, required=True)
    parser.add_argument('--dim',              type=int, default=256)
    parser.add_argument('--max_length',       type=int, default=150)
    parser.add_argument('--device',           type=str, default='cuda')
    parser.add_argument('--refined_labels',   type=str, default=None)
    parser.add_argument('--prev_state',       type=str, default=None)
    parser.add_argument('--round_idx',        type=int, default=1)
    parser.add_argument('--output_dir',       type=str, default=f'./step2_results')

    args = parser.parse_args()
    try:
        run_step2(args)
    except Exception as e:
        print(f"âŒ Step2 è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)