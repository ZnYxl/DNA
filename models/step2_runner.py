# models/step2_runner.py
"""
Step2 ä¸»å…¥å£: Evidence-Guided Refinement & Decoding (Universal Edition)

ä¿®å¤æ¸…å•:
  [FIX-#1]  â˜…â˜…â˜… inference_mode=True â†’ Step2 æ¨ç†å…¨é‡æ•°æ® (15M ä¸æ˜¯ 2M)
  [FIX-MEM] æ¨ç†é˜¶æ®µä¸ç§¯ç´¯ evidence/alpha (çœ ~100GB)
            Consensus é˜¶æ®µæŒ‰ç°‡é‡æ–°æ¨ç†
  [FIX-OOM] æ¨ç†åç«‹å³é‡Šæ”¾ GPU, åç»­ refine å…¨åœ¨ CPU
  [FIX]     num_workers=0 é¿å… fork 15M reads å¡æ­»
  [FIX]     bare except â†’ except Exception
  [NEW]     GT è¯„ä¼° (id20 ç­‰æœ‰ GT çš„æ•°æ®é›†)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import argparse
import numpy as np
from datetime import datetime
from collections import defaultdict

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir  = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from models.step1_model  import Step1EvidentialModel, decompose_uncertainty
from models.step1_data   import CloverDataLoader, Step1Dataset
from models.step2_refine import (
    split_confidence_by_zone,
    compute_centroids_weighted,
    compute_global_delta,
    refine_reads
)
from models.step2_decode import (
    decode_cluster_consensus,
    save_consensus_sequences
)
from models.step1_visualizer import Step1Visualizer

# ---------------------------------------------------------------------------
# å…¨å±€è¶…å‚
# ---------------------------------------------------------------------------
MOMENTUM_CURR = 0.7
MOMENTUM_PREV = 0.3
RESURRECTION_SENTINEL = 999999


@torch.no_grad()
def run_step2(args):
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  Step2 å¯åŠ¨ | è®¾å¤‡: {device} | è½®æ¬¡: {args.round_idx}")

    os.makedirs(args.output_dir, exist_ok=True)

    # =====================================================================
    # 1. åŠ è½½æ¨¡å‹ä¸æ•°æ®
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ“¦ åŠ è½½æ¨¡å‹ä¸æ•°æ®")
    print("=" * 60)

    try:
        checkpoint = torch.load(args.step1_checkpoint, map_location=device)
        step1_args = checkpoint.get('args', {})
        model_dim = step1_args.get('dim', args.dim)
        model_max_len = step1_args.get('max_length', args.max_length)
        print(f"   âœ… æ¨¡å‹å‚æ•°: Dim={model_dim}, MaxLen={model_max_len}")
    except Exception as e:
        print(f"   âŒ Checkpoint åŠ è½½å¤±è´¥: {e}")
        return None

    try:
        labels_path = getattr(args, 'refined_labels', None)
        data_loader = CloverDataLoader(args.experiment_dir, labels_path=labels_path)
        TOTAL_READS = len(data_loader.reads)

        current_clusters = set(data_loader.clover_labels)
        if -1 in current_clusters:
            current_clusters.remove(-1)
        num_clusters = max(50, len(current_clusters))

        print(f"   ğŸ“Š æ•°æ®: {TOTAL_READS} Reads, {len(current_clusters)} æœ‰æ•ˆç°‡")
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # GT æ ‡ç­¾åŠ è½½ (å¯é€‰)
    gt_tags_file = getattr(args, 'gt_tags_file', None)
    if gt_tags_file and os.path.exists(gt_tags_file):
        data_loader.load_gt_tags(gt_tags_file)

    # æ¨¡å‹åˆå§‹åŒ–
    model = Step1EvidentialModel(
        dim=model_dim, max_length=model_max_len,
        num_clusters=num_clusters, device=device
    ).to(device)

    sd = checkpoint['model_state_dict']
    if 'length_adapter.weight' in sd:
        sh = sd['length_adapter.weight'].shape
        if sh[0] == model_max_len:
            model.length_adapter = nn.Linear(sh[1], sh[0]).to(device)
    model.load_state_dict(sd, strict=False)
    model.eval()

    # =====================================================================
    # 2. å™ªå£°å¤æ´»é¢„å¤„ç†
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ”„ å™ªå£°å¤æ´»æ£€æµ‹")
    print("=" * 60)

    original_labels = list(data_loader.clover_labels)
    labels_np = np.array(original_labels)
    resurrection_mask = (labels_np == -1)
    n_resurrect = resurrection_mask.sum()

    if n_resurrect > 0:
        print(f"   ğŸ”™ å°è¯•å¤æ´» {n_resurrect} æ¡å™ªå£° Reads...")
        for idx in np.where(resurrection_mask)[0]:
            data_loader.clover_labels[idx] = RESURRECTION_SENTINEL
    else:
        print("   âœ… æ— å™ªå£° Reads éœ€è¦å¤æ´»")

    # =====================================================================
    # 3. æ¨ç† (respect training_cap for testing)
    #    åªä¿ç•™æ ‡é‡: pooled_emb, strength, u_epi, u_ale
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ”® æ¨ç† (æå– Embeddings)")
    print("=" * 60)

    # åˆ¤æ–­æ˜¯å¦å…¨é‡æ¨ç†:
    #   training_cap >= æ€»æ•° â†’ å…¨é‡ (ç”Ÿäº§æ¨¡å¼)
    #   training_cap < æ€»æ•°  â†’ å—é™ (æµ‹è¯•æ¨¡å¼, ä¸ Step1 ä¸€è‡´)
    cap = getattr(args, 'training_cap', TOTAL_READS)
    use_full = (cap >= TOTAL_READS)

    if use_full:
        dataset = Step1Dataset(
            data_loader,
            max_len=model_max_len,
            inference_mode=True  # å…¨é‡æ¨ç†
        )
        print(f"   ğŸ”® å…¨é‡æ¨ç†: {TOTAL_READS} reads")
    else:
        dataset = Step1Dataset(
            data_loader,
            max_len=model_max_len,
            training_cap=cap,
            round_idx=args.round_idx,
            inference_mode=False
        )
        print(f"   ğŸ§ª æµ‹è¯•æ¨¡å¼: {cap} reads (ä¸ Step1 ä¸€è‡´)")

    inference_loader = torch.utils.data.DataLoader(
        dataset, batch_size=1024, shuffle=False,
        num_workers=0,       # â˜… é¿å… fork 15M reads å¡æ­»
        pin_memory=True
    )

    # â˜…â˜…â˜… é¢„åˆ†é…å¼ é‡, é¿å… list + torch.cat åŒå€å†…å­˜å³°å€¼
    N = len(dataset)
    D = step1_args.get('dim', args.dim)
    print(f"   ğŸ“¦ é¢„åˆ†é…: {N} samples Ã— {D} dim", flush=True)

    embeddings       = torch.zeros(N, D)
    strength         = torch.zeros(N)
    u_epi            = torch.zeros(N)
    u_ale            = torch.zeros(N)
    labels           = torch.zeros(N, dtype=torch.long)
    flat_real_indices = torch.zeros(N, dtype=torch.long)

    offset = 0
    total_batches = len(inference_loader)
    for batch_idx, batch in enumerate(inference_loader):
        reads = batch['encoding'].to(device)
        lbls  = batch['clover_label']
        idxs  = batch['read_idx']
        B = reads.shape[0]

        # Padding / Truncation
        if reads.shape[1] != model_max_len:
            if reads.shape[1] < model_max_len:
                reads = F.pad(reads, (0, 0, 0, model_max_len - reads.shape[1]))
            else:
                reads = reads[:, :model_max_len, :]

        emb, pooled = model.encode_reads(reads)
        evid, stre, alph = model.decode_to_evidence(emb)
        epi, ale = decompose_uncertainty(alph)

        # ç›´æ¥å†™å…¥é¢„åˆ†é…å¼ é‡ (é›¶æ‹·è´, æ—  list ç§¯ç´¯)
        embeddings[offset:offset+B] = pooled.cpu()
        strength[offset:offset+B]   = stre.mean(dim=1).cpu()
        u_epi[offset:offset+B]      = epi.cpu()
        u_ale[offset:offset+B]      = ale.cpu()
        flat_real_indices[offset:offset+B] = idxs

        if isinstance(lbls, torch.Tensor):
            labels[offset:offset+B] = lbls
        else:
            labels[offset:offset+B] = torch.tensor(lbls)

        offset += B

        if (batch_idx + 1) % 500 == 0 or (batch_idx + 1) == total_batches:
            print(f"      è¿›åº¦: {batch_idx + 1}/{total_batches}", flush=True)

    # æˆªæ–­ (ä»¥é˜²æœ€åä¸è¶³)
    embeddings       = embeddings[:offset]
    strength         = strength[:offset]
    u_epi            = u_epi[:offset]
    u_ale            = u_ale[:offset]
    labels           = labels[:offset]
    flat_real_indices = flat_real_indices[:offset].numpy()

    print(f"   âœ… æ¨ç†å†™å…¥å®Œæˆ: {offset} samples, embeddings {embeddings.shape}", flush=True)

    # â˜…â˜…â˜… åŸåœ°å½’ä¸€åŒ– (é¿å…åç»­å‡½æ•°åå¤æ‹·è´, çœ 4.1GB)
    print(f"   ğŸ”„ åŸåœ°å½’ä¸€åŒ– embeddings...", flush=True)
    norms = embeddings.norm(dim=-1, keepdim=True).clamp(min=1e-8)
    embeddings.div_(norms)
    del norms
    print(f"   âœ… å½’ä¸€åŒ–å®Œæˆ (in-place, é›¶é¢å¤–å†…å­˜)", flush=True)

    # â˜…â˜…â˜… é‡Šæ”¾ GPU æ˜¾å­˜ â€” åç»­ refine å…¨åœ¨ CPU
    print(f"   ğŸ—‘ï¸ é‡Šæ”¾ GPU æ˜¾å­˜...", flush=True)
    del model
    torch.cuda.empty_cache()
    print(f"   âœ… GPU æ˜¾å­˜å·²é‡Šæ”¾", flush=True)

    # æ¢å¤ Loader çŠ¶æ€
    data_loader.clover_labels = original_labels

    # è¯†åˆ«å“¨å…µ
    sentinel_tensor_mask = (labels == RESURRECTION_SENTINEL)
    labels[sentinel_tensor_mask] = -1

    print(f"\n   âœ… æ¨ç†å®Œæˆï¼Œæ ·æœ¬æ•°: {len(labels)}")

    # =====================================================================
    # 4. åŠ¨é‡æ›´æ–°
    # =====================================================================
    if getattr(args, 'prev_state', None) and os.path.exists(args.prev_state):
        try:
            print(f"   ğŸ“Š åŠ¨é‡æ›´æ–°...")
            prev_state = torch.load(args.prev_state, map_location='cpu')
            prev_str_full = prev_state['strength']
            if len(prev_str_full) >= flat_real_indices.max() + 1:
                prev_str_sub = torch.tensor(
                    prev_str_full[flat_real_indices], dtype=torch.float32
                )
                strength = MOMENTUM_CURR * strength + MOMENTUM_PREV * prev_str_sub
                print(f"   âœ… åŠ¨é‡æ›´æ–°å®Œæˆ")
            else:
                print(f"   âš ï¸ prev_state é•¿åº¦ä¸åŒ¹é…, è·³è¿‡")
        except Exception as e:
            print(f"   âš ï¸ åŠ¨é‡æ›´æ–°è·³è¿‡: {e}")

    # =====================================================================
    # 5. ä¸‰åŒºåˆ¶åˆ’åˆ† & è´¨å¿ƒ & Delta
    #    æ³¨æ„: embeddings/labels æ­¤æ—¶å·²åœ¨ CPU
    #    refine å‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨æ¬ CPU, æ‰€ä»¥ç›´æ¥ä¼ å³å¯
    # =====================================================================
    labels_for_zone = labels.clone()
    labels_for_zone[sentinel_tensor_mask] = 0

    zone_ids, zone_stats = split_confidence_by_zone(u_epi, u_ale, labels_for_zone)
    zone_ids[sentinel_tensor_mask] = 2  # å¤æ´»è¯»æ®µå¼ºåˆ¶è¿› Zone II

    centroids, _ = compute_centroids_weighted(embeddings, labels, strength, zone_ids)
    delta = compute_global_delta(embeddings, labels, zone_ids, centroids)

    # =====================================================================
    # 6. Zone-aware ä¿®æ­£
    # =====================================================================
    new_labels, noise_mask, refine_stats = refine_reads(
        embeddings, labels, zone_ids, centroids, delta, round_idx=args.round_idx
    )

    # =====================================================================
    # 7. Consensus è§£ç  (æŒ‰ç°‡é‡æ–°æ¨ç† evidence)
    # =====================================================================
    print("\n" + "=" * 60)
    print("ğŸ§¬ Consensus è§£ç  (æŒ‰ç°‡é‡æ–°æ¨ç†)")
    print("=" * 60)

    # â˜… é‡Šæ”¾æœ€å¤§çš„å¼ é‡ (embeddings ~2GB + centroids), consensus ä¸éœ€è¦å®ƒä»¬
    del embeddings, centroids
    import gc; gc.collect()
    print("   ğŸ—‘ï¸ å·²é‡Šæ”¾ embeddings/centroids, å›æ”¶å†…å­˜", flush=True)

    # é‡æ–°åŠ è½½æ¨¡å‹åˆ° GPU åš consensus
    device_consensus = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    model_consensus = Step1EvidentialModel(
        dim=step1_args.get('dim', args.dim),
        max_length=model_max_len,
        num_clusters=num_clusters,
        device=device_consensus
    ).to(device_consensus)

    sd = checkpoint['model_state_dict']
    if 'length_adapter.weight' in sd:
        sh = sd['length_adapter.weight'].shape
        if sh[0] == model_max_len:
            model_consensus.length_adapter = nn.Linear(sh[1], sh[0]).to(device_consensus)
    model_consensus.load_state_dict(sd, strict=False)
    model_consensus.eval()

    consensus_dict = _consensus_with_reinference(
        model_consensus, dataset, new_labels, zone_ids, flat_real_indices,
        model_max_len, device_consensus
    )

    del model_consensus
    torch.cuda.empty_cache()

    fasta_path = os.path.join(args.output_dir, "consensus_sequences.fasta")
    save_consensus_sequences(consensus_dict, fasta_path)

    # =====================================================================
    # 7b. GT è¯„ä¼° (å¯é€‰)
    # =====================================================================
    if gt_tags_file and os.path.exists(gt_tags_file):
        _evaluate_with_gt(consensus_dict, data_loader, new_labels,
                          flat_real_indices, args.output_dir)

    # =====================================================================
    # 8. ä¿å­˜å…¨é•¿çŠ¶æ€
    # =====================================================================
    next_round_dir = os.path.join(args.experiment_dir, "04_Iterative_Labels")
    os.makedirs(next_round_dir, exist_ok=True)
    ts = datetime.now().strftime("%H%M%S")

    full_labels = np.full(TOTAL_READS, -1, dtype=int)
    full_labels[flat_real_indices] = new_labels.cpu().numpy()
    label_path = os.path.join(next_round_dir, f"refined_labels_{ts}.txt")
    np.savetxt(label_path, full_labels, fmt='%d')

    full_u_epi    = np.zeros(TOTAL_READS, dtype=np.float32)
    full_u_ale    = np.zeros(TOTAL_READS, dtype=np.float32)
    full_strength = np.zeros(TOTAL_READS, dtype=np.float32)
    full_zone_ids = np.zeros(TOTAL_READS, dtype=np.int64)

    full_u_epi[flat_real_indices]    = u_epi.cpu().numpy()
    full_u_ale[flat_real_indices]    = u_ale.cpu().numpy()
    full_strength[flat_real_indices] = strength.cpu().numpy()
    full_zone_ids[flat_real_indices] = zone_ids.cpu().numpy()

    state_path = os.path.join(next_round_dir, f"read_state_{ts}.pt")
    torch.save({
        'u_epi': full_u_epi, 'u_ale': full_u_ale,
        'strength': full_strength, 'zone_ids': full_zone_ids,
        'round_idx': args.round_idx
    }, state_path)

    # =====================================================================
    # 9. è®ºæ–‡æ•°æ®åŸ‹ç‚¹
    # =====================================================================
    _record_paper_log(args, TOTAL_READS, refine_stats, consensus_dict,
                      sentinel_tensor_mask, new_labels, strength, delta)

    # å¯è§†åŒ–
    try:
        viz = Step1Visualizer(args.output_dir)
        viz.plot_uncertainty_distribution(u_epi, u_ale, zone_ids)
    except Exception as e:
        print(f"   âš ï¸ å¯è§†åŒ–è·³è¿‡: {e}")

    return {
        'next_round_files': {
            'labels': label_path,
            'state': state_path,
            'reference': fasta_path
        }
    }


# ===========================================================================
# æŒ‰ç°‡é‡æ–°æ¨ç† evidence åš consensus (çœå†…å­˜)
# ===========================================================================
def _consensus_with_reinference(model, dataset, new_labels, zone_ids,
                                flat_real_indices, max_len, device):
    from models.step1_data import seq_to_onehot

    # å»ºç«‹ ç°‡ID â†’ dataset å†…éƒ¨ç´¢å¼•
    cluster_to_didx = defaultdict(list)
    labels_np = new_labels.cpu().numpy()

    for didx in range(len(dataset)):
        real_idx = dataset.valid_indices[didx]
        # labels_np çš„ç´¢å¼•æ˜¯ dataset å†…éƒ¨ç´¢å¼•
        label = int(labels_np[didx]) if didx < len(labels_np) else -1
        if label >= 0:
            cluster_to_didx[label].append(didx)

    zone_np = zone_ids.cpu().numpy()
    consensus_dict = {}
    base_map = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}

    processed = 0
    total_clusters = len(cluster_to_didx)
    CONSENSUS_BATCH = 512  # å¤§ç°‡åˆ†æ‰¹æ¨ç†, é˜² OOM

    for label, didx_list in cluster_to_didx.items():
        if len(didx_list) < 2:
            continue

        # æ”¶é›†è¿™ä¸ªç°‡çš„ reads
        reads_list = []
        hc_flags = []
        for didx in didx_list:
            real_idx = dataset.valid_indices[didx]
            seq = dataset.data_loader.reads[real_idx]
            reads_list.append(seq_to_onehot(seq, max_len))
            hc_flags.append(zone_np[didx] == 1)

        hc_mask = torch.tensor(hc_flags)
        count = len(didx_list)

        # â˜… åˆ†æ‰¹æ¨ç† (é˜²æ­¢å¤§ç°‡ OOM)
        all_alph = []
        all_stre = []
        for bi in range(0, count, CONSENSUS_BATCH):
            be = min(bi + CONSENSUS_BATCH, count)
            batch_tensor = torch.stack(reads_list[bi:be]).to(device)
            with torch.no_grad():
                emb, pooled = model.encode_reads(batch_tensor)
                evid, stre, alph = model.decode_to_evidence(emb)
            all_alph.append(alph.cpu())
            all_stre.append(stre.mean(dim=1).cpu())
            del emb, pooled, evid, stre, alph, batch_tensor

        alph = torch.cat(all_alph)
        strength_seq = torch.cat(all_stre)

        high_conf_count = int(hc_mask.sum().item())

        if high_conf_count == 0:
            continue

        if high_conf_count >= 2:
            fused_alpha = alph[hc_mask].mean(dim=0)
        else:
            conf_weights = torch.where(hc_mask, 2.0, 0.5)
            str_weights  = F.softmax(strength_seq, dim=0)
            combined     = conf_weights * str_weights
            combined     = (combined / combined.sum()).view(-1, 1, 1)
            fused_alpha  = torch.sum(alph * combined, dim=0)

        consensus_prob = fused_alpha / fused_alpha.sum(dim=-1, keepdim=True)
        consensus_indices = torch.argmax(consensus_prob, dim=-1)
        consensus_seq = ''.join([base_map[idx.item()] for idx in consensus_indices])

        consensus_dict[int(label)] = {
            'consensus_prob': consensus_prob.cpu(),
            'consensus_seq': consensus_seq,
            'num_reads': count,
            'num_high_conf': high_conf_count,
            'avg_strength': strength_seq.mean().item()
        }

        processed += 1
        if processed % 5000 == 0:
            torch.cuda.empty_cache()  # å®šæœŸæ¸…ç†ç¢ç‰‡
            print(f"      Consensus: {processed}/{total_clusters}", flush=True)

    print(f"\n   ğŸ§¬ å…±è¯†åºåˆ—: {len(consensus_dict)} ä¸ª")
    return consensus_dict


# ===========================================================================
# GT è¯„ä¼°
# ===========================================================================
def _evaluate_with_gt(consensus_dict, data_loader, new_labels,
                      flat_real_indices, output_dir):
    print("\n" + "=" * 60)
    print("ğŸ“‹ GT è¯„ä¼°")
    print("=" * 60)

    from collections import Counter
    labels_np = new_labels.cpu().numpy()
    gt_labels = data_loader.gt_labels

    cluster_gt_counts = defaultdict(Counter)
    total_assigned = 0

    for i, didx_real in enumerate(flat_real_indices):
        label = int(labels_np[i])
        if label < 0:
            continue
        gt = gt_labels[didx_real]
        if gt >= 0:
            cluster_gt_counts[label][gt] += 1
            total_assigned += 1

    if total_assigned == 0:
        print("   âš ï¸ æ— åŒ¹é… GT æ ‡ç­¾, è·³è¿‡")
        return

    correct = 0
    recovered_tags = set()
    purities = []

    for cid, counts in cluster_gt_counts.items():
        dominant_tag, dominant_count = counts.most_common(1)[0]
        total_in_cluster = sum(counts.values())
        purity = dominant_count / total_in_cluster
        purities.append(purity)
        correct += dominant_count
        recovered_tags.add(dominant_tag)

    unique_gt_tags = set(gt for gt in gt_labels if gt >= 0)
    n_unique = len(unique_gt_tags)

    avg_purity = sum(purities) / len(purities) if purities else 0
    micro_acc = correct / total_assigned if total_assigned else 0
    recovery = len(recovered_tags) / n_unique if n_unique else 0

    print(f"   ğŸ·ï¸ GT Tags: {n_unique}")
    print(f"   ğŸ“Š Recovery:    {len(recovered_tags)}/{n_unique} ({recovery*100:.2f}%)")
    print(f"   ğŸ“Š Purity:      {avg_purity*100:.2f}%")
    print(f"   ğŸ“Š Micro Acc:   {micro_acc*100:.2f}%")
    print(f"   ğŸ“Š Clustered:   {total_assigned}")
    print(f"   ğŸ“Š Clusters:    {len(cluster_gt_counts)}")

    report_path = os.path.join(output_dir, "gt_evaluation.txt")
    with open(report_path, 'w') as f:
        f.write(f"Recovery: {len(recovered_tags)}/{n_unique} ({recovery*100:.2f}%)\n")
        f.write(f"Purity: {avg_purity*100:.2f}%\n")
        f.write(f"Micro Acc: {micro_acc*100:.2f}%\n")
        f.write(f"Clustered Reads: {total_assigned}\n")
        f.write(f"Clusters: {len(cluster_gt_counts)}\n")
    print(f"   ğŸ’¾ æŠ¥å‘Š: {report_path}")


# ===========================================================================
# è®ºæ–‡æ•°æ®åŸ‹ç‚¹
# ===========================================================================
def _record_paper_log(args, total_reads, refine_stats, consensus_dict,
                      sentinel_mask, new_labels, strength, delta):
    print("\nğŸ“ è®°å½•å®éªŒæ•°æ®...")

    log_file = os.path.join(args.experiment_dir, "experiment_log.csv")
    file_exists = os.path.exists(log_file)

    try:
        with open(log_file, 'a') as f:
            if not file_exists:
                f.write("Round,Total_Reads,Zone1_Safe,Zone2_Reassigned,Zone3_Dirty,"
                        "Resurrected,Final_Clusters,Avg_Strength,Avg_HC_Ratio,Delta\n")

            z1 = refine_stats.get('zone1_kept', 0)
            z2_fix = refine_stats.get('zone2_reassigned', 0)
            z3 = refine_stats.get('zone3_dirty', 0)
            resurrect_cnt = int((sentinel_mask & (new_labels >= 0)).sum().item())
            final_clusters = len(consensus_dict)
            avg_s = strength.mean().item()

            hc_ratios = [c['num_high_conf'] / c['num_reads']
                         for c in consensus_dict.values() if c['num_reads'] > 0]
            avg_hc = sum(hc_ratios) / len(hc_ratios) if hc_ratios else 0.0

            f.write(f"{args.round_idx},{total_reads},{z1},{z2_fix},{z3},"
                    f"{resurrect_cnt},{final_clusters},{avg_s:.4f},{avg_hc:.4f},{delta:.4f}\n")

        print(f"   âœ… æ•°æ®: {log_file}")
    except Exception as e:
        print(f"   âš ï¸ å†™æ—¥å¿—å¤±è´¥: {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step2: Refinement & Decoding')
    parser.add_argument('--experiment_dir',   type=str, required=True)
    parser.add_argument('--step1_checkpoint', type=str, required=True)
    parser.add_argument('--dim',              type=int, default=256)
    parser.add_argument('--max_length',       type=int, default=150)
    parser.add_argument('--device',           type=str, default='cuda')
    parser.add_argument('--refined_labels',   type=str, default=None)
    parser.add_argument('--prev_state',       type=str, default=None)
    parser.add_argument('--round_idx',        type=int, default=1)
    parser.add_argument('--output_dir',       type=str, default='./step2_results')
    parser.add_argument('--gt_tags_file',     type=str, default=None)
    parser.add_argument('--gt_refs_file',     type=str, default=None)
    parser.add_argument('--training_cap',     type=int, default=2000000)

    args = parser.parse_args()
    try:
        run_step2(args)
    except Exception as e:
        print(f"âŒ Step2 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)