# models/step2_runner.py
"""
Step2 ä¸»å…¥å£ï¼šEvidence-Guided Refinement & Decoding
å…³é”®ï¼šä¸è®­ç»ƒï¼Œåªæ¨ç†+å†³ç­–
âœ… ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ï¼šç°‡å†…æ¯”è¾ƒï¼Œåå‘ç¡®å®šæ€§
âœ… ä¿®å¤ç‰ˆ v2ï¼š
   1. åŒ…å« length_adapter æƒé‡é¢„åŠ è½½ä¿®å¤
   2. åŒ…å« è¿­ä»£æ¥å£ (next_round_files)
   3. ğŸ”¥ æ–°å¢ï¼šå¼ºåˆ¶ Padding åˆ° max_lengthï¼Œé¿å¼€æœªè®­ç»ƒçš„ adapter
"""
import torch
import torch.nn as nn
import torch.nn.functional as F  # âœ… éœ€è¦ç”¨åˆ° F.pad
import os
import sys
import argparse
import numpy as np
from datetime import datetime

# âœ… æ·»åŠ è·¯å¾„å¤„ç†ï¼ˆä¸step1_train.pyç›¸åŒï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# ç°åœ¨å¯ä»¥æ­£å¸¸å¯¼å…¥
from models.step1_model import Step1EvidentialModel, load_pretrained_feddna
from models.step1_data import CloverDataLoader, Step1Dataset
from models.step2_refine import (
    split_confidence_by_percentile,
    compute_cluster_centroids,
    refine_low_confidence_reads,
    compute_adaptive_delta
)
from models.step2_decode import (
    decode_cluster_consensus,
    save_consensus_sequences,
    compute_consensus_quality_metrics
)


@torch.no_grad()
def run_step2(args):
    """
    Step2ä¸»æµç¨‹ï¼š
    1. åŠ è½½Step1æ¨¡å‹ï¼ˆfreezeï¼‰
    2. æ¨ç†å¾—åˆ°embeddings + evidence
    3. ç›¸å¯¹è¯æ®ç­›é€‰ï¼ˆç°‡å†…æ¯”è¾ƒï¼‰
    4. ç°‡ä¿®æ­£ï¼ˆåªä¿®æ­£ä½ç½®ä¿¡åº¦ï¼‰
    5. åå‘ç¡®å®šæ€§çš„consensusè§£ç 
    6. å‡†å¤‡ä¸‹ä¸€è½®è¿­ä»£æ•°æ®
    """
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # ========== 1ï¸âƒ£ åŠ è½½Step1æ¨¡å‹ ==========
    print("\n" + "=" * 60)
    print("ğŸ“¦ åŠ è½½Step1è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("=" * 60)

    try:
        checkpoint = torch.load(args.step1_checkpoint, map_location=device)
        print(f"   âœ… checkpointåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ checkpointåŠ è½½å¤±è´¥: {e}")
        return None

    # âœ… ä½¿ç”¨Step1è®­ç»ƒæ—¶ä¿å­˜çš„å‚æ•°
    if 'args' in checkpoint:
        step1_args = checkpoint['args']
        print(f"   ğŸ“‹ Step1è®­ç»ƒå‚æ•°:")
        print(f"      dim: {step1_args.get('dim', args.dim)}")
        print(f"      max_length: {step1_args.get('max_length', args.max_length)}")

        # ä½¿ç”¨Step1çš„å‚æ•°
        model_dim = step1_args.get('dim', args.dim)
        model_max_length = step1_args.get('max_length', args.max_length)
    else:
        print(f"   âš ï¸ checkpointä¸­æ²¡æœ‰ä¿å­˜argsï¼Œä½¿ç”¨å½“å‰å‚æ•°")
        model_dim = args.dim
        model_max_length = args.max_length

    # é‡å»ºæ•°æ®åŠ è½½å™¨
    try:
        data_loader = CloverDataLoader(args.experiment_dir)
        num_clusters = len(set(data_loader.clover_labels))
        print(f"   ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(data_loader.reads)} reads, {num_clusters} ç°‡")
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

    # âœ… ä½¿ç”¨Step1ç›¸åŒçš„å‚æ•°é‡å»ºæ¨¡å‹
    try:
        model = Step1EvidentialModel(
            dim=model_dim,
            max_length=model_max_length,
            num_clusters=num_clusters,
            device=device
        ).to(device)
        print(f"   âœ… æ¨¡å‹ç»“æ„åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

    # ================= ğŸ”´ æ ¸å¿ƒä¿®å¤ 1: æƒé‡é¢„åŠ è½½ ğŸ”´ =================
    # æ‰‹åŠ¨æ£€æŸ¥å¹¶åˆå§‹åŒ– length_adapterï¼Œé˜²æ­¢æƒé‡åŠ è½½è¢«è·³è¿‡
    try:
        state_dict = checkpoint['model_state_dict']
        if 'length_adapter.weight' in state_dict:
            print(f"   ğŸ”§ æ£€æµ‹åˆ° checkpoint åŒ…å« length_adapterï¼Œæ­£åœ¨é¢„åˆå§‹åŒ–...")
            weight_shape = state_dict['length_adapter.weight'].shape
            in_features = weight_shape[1]
            out_features = weight_shape[0]
            
            # æ‰‹åŠ¨åˆå§‹åŒ–å±‚
            model.length_adapter = nn.Linear(in_features, out_features).to(device)
            print(f"      å·²åˆå§‹åŒ–: Linear({in_features} -> {out_features})")
    except Exception as e:
        print(f"   âš ï¸ é¢„åˆå§‹åŒ– length_adapter æ—¶å‡ºé”™ (éè‡´å‘½): {e}")

    # âœ… å°è¯•åŠ è½½
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=True)
        print(f"   âœ… æ¨¡å‹å®Œå…¨åŒ¹é…åŠ è½½")
    except RuntimeError as e:
        print(f"   âš ï¸ æ¨¡å‹ç»“æ„ä¸å®Œå…¨åŒ¹é…: {e}")
        # ... (çœç•¥ä¹‹å‰çš„è¿‡æ»¤åŠ è½½ä»£ç ï¼Œä¿æŒç®€æ´ï¼Œé€»è¾‘ä¸€æ ·) ...
        # å¦‚æœéœ€è¦å®Œæ•´çš„è¿‡æ»¤ä»£ç ï¼Œå¯ä»¥ä¿ç•™ä¹‹å‰çš„å†™æ³•ï¼Œè¿™é‡Œç®€åŒ–å±•ç¤ºæ ¸å¿ƒé€»è¾‘
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print(f"   âœ… å°è¯•å¼ºåˆ¶åŠ è½½ (strict=False)")

    model.eval()
    print(f"   ğŸ“Š æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")

    # ========== 2ï¸âƒ£ æ¨ç†å…¨éƒ¨æ•°æ® (æ‰¹é‡ä¼˜åŒ–ç‰ˆ) ==========
    print("\n" + "=" * 60)
    print("ğŸ”® Step1æ¨¡å‹æ¨ç†ï¼ˆæå–embeddings + evidenceï¼‰")
    print("=" * 60)

    try:
        dataset = Step1Dataset(data_loader, max_len=model_max_length)
        # âœ… ä½¿ç”¨ DataLoader è¿›è¡Œæ‰¹é‡æ¨ç†
        # num_workers=4 å¯ä»¥åˆ©ç”¨ä½ çš„å¤šæ ¸CPUåŠ é€Ÿæ•°æ®åŠ è½½
        inference_loader = torch.utils.data.DataLoader(
            dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True
        )
        print(f"   ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} | Batch Size: 1024")
    except Exception as e:
        print(f"   âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return None

    all_embeddings = []
    all_strength = []
    all_alpha = []
    all_evidence = []
    all_labels = []
    
    # åªéœ€è¦å­˜ read_idx ç”¨äºåç»­å¯¹é½ï¼Œæˆ–è€…ç›´æ¥æŒ‰é¡ºåº
    print(f"   ğŸ”„ å¼€å§‹æ‰¹é‡æ¨ç†...")

    model.eval()
    with torch.no_grad():
        for batch_idx, batch_data in enumerate(inference_loader):
            # è·å–æ•°æ®
            reads = batch_data['encoding'].to(device) # (B, L, 4)
            labels = batch_data['clover_label']       # list or tensor
            
            # ================= å¼ºåˆ¶ Padding (ä½ çš„æ ¸å¿ƒä¿®å¤) =================
            curr_len = reads.shape[1]
            target_len = model_max_length
            if curr_len > target_len:
                reads = reads[:, :target_len, :]
            elif curr_len < target_len:
                pad_len = target_len - curr_len
                reads = F.pad(reads, (0, 0, 0, pad_len), "constant", 0)
            # ================================================================

            # æ‰¹é‡æ¨ç†
            embeddings, pooled_emb = model.encode_reads(reads)
            evidence, strength, alpha = model.decode_to_evidence(embeddings)

            # æ”¶é›†ç»“æœ (è½¬åˆ°CPUä»¥èŠ‚çœæ˜¾å­˜)
            all_embeddings.append(pooled_emb.cpu())
            all_strength.append(strength.mean(dim=1).cpu())
            all_alpha.append(alpha.cpu())
            all_evidence.append(evidence.cpu())
            
            # å¤„ç† labels (å¦‚æœæ˜¯tensorè½¬listï¼Œæˆ–è€…ç›´æ¥extend)
            if isinstance(labels, torch.Tensor):
                all_labels.extend(labels.tolist())
            else:
                all_labels.extend(labels)

            if (batch_idx + 1) % 100 == 0:
                print(f"      å·²å¤„ç† Batch: {batch_idx + 1}/{len(inference_loader)}")

    if len(all_embeddings) == 0:
        print(f"   âŒ æ²¡æœ‰æˆåŠŸæ¨ç†çš„readsï¼")
        return None

    # æ‹¼æ¥ç»“æœ
    try:
        embeddings = torch.cat(all_embeddings, dim=0).to(device)
        strength = torch.cat(all_strength, dim=0).to(device)
        alpha = torch.cat(all_alpha, dim=0).to(device)
        evidence = torch.cat(all_evidence, dim=0).to(device)
        labels = torch.tensor(all_labels, device=device)

        print(f"   âœ… æ¨ç†å®Œæˆ:")
        print(f"      Total: {len(labels)} reads")
        print(f"      Embeddings: {embeddings.shape}")
    except Exception as e:
        print(f"   âŒ æ‹¼æ¥å¼ é‡å¤±è´¥ (å¯èƒ½æ˜¾å­˜ä¸è¶³): {e}")
        return None
    # ========== 3ï¸âƒ£ Phase A: ç›¸å¯¹è¯æ®ç­›é€‰ ==========
    print("\n" + "=" * 60)
    print("ğŸ” Phase A: ç›¸å¯¹è¯æ®ç­›é€‰ï¼ˆç°‡å†…æ¯”è¾ƒï¼‰")
    print("=" * 60)

    try:
        low_conf_mask, conf_stats = split_confidence_by_percentile(
            strength, labels, p=args.uncertainty_percentile
        )
        high_conf_mask = ~low_conf_mask

    except Exception as e:
        print(f"   âŒ ç›¸å¯¹è¯æ®ç­›é€‰å¤±è´¥: {e}")
        return None

    # ========== 4ï¸âƒ£ Phase B: ç°‡ä¿®æ­£ ==========
    print("\n" + "=" * 60)
    print("ğŸ”„ Phase B: ç°‡ä¿®æ­£ï¼ˆåªä¿®æ­£ä½ç½®ä¿¡åº¦readsï¼‰")
    print("=" * 60)

    try:
        centroids, cluster_sizes = compute_cluster_centroids(
            embeddings, labels, high_conf_mask
        )

        if args.delta is None:
            delta = compute_adaptive_delta(
                embeddings, centroids, percentile=args.delta_percentile
            )
        else:
            delta = args.delta
            print(f"   ğŸ¯ ä½¿ç”¨å›ºå®šdelta: {delta:.4f}")

        new_labels, noise_mask, refine_stats = refine_low_confidence_reads(
            embeddings, labels, low_conf_mask, centroids, delta
        )

    except Exception as e:
        print(f"   âŒ ç°‡ä¿®æ­£å¤±è´¥: {e}")
        return None

    # ========== 5ï¸âƒ£ Phase C: åå‘ç¡®å®šæ€§çš„Consensusè§£ç  ==========
    print("\n" + "=" * 60)
    print("ğŸ§¬ Phase C: åå‘ç¡®å®šæ€§çš„Consensusè§£ç ")
    print("=" * 60)

    try:
        consensus_dict = decode_cluster_consensus(
            evidence, alpha, new_labels, strength, high_conf_mask
        )

        os.makedirs(args.output_dir, exist_ok=True)
        consensus_path = os.path.join(args.output_dir, "consensus_sequences.fasta")
        save_consensus_sequences(consensus_dict, consensus_path)

    except Exception as e:
        print(f"   âŒ Consensusè§£ç å¤±è´¥: {e}")
        return None

    # ========== 6ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š ==========
    print("\n" + "=" * 60)
    print("ğŸ“Š Step2 æœ€ç»ˆç»Ÿè®¡")
    print("=" * 60)

    print(f"\n   ğŸ“ˆ ç°‡ä¿®æ­£æ•ˆæœ:")
    print(f"      åŸå§‹ç°‡æ•°: {len(torch.unique(labels))}")
    print(f"      ä¿®æ­£åç°‡æ•°: {len(consensus_dict)}")
    print(f"      æ€»å™ªå£°reads: {noise_mask.sum()}/{len(labels)} ({noise_mask.float().mean() * 100:.1f}%)")
    print(f"      é‡æ–°åˆ†é…: {refine_stats['reassigned']}")
    print(f"      æ–°å¢å™ªå£°: {refine_stats['marked_noise']}")

    print(f"\n   ğŸ§¬ Consensusè´¨é‡:")
    for label in sorted(list(consensus_dict.keys())[:5]):
        info = consensus_dict[label]
        print(f"      ç°‡{label}: {info['num_reads']} reads ({info['num_high_conf']} é«˜ç½®ä¿¡åº¦), "
              f"strength={info['avg_strength']:.3f}, "
              f"len={len(info['consensus_seq'])}")

    # ========== 7ï¸âƒ£ å‡†å¤‡ä¸‹ä¸€è½®è¿­ä»£çš„æ•°æ® (ç¼åˆæ¥å£) ==========
    print("\n" + "=" * 60)
    print("ğŸ”„ å‡†å¤‡ä¸‹ä¸€è½® (Next Round) æ•°æ®")
    print("=" * 60)
    
    # 1. ä¿å­˜æ–°çš„ä¼ªæ ‡ç­¾ (Refined Labels)
    next_round_dir = os.path.join(args.experiment_dir, "04_Iterative_Labels")
    os.makedirs(next_round_dir, exist_ok=True)
    
    timestamp_id = datetime.now().strftime("%H%M%S")
    label_save_path = os.path.join(next_round_dir, f"refined_labels_{timestamp_id}.txt")
    
    np.savetxt(label_save_path, new_labels.cpu().numpy(), fmt='%d')
    print(f"   ğŸ“ ä¿®æ­£æ ‡ç­¾å·²ä¿å­˜: {label_save_path}")

    # ä¿å­˜å®Œæ•´ç»“æœ
    try:
        results = {
            'new_labels': new_labels.cpu(),
            'noise_mask': noise_mask.cpu(),
            'high_conf_mask': high_conf_mask.cpu(),
            'low_conf_mask': low_conf_mask.cpu(),
            'strength': strength.cpu(),
            'consensus_dict': consensus_dict,
            'refine_stats': refine_stats,
            'conf_stats': conf_stats,
            'next_round_files': {
                'labels': label_save_path,
                'reference': consensus_path
            },
            'args': vars(args)
        }

        results_path = os.path.join(args.output_dir, "step2_results.pth")
        torch.save(results, results_path)
        print(f"\n   ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {results_path}")

    except Exception as e:
        print(f"   âš ï¸ ç»“æœä¿å­˜å¤±è´¥: {e}")
        results = None

    print(f"\nğŸ‰ Step2å®Œæˆï¼ç›¸å¯¹ä¸ç¡®å®šæ€§åŸåˆ™ç”Ÿæ•ˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Step2: Evidence-Guided Refinement & Decoding')

    parser.add_argument('--experiment_dir', type=str, required=True, help='å®éªŒç›®å½•')
    parser.add_argument('--step1_checkpoint', type=str, required=True, help='Step1 checkpoint')
    parser.add_argument('--dim', type=int, default=256)
    parser.add_argument('--max_length', type=int, default=150)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--uncertainty_percentile', type=float, default=0.2, help='ä½ç½®ä¿¡åº¦ç™¾åˆ†æ¯”')
    parser.add_argument('--delta', type=float, default=None, help='è·ç¦»é˜ˆå€¼')
    parser.add_argument('--delta_percentile', type=int, default=10, help='deltaç™¾åˆ†ä½æ•°')
    parser.add_argument('--output_dir', type=str,
                        default=f'./step2_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                        help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    try:
        run_step2(args)
    except Exception as e:
        print(f"âŒ Step2æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)