# models/step2_decode.py
"""
Step2: Evidence-Weighted Consensus Decoding
æ ¸å¿ƒï¼šç”Ÿæˆæ¯ä¸ªç°‡çš„æœ€ç»ˆå…±è¯†åºåˆ—
"""
import torch
import torch.nn.functional as F

def decode_cluster_consensus(evidence, alpha, labels, strength):
    """
    âœ… Phase C: Evidence-weighted consensusè§£ç 
    
    Args:
        evidence: (N, L, 4) æ¯æ¡readçš„evidence
        alpha: (N, L, 4) Dirichletå‚æ•°
        labels: (N,) ä¿®æ­£åŽçš„æ ‡ç­¾
        strength: (N,) evidence strength
    
    Returns:
        consensus_dict: dict[label] -> {
            'consensus_prob': (L, 4),
            'consensus_seq': str,
            'num_reads': int,
            'avg_strength': float
        }
    """
    consensus_dict = {}
    base_map = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}
    
    unique_labels = torch.unique(labels)
    
    for k in unique_labels:
        if k < 0:  # è·³è¿‡å™ªå£°
            continue
        
        mask = (labels == k)
        count = mask.sum().item()
        
        if count < 2:
            continue
        
        # èŽ·å–è¯¥ç°‡çš„æ‰€æœ‰reads
        cluster_alpha = alpha[mask]  # (cluster_size, L, 4)
        cluster_strength = strength[mask]  # (cluster_size,)
        
        # âœ… Evidence-weighted fusion
        # ä½¿ç”¨strengthä½œä¸ºæƒé‡
        weights = F.softmax(cluster_strength, dim=0).view(-1, 1, 1)
        
        # åŠ æƒèžåˆ
        fused_alpha = torch.sum(cluster_alpha * weights, dim=0)  # (L, 4)
        
        # å½’ä¸€åŒ–å¾—åˆ°æ¦‚çŽ‡åˆ†å¸ƒ
        consensus_prob = fused_alpha / fused_alpha.sum(dim=-1, keepdim=True)
        
        # è§£ç ä¸ºåºåˆ—
        consensus_indices = torch.argmax(consensus_prob, dim=-1)
        consensus_seq = ''.join([base_map[idx.item()] for idx in consensus_indices])
        
        consensus_dict[int(k.item())] = {
            'consensus_prob': consensus_prob.cpu(),
            'consensus_seq': consensus_seq,
            'num_reads': count,
            'avg_strength': cluster_strength.mean().item()
        }
    
    print(f"\n   ðŸ§¬ ç”Ÿæˆ {len(consensus_dict)} ä¸ªå…±è¯†åºåˆ—")
    print(f"      å¹³å‡é•¿åº¦: {len(next(iter(consensus_dict.values()))['consensus_seq'])}")
    
    return consensus_dict


def save_consensus_sequences(consensus_dict, output_path):
    """
    ä¿å­˜å…±è¯†åºåˆ—ä¸ºFASTAæ ¼å¼
    
    Args:
        consensus_dict: decode_cluster_consensusçš„è¾“å‡º
        output_path: str, è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w') as f:
        for label, info in sorted(consensus_dict.items()):
            f.write(f">cluster_{label}_reads{info['num_reads']}_strength{info['avg_strength']:.3f}\n")
            f.write(f"{info['consensus_seq']}\n")
    
    print(f"   ðŸ’¾ å…±è¯†åºåˆ—å·²ä¿å­˜: {output_path}")


def compute_consensus_quality_metrics(consensus_dict, gt_sequences=None):
    """
    è®¡ç®—å…±è¯†åºåˆ—è´¨é‡æŒ‡æ ‡
    
    Args:
        consensus_dict: decode_cluster_consensusçš„è¾“å‡º
        gt_sequences: dict[label] -> str, ground truthåºåˆ—ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        metrics: dict, è´¨é‡æŒ‡æ ‡
    """
    metrics = {
        'num_clusters': len(consensus_dict),
        'avg_reads_per_cluster': sum(c['num_reads'] for c in consensus_dict.values()) / len(consensus_dict),
        'avg_strength': sum(c['avg_strength'] for c in consensus_dict.values()) / len(consensus_dict)
    }
    
    # å¦‚æžœæœ‰GTï¼Œè®¡ç®—å‡†ç¡®çŽ‡
    if gt_sequences is not None:
        matches = 0
        total = 0
        for label, info in consensus_dict.items():
            if label in gt_sequences:
                pred_seq = info['consensus_seq']
                gt_seq = gt_sequences[label]
                matches += sum(p == g for p, g in zip(pred_seq, gt_seq))
                total += len(pred_seq)
        
        if total > 0:
            metrics['sequence_accuracy'] = matches / total
    
    return metrics
