# models/step2_decode.py
"""
Step2: Evidence-Weighted Consensus Decoding
æ ¸å¿ƒï¼šç”Ÿæˆæ¯ä¸ªç°‡çš„æœ€ç»ˆå…±è¯†åºåˆ—
âœ… åå‘ç¡®å®šæ€§ï¼šé«˜ç½®ä¿¡åº¦readsä¸»å¯¼consensus
"""
import torch
import torch.nn.functional as F


def decode_cluster_consensus(evidence, alpha, labels, strength, high_conf_mask):
    """
    âœ… Phase C: åå‘ç¡®å®šæ€§çš„consensusè§£ç 
    é«˜ç½®ä¿¡åº¦reads 100%æƒé‡ï¼Œä½Žç½®ä¿¡åº¦readsä»…ä½œå‚è€ƒ

    Args:
        evidence: (N, L, 4) æ¯æ¡readçš„evidence
        alpha: (N, L, 4) Dirichletå‚æ•°
        labels: (N,) ä¿®æ­£åŽçš„æ ‡ç­¾
        strength: (N,) evidence strength
        high_conf_mask: (N,) é«˜ç½®ä¿¡åº¦mask

    Returns:
        consensus_dict: dict[label] -> {
            'consensus_prob': (L, 4),
            'consensus_seq': str,
            'num_reads': int,
            'num_high_conf': int,
            'avg_strength': float
        }
    """
    consensus_dict = {}
    base_map = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}

    unique_labels = torch.unique(labels)

    for k in unique_labels:
        if k < 0:  # è·³è¿‡å™ªå£°
            continue

        mask = (labels == k)
        count = mask.sum().item()

        if count < 2:
            continue

        # è¯¥ç°‡çš„reads
        cluster_alpha = alpha[mask]  # (cluster_size, L, 4)
        cluster_strength = strength[mask]  # (cluster_size,)
        cluster_high_conf = high_conf_mask[mask]  # (cluster_size,)

        high_conf_count = cluster_high_conf.sum().item()

        if high_conf_count == 0:
            print(f"   âš ï¸ ç°‡{k}: æ²¡æœ‰é«˜ç½®ä¿¡åº¦readsï¼Œè·³è¿‡consensus")
            continue

        # âœ… ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨é«˜ç½®ä¿¡åº¦reads
        if high_conf_count >= 2:
            # åªç”¨é«˜ç½®ä¿¡åº¦readsåšconsensus
            consensus_alpha = cluster_alpha[cluster_high_conf]
            consensus_strength = cluster_strength[cluster_high_conf]
            print(f"   ðŸŽ¯ ç°‡{k}: åªç”¨ {high_conf_count}/{count} é«˜ç½®ä¿¡åº¦reads")
        else:
            # é«˜ç½®ä¿¡åº¦ä¸å¤Ÿï¼ŒåŠ æƒèžåˆï¼ˆé«˜ç½®ä¿¡åº¦æƒé‡æ›´å¤§ï¼‰
            weights = torch.where(cluster_high_conf, 2.0, 0.5)  # é«˜ç½®ä¿¡åº¦2å€æƒé‡
            weights = weights / weights.sum()

            consensus_alpha = cluster_alpha
            consensus_strength = cluster_strength
            print(f"   âš–ï¸ ç°‡{k}: åŠ æƒèžåˆ {high_conf_count}/{count} é«˜ç½®ä¿¡åº¦reads")

        # Evidence-weighted fusion
        if high_conf_count >= 2:
            # ç®€å•å¹³å‡ï¼ˆé«˜ç½®ä¿¡åº¦readsè´¨é‡ç›¸è¿‘ï¼‰
            fused_alpha = consensus_alpha.mean(dim=0)  # (L, 4)
        else:
            # åŠ æƒèžåˆ
            weights = F.softmax(consensus_strength, dim=0).view(-1, 1, 1)
            fused_alpha = torch.sum(consensus_alpha * weights, dim=0)  # (L, 4)

        # å½’ä¸€åŒ–å¾—åˆ°æ¦‚çŽ‡åˆ†å¸ƒ
        consensus_prob = fused_alpha / fused_alpha.sum(dim=-1, keepdim=True)

        # è§£ç ä¸ºåºåˆ—
        consensus_indices = torch.argmax(consensus_prob, dim=-1)
        consensus_seq = ''.join([base_map[idx.item()] for idx in consensus_indices])

        consensus_dict[int(k.item())] = {
            'consensus_prob': consensus_prob.cpu(),
            'consensus_seq': consensus_seq,
            'num_reads': count,
            'num_high_conf': high_conf_count,
            'avg_strength': cluster_strength.mean().item()
        }

    print(f"\n   ðŸ§¬ ç”Ÿæˆ {len(consensus_dict)} ä¸ªå…±è¯†åºåˆ—")
    if consensus_dict:
        avg_len = sum(len(info['consensus_seq']) for info in consensus_dict.values()) / len(consensus_dict)
        print(f"      å¹³å‡é•¿åº¦: {avg_len:.1f}")

        high_conf_ratio = sum(info['num_high_conf'] / info['num_reads'] for info in consensus_dict.values()) / len(
            consensus_dict)
        print(f"      å¹³å‡é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {high_conf_ratio:.1%}")

    return consensus_dict


def save_consensus_sequences(consensus_dict, output_path):
    """
    ä¿å­˜å…±è¯†åºåˆ—ä¸ºFASTAæ ¼å¼

    Args:
        consensus_dict: decode_cluster_consensusçš„è¾“å‡º
        output_path: str, è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w') as f:
        for label, info in sorted(consensus_dict.items()):
            f.write(
                f">cluster_{label}_reads{info['num_reads']}_highconf{info['num_high_conf']}_strength{info['avg_strength']:.3f}\n")
            f.write(f"{info['consensus_seq']}\n")

    print(f"   ðŸ’¾ å…±è¯†åºåˆ—å·²ä¿å­˜: {output_path}")


def compute_consensus_quality_metrics(consensus_dict, gt_sequences=None):
    """
    è®¡ç®—å…±è¯†åºåˆ—è´¨é‡æŒ‡æ ‡

    Args:
        consensus_dict: decode_cluster_consensusçš„è¾“å‡º
        gt_sequences: dict[label] -> str, ground truthåºåˆ—ï¼ˆå¯é€‰ï¼‰

    Returns:
        metrics: dict, è´¨é‡æŒ‡æ ‡
    """
    metrics = {
        'num_clusters': len(consensus_dict),
        'avg_reads_per_cluster': sum(c['num_reads'] for c in consensus_dict.values()) / len(
            consensus_dict) if consensus_dict else 0,
        'avg_strength': sum(c['avg_strength'] for c in consensus_dict.values()) / len(
            consensus_dict) if consensus_dict else 0,
        'avg_high_conf_ratio': sum(c['num_high_conf'] / c['num_reads'] for c in consensus_dict.values()) / len(
            consensus_dict) if consensus_dict else 0
    }

    # å¦‚æžœæœ‰GTï¼Œè®¡ç®—å‡†ç¡®çŽ‡
    if gt_sequences is not None:
        matches = 0
        total = 0
        for label, info in consensus_dict.items():
            if label in gt_sequences:
                pred_seq = info['consensus_seq']
                gt_seq = gt_sequences[label]
                matches += sum(p == g for p, g in zip(pred_seq, gt_seq))
                total += len(pred_seq)

        if total > 0:
            metrics['sequence_accuracy'] = matches / total

    return metrics