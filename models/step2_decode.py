# models/step2_decode.py
"""
Step2: Evidence-Weighted Consensus Decoding

ä¿®å¤æ¸…å•:
  [FIX-#2]  æƒ…å†µ2çš„ conf_weights ä¸å†è¢« strength softmax è¦†ç›–ï¼Œ
            æ”¹ä¸ºä¸¤ç§æƒé‡ç›¸ä¹˜åå½’ä¸€åŒ–
"""
import torch
import torch.nn.functional as F


def decode_cluster_consensus(evidence, alpha, labels, strength, high_conf_mask,
                             verbose=False):
    """
    å¯¹æ¯ä¸ªæœ‰æ•ˆç°‡ï¼Œç”¨é«˜ç½®ä¿¡åº¦ reads çš„ Î± åšé€ä½æŠ•ç¥¨æ¢å¤åŸå§‹åºåˆ—ã€‚

    Args:
        evidence:       (N, L, 4)
        alpha:          (N, L, 4)
        labels:         (N,) ä¿®æ­£åæ ‡ç­¾
        strength:       (N,) evidence strength
        high_conf_mask: (N,) bool é«˜ç½®ä¿¡åº¦ mask
        verbose:        æ˜¯å¦æ‰“å°æ¯ä¸ªç°‡çš„è¯¦æƒ… (å¤§æ•°æ®é›†å»ºè®® False)
    """
    consensus_dict = {}
    base_map = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}

    unique_labels = torch.unique(labels)
    skipped_no_hc = 0

    for k in unique_labels:
        if k < 0:
            continue

        mask = (labels == k)
        count = mask.sum().item()
        if count < 2:
            continue

        cluster_alpha = alpha[mask]
        cluster_strength = strength[mask]
        cluster_high_conf = high_conf_mask[mask]
        high_conf_count = cluster_high_conf.sum().item()

        if high_conf_count == 0:
            skipped_no_hc += 1
            continue

        # ====== Consensus ç­–ç•¥ ======
        if high_conf_count >= 2:
            # æƒ…å†µ1: è¶³å¤Ÿçš„é«˜ç½®ä¿¡åº¦ reads â†’ ç®€å•å¹³å‡
            consensus_alpha = cluster_alpha[cluster_high_conf]
            fused_alpha = consensus_alpha.mean(dim=0)
            if verbose:
                print(f"   ğŸ¯ ç°‡{k}: {high_conf_count}/{count} é«˜ç½®ä¿¡åº¦reads")
        else:
            # æƒ…å†µ2: é«˜ç½®ä¿¡åº¦ä¸è¶³ â†’ åŠ æƒèåˆ
            # [FIX-#2] ä¸¤ç§æƒé‡ç›¸ä¹˜: conf æƒé‡ Ã— strength æƒé‡
            conf_weights = torch.where(cluster_high_conf, 2.0, 0.5)
            str_weights  = F.softmax(cluster_strength, dim=0)
            combined     = conf_weights * str_weights
            combined     = (combined / combined.sum()).view(-1, 1, 1)
            fused_alpha  = torch.sum(cluster_alpha * combined, dim=0)
            if verbose:
                print(f"   âš–ï¸ ç°‡{k}: åŠ æƒèåˆ {high_conf_count}/{count}")

        # è§£ç ä¸ºåºåˆ—
        consensus_prob = fused_alpha / fused_alpha.sum(dim=-1, keepdim=True)
        consensus_indices = torch.argmax(consensus_prob, dim=-1)
        consensus_seq = ''.join([base_map[idx.item()] for idx in consensus_indices])

        consensus_dict[int(k.item())] = {
            'consensus_prob': consensus_prob.cpu(),
            'consensus_seq': consensus_seq,
            'num_reads': count,
            'num_high_conf': int(high_conf_count),
            'avg_strength': cluster_strength.mean().item()
        }

    print(f"\n   ğŸ§¬ å…±è¯†åºåˆ—: {len(consensus_dict)} ä¸ª (è·³è¿‡ {skipped_no_hc} ä¸ªæ— é«˜ç½®ä¿¡åº¦ç°‡)")
    if consensus_dict:
        avg_len = sum(len(v['consensus_seq']) for v in consensus_dict.values()) / len(consensus_dict)
        hc_ratios = [v['num_high_conf'] / v['num_reads'] for v in consensus_dict.values()]
        print(f"      å¹³å‡é•¿åº¦: {avg_len:.1f}, å¹³å‡é«˜ç½®ä¿¡åº¦æ¯”: {sum(hc_ratios)/len(hc_ratios):.1%}")

    return consensus_dict


def save_consensus_sequences(consensus_dict, output_path):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        for label, info in sorted(consensus_dict.items()):
            f.write(f">cluster_{label}_reads{info['num_reads']}"
                    f"_highconf{info['num_high_conf']}"
                    f"_strength{info['avg_strength']:.3f}\n")
            f.write(f"{info['consensus_seq']}\n")
    print(f"   ğŸ’¾ å…±è¯†åºåˆ—å·²ä¿å­˜: {output_path}")


import os


def compute_consensus_quality_metrics(consensus_dict, gt_sequences=None):
    metrics = {
        'num_clusters': len(consensus_dict),
        'avg_reads_per_cluster': sum(c['num_reads'] for c in consensus_dict.values()) / max(len(consensus_dict), 1),
        'avg_strength': sum(c['avg_strength'] for c in consensus_dict.values()) / max(len(consensus_dict), 1),
        'avg_high_conf_ratio': sum(c['num_high_conf'] / c['num_reads'] for c in consensus_dict.values()) / max(len(consensus_dict), 1)
    }

    if gt_sequences is not None:
        matches = 0
        total = 0
        for label, info in consensus_dict.items():
            if label in gt_sequences:
                pred_seq = info['consensus_seq']
                gt_seq = gt_sequences[label]
                min_len = min(len(pred_seq), len(gt_seq))
                matches += sum(p == g for p, g in zip(pred_seq[:min_len], gt_seq[:min_len]))
                total += max(len(pred_seq), len(gt_seq))
        if total > 0:
            metrics['sequence_accuracy'] = matches / total

    return metrics