#!/usr/bin/env python3
"""
FedDNA ç°‡åºåˆ—é‡å»ºç³»ç»Ÿ - é€‚é…ç‰ˆ v3
=====================================
é€‚é…ä½ çš„æ•°æ®æ ¼å¼ï¼š
- read.txt: æŒ‰ç°‡åˆ†ç»„ï¼Œç”¨=======åˆ†éš”
- ground_truth_clusters.txt: Cluster_ID \t Ref_Seq
- ground_truth_reads.txt: Read_ID \t Cluster_ID \t Ref_Seq \t Quality
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict, Counter
from dataclasses import dataclass
import matplotlib.pyplot as plt
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# é…ç½®
# ============================================================================
@dataclass
class Config:
    """è®­ç»ƒé…ç½®"""
    # æ•°æ®è·¯å¾„
    experiment_dir: str = ""  # å®éªŒæ ¹ç›®å½•
    
    # æ¨¡å‹å‚æ•°
    input_dim: int = 6
    hidden_dim: int = 128
    latent_dim: int = 64
    num_heads: int = 4
    num_layers: int = 3
    dropout: float = 0.1
    
    # è®­ç»ƒå‚æ•°
    k_target: int = 50
    batch_size: int = 64
    num_epochs: int = 50
    learning_rate: float = 1e-3
    
    # æŸå¤±æƒé‡
    lambda_contrastive: float = 1.0
    lambda_del: float = 0.5
    lambda_k: float = 2.0
    
    # é˜ˆå€¼
    similarity_threshold: float = 0.7
    min_cluster_ratio: float = 0.005
    weak_consistency_threshold: float = 0.6
    
    # è®¾å¤‡
    device: str = "cuda" if torch.cuda.is_available() else "cpu"


# ============================================================================
# æ•°æ®åŠ è½½ - é€‚é…ä½ çš„æ ¼å¼
# ============================================================================
def load_feddna_format(feddna_dir: str) -> Tuple[List[str], List[int]]:
    """
    åŠ è½½ FedDNA æ ¼å¼çš„æ•°æ®
    read.txt: æŒ‰ç°‡åˆ†ç»„ï¼Œç”¨ =============================== åˆ†éš”
    
    è¿”å›: (readsåˆ—è¡¨, ç°‡æ ‡ç­¾åˆ—è¡¨)
    """
    read_path = os.path.join(feddna_dir, "read.txt")
    
    if not os.path.exists(read_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° read.txt: {read_path}")
    
    reads = []
    labels = []
    current_cluster = 0
    
    print(f"ğŸ“‚ åŠ è½½ FedDNA æ ¼å¼æ•°æ®: {read_path}")
    
    with open(read_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹åˆ†éš”ç¬¦
            if line.startswith("====="):
                current_cluster += 1
            else:
                # è¿™æ˜¯ä¸€ä¸ªreadåºåˆ—
                reads.append(line)
                labels.append(current_cluster)
    
    print(f"   âœ… åŠ è½½ {len(reads)} æ¡ reads")
    print(f"   âœ… æ£€æµ‹åˆ° {current_cluster + 1} ä¸ªç°‡ (Cloverèšç±»ç»“æœ)")
    
    return reads, labels


def load_raw_reads_with_ids(raw_dir: str) -> Dict[str, str]:
    """
    åŠ è½½åŸå§‹reads (å¸¦ID)
    raw_reads.txt: Read_ID \t Sequence
    
    è¿”å›: {read_id: sequence}
    """
    raw_path = os.path.join(raw_dir, "raw_reads.txt")
    
    if not os.path.exists(raw_path):
        print(f"   âš ï¸ raw_reads.txt ä¸å­˜åœ¨")
        return {}
    
    reads_dict = {}
    with open(raw_path, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                reads_dict[parts[0]] = parts[1]
    
    print(f"   âœ… åŠ è½½ {len(reads_dict)} æ¡åŸå§‹reads (å¸¦ID)")
    return reads_dict


def load_read_level_gt(raw_dir: str) -> Dict[str, Tuple[int, str, str]]:
    """
    åŠ è½½ Read çº§åˆ«çš„ GT
    ground_truth_reads.txt: Read_ID \t Cluster_ID \t Ref_Seq \t Quality
    
    è¿”å›: {read_id: (cluster_id, ref_seq, quality)}
    """
    gt_path = os.path.join(raw_dir, "ground_truth_reads.txt")
    
    if not os.path.exists(gt_path):
        print(f"   âš ï¸ ground_truth_reads.txt ä¸å­˜åœ¨")
        return {}
    
    gt_dict = {}
    with open(gt_path, 'r') as f:
        header = f.readline()  # è·³è¿‡è¡¨å¤´
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 4:
                read_id = parts[0]
                cluster_id = int(parts[1])
                ref_seq = parts[2]
                quality = parts[3]
                gt_dict[read_id] = (cluster_id, ref_seq, quality)
    
    print(f"   âœ… åŠ è½½ {len(gt_dict)} æ¡ Read-Level GT")
    return gt_dict


def load_cluster_level_gt(raw_dir: str) -> Dict[int, str]:
    """
    åŠ è½½ Cluster çº§åˆ«çš„ GT
    ground_truth_clusters.txt: Cluster_ID \t Ref_Seq
    
    è¿”å›: {cluster_id: ref_seq}
    """
    gt_path = os.path.join(raw_dir, "ground_truth_clusters.txt")
    
    if not os.path.exists(gt_path):
        print(f"   âš ï¸ ground_truth_clusters.txt ä¸å­˜åœ¨")
        return {}
    
    print(f"\nğŸ” åŠ è½½ Cluster-Level GT: {gt_path}")
    
    gt_dict = {}
    with open(gt_path, 'r') as f:
        header = f.readline()  # è·³è¿‡è¡¨å¤´
        print(f"   è¡¨å¤´: {header.strip()}")
        
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                try:
                    cluster_id = int(parts[0])
                    ref_seq = parts[1]
                    gt_dict[cluster_id] = ref_seq
                    
                    if len(gt_dict) <= 3:
                        print(f"     GT[{cluster_id}]: {ref_seq[:50]}...")
                except ValueError:
                    continue
    
    print(f"   âœ… åŠ è½½ {len(gt_dict)} ä¸ª Cluster GT åºåˆ—")
    if gt_dict:
        print(f"   GTç°‡IDèŒƒå›´: {min(gt_dict.keys())} - {max(gt_dict.keys())}")
    
    return gt_dict


def build_sequence_to_gt_mapping(feddna_reads: List[str], 
                                 raw_reads: Dict[str, str],
                                 read_gt: Dict[str, Tuple[int, str, str]]) -> List[int]:
    """
    å»ºç«‹ FedDNA reads åˆ°åŸå§‹ GT ç°‡çš„æ˜ å°„
    
    é€šè¿‡åºåˆ—åŒ¹é…æ‰¾åˆ°æ¯ä¸ªreadå¯¹åº”çš„åŸå§‹GTç°‡ID
    """
    # åå‘æ˜ å°„ï¼šsequence -> read_id
    seq_to_id = {seq: rid for rid, seq in raw_reads.items()}
    
    original_gt_labels = []
    matched = 0
    
    for seq in feddna_reads:
        if seq in seq_to_id:
            read_id = seq_to_id[seq]
            if read_id in read_gt:
                gt_cluster_id = read_gt[read_id][0]
                original_gt_labels.append(gt_cluster_id)
                matched += 1
            else:
                original_gt_labels.append(-1)
        else:
            original_gt_labels.append(-1)
    
    print(f"   âœ… GTæ ‡ç­¾åŒ¹é…: {matched}/{len(feddna_reads)} ({matched/len(feddna_reads)*100:.1f}%)")
    
    return original_gt_labels


# ============================================================================
# æ•°æ®ç®¡ç†å™¨ - é€‚é…ç‰ˆ
# ============================================================================
class ClusterDataManager:
    """ç°‡æ•°æ®ç®¡ç†å™¨ - é€‚é…ä½ çš„æ•°æ®æ ¼å¼"""
    
    def __init__(self, experiment_dir: str, config: Config):
        self.experiment_dir = experiment_dir
        self.config = config
        
        # è·¯å¾„
        self.raw_dir = os.path.join(experiment_dir, "01_RawData")
        self.feddna_dir = os.path.join(experiment_dir, "03_FedDNA_In")
        
        # æ•°æ®å­˜å‚¨
        self.reads: List[str] = []
        self.qualities: List[str] = []
        self.clover_labels: np.ndarray = None  # Cloverèšç±»ç»“æœ
        self.original_gt_labels: np.ndarray = None  # åŸå§‹GTç°‡æ ‡ç­¾
        self.current_labels: np.ndarray = None
        
        # ç°‡ç®¡ç†
        self.cluster_assignments: Dict[int, Set[int]] = defaultdict(set)
        self.cluster_status: Dict[int, str] = {}
        self.noise_reads: Set[int] = set()
        
        # GT
        self.cluster_gt: Dict[int, str] = {}  # ç°‡çº§GT
        
        # åŠ è½½æ•°æ®
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("\n" + "=" * 60)
        print("ğŸ“‚ åŠ è½½æ•°æ®")
        print("=" * 60)
        
        # 1. åŠ è½½ FedDNA æ ¼å¼çš„ reads
        self.reads, clover_labels = load_feddna_format(self.feddna_dir)
        self.clover_labels = np.array(clover_labels)
        
        # 2. ç”Ÿæˆé»˜è®¤è´¨é‡åˆ†æ•°
        self.qualities = ['I' * len(read) for read in self.reads]
        print(f"   âœ… ç”Ÿæˆé»˜è®¤è´¨é‡åˆ†æ•°")
        
        # 3. åŠ è½½åŸå§‹readså’ŒGT
        raw_reads = load_raw_reads_with_ids(self.raw_dir)
        read_gt = load_read_level_gt(self.raw_dir)
        self.cluster_gt = load_cluster_level_gt(self.raw_dir)
        
        # 4. å»ºç«‹GTæ˜ å°„
        if raw_reads and read_gt:
            self.original_gt_labels = np.array(
                build_sequence_to_gt_mapping(self.reads, raw_reads, read_gt)
            )
        else:
            self.original_gt_labels = np.full(len(self.reads), -1)
            print(f"   âš ï¸ æ— æ³•å»ºç«‹GTæ˜ å°„ï¼Œä½¿ç”¨-1å¡«å……")
        
        # 5. åˆå§‹åŒ–å½“å‰æ ‡ç­¾ (ä½¿ç”¨Cloverç»“æœä½œä¸ºèµ·ç‚¹)
        self.current_labels = self.clover_labels.copy()
        
        # 6. åˆå§‹åŒ–ç°‡åˆ†é…
        for idx, label in enumerate(self.current_labels):
            if label >= 0:
                self.cluster_assignments[label].add(idx)
                self.cluster_status[label] = 'healthy'
        
        self.total_reads = len(self.reads)
        
        print(f"\nğŸ“Š æ•°æ®æ‘˜è¦:")
        print(f"   - æ€»Reads: {self.total_reads}")
        print(f"   - Cloverç°‡æ•°: {len(np.unique(self.clover_labels))}")
        print(f"   - GTç°‡æ•°: {len(self.cluster_gt)}")
        print(f"   - ç›®æ ‡K: {self.config.k_target}")
    
    def get_cluster_reads(self, cluster_id: int) -> List[int]:
        """è·å–ç°‡å†…æ‰€æœ‰readç´¢å¼•"""
        return list(self.cluster_assignments.get(cluster_id, set()))
    
    def get_active_clusters(self) -> List[int]:
        """è·å–æ‰€æœ‰æ´»è·ƒç°‡ID"""
        return [cid for cid, reads in self.cluster_assignments.items() 
                if len(reads) > 0 and self.cluster_status.get(cid) != 'eliminated']
    
    def reassign_read(self, read_idx: int, new_cluster_id: int):
        """é‡åˆ†é…readåˆ°æ–°ç°‡"""
        old_cluster = self.current_labels[read_idx]
        
        if old_cluster >= 0 and old_cluster in self.cluster_assignments:
            self.cluster_assignments[old_cluster].discard(read_idx)
        
        self.noise_reads.discard(read_idx)
        
        self.current_labels[read_idx] = new_cluster_id
        self.cluster_assignments[new_cluster_id].add(read_idx)
        
        if new_cluster_id not in self.cluster_status:
            self.cluster_status[new_cluster_id] = 'healthy'
    
    def mark_as_noise(self, read_idx: int):
        """æ ‡è®°readä¸ºå™ªå£°"""
        old_cluster = self.current_labels[read_idx]
        
        if old_cluster >= 0 and old_cluster in self.cluster_assignments:
            self.cluster_assignments[old_cluster].discard(read_idx)
        
        self.current_labels[read_idx] = -1
        self.noise_reads.add(read_idx)
    
    def remove_cluster(self, cluster_id: int):
        """ç§»é™¤ç°‡"""
        if cluster_id in self.cluster_assignments:
            del self.cluster_assignments[cluster_id]
        self.cluster_status[cluster_id] = 'eliminated'
    
    def get_k_effective(self) -> int:
        """è·å–å½“å‰æœ‰æ•ˆç°‡æ•°"""
        return len([cid for cid, reads in self.cluster_assignments.items() 
                   if len(reads) > 0])


# ============================================================================
# åºåˆ—ç¼–ç 
# ============================================================================
class SequenceEncoder:
    """åºåˆ—ç¼–ç å™¨"""
    
    BASE_MAP = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4}
    
    @staticmethod
    def encode_sequence(seq: str, quality: str, max_len: int = 150) -> torch.Tensor:
        """ç¼–ç åºåˆ—ä¸ºå¼ é‡"""
        seq_len = min(len(seq), max_len)
        encoding = torch.zeros(max_len, 6)
        
        for i in range(seq_len):
            base = seq[i].upper()
            if base in SequenceEncoder.BASE_MAP and SequenceEncoder.BASE_MAP[base] < 4:
                encoding[i, SequenceEncoder.BASE_MAP[base]] = 1.0
            
            if i < len(quality):
                q = ord(quality[i]) - 33
                encoding[i, 4] = q / 40.0
            else:
                encoding[i, 4] = 0.5
            
            encoding[i, 5] = i / max_len
        
        return encoding


# ============================================================================
# æ•°æ®é›†
# ============================================================================
class ClusterDataset(Dataset):
    """ç°‡æ•°æ®é›†"""
    
    def __init__(self, data_manager: ClusterDataManager, max_len: int = 150):
        self.data_manager = data_manager
        self.max_len = max_len
        self.encoder = SequenceEncoder()
        
        self.valid_indices = [i for i in range(len(data_manager.reads))
                             if i not in data_manager.noise_reads]
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        
        seq = self.data_manager.reads[real_idx]
        qual = self.data_manager.qualities[real_idx]
        label = self.data_manager.current_labels[real_idx]
        
        encoding = self.encoder.encode_sequence(seq, qual, self.max_len)
        
        return {
            'encoding': encoding,
            'label': label,
            'index': real_idx
        }


# ============================================================================
# æ¨¡å‹
# ============================================================================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 200):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


class TransformerEncoder(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        self.input_proj = nn.Linear(config.input_dim, config.hidden_dim)
        self.pos_encoding = PositionalEncoding(config.hidden_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.num_heads,
            dim_feedforward=config.hidden_dim * 4,
            dropout=config.dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)
        self.output_proj = nn.Linear(config.hidden_dim, config.latent_dim)
    
    def forward(self, x):
        x = self.input_proj(x)
        x = self.pos_encoding(x)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.output_proj(x)
        return F.normalize(x, p=2, dim=-1)


class SequenceDecoder(nn.Module):
    def __init__(self, config: Config, max_len: int = 150):
        super().__init__()
        self.max_len = max_len
        self.latent_proj = nn.Linear(config.latent_dim, config.hidden_dim)
        self.decoder = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim * 2, max_len * 4)
        )
    
    def forward(self, z):
        x = self.latent_proj(z)
        x = self.decoder(x)
        x = x.view(-1, self.max_len, 4)
        return x


class ClusterReconstructionModel(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        self.encoder = TransformerEncoder(config)
        self.decoder = SequenceDecoder(config)
        self.cluster_centers = nn.Parameter(
            torch.randn(config.k_target + 20, config.latent_dim)
        )
        nn.init.xavier_uniform_(self.cluster_centers)
    
    def forward(self, x):
        z = self.encoder(x)
        logits = self.decoder(z)
        return z, logits


# ============================================================================
# æŸå¤±å‡½æ•°
# ============================================================================
class ContrastiveLoss(nn.Module):
    def __init__(self, temperature: float = 0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, z: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        batch_size = z.size(0)
        if batch_size < 2:
            return torch.tensor(0.0, device=z.device)
        
        sim_matrix = torch.mm(z, z.t()) / self.temperature
        
        labels = labels.view(-1, 1)
        positive_mask = (labels == labels.t()).float()
        positive_mask.fill_diagonal_(0)
        
        diag_mask = torch.eye(batch_size, device=z.device)
        exp_sim = torch.exp(sim_matrix) * (1 - diag_mask)
        
        positive_sum = (exp_sim * positive_mask).sum(dim=1)
        total_sum = exp_sim.sum(dim=1)
        
        valid_mask = positive_sum > 0
        if valid_mask.sum() == 0:
            return torch.tensor(0.0, device=z.device)
        
        loss = -torch.log(positive_sum[valid_mask] / (total_sum[valid_mask] + 1e-8))
        return loss.mean()


class DELLoss(nn.Module):
    def forward(self, z: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        unique_labels = torch.unique(labels[labels >= 0])
        
        if len(unique_labels) < 2:
            return torch.tensor(0.0, device=z.device)
        
        centers = []
        intra_vars = []
        
        for label in unique_labels:
            mask = labels == label
            if mask.sum() > 0:
                cluster_z = z[mask]
                center = cluster_z.mean(dim=0)
                centers.append(center)
                
                if cluster_z.size(0) > 1:
                    var = ((cluster_z - center) ** 2).sum(dim=1).mean()
                    intra_vars.append(var)
        
        if len(centers) < 2:
            return torch.tensor(0.0, device=z.device)
        
        centers = torch.stack(centers)
        
        inter_dist = torch.cdist(centers, centers)
        inter_dist = inter_dist[torch.triu(torch.ones_like(inter_dist), diagonal=1) == 1]
        
        intra_loss = torch.stack(intra_vars).mean() if intra_vars else torch.tensor(0.0, device=z.device)
        inter_loss = -inter_dist.mean() if inter_dist.numel() > 0 else torch.tensor(0.0, device=z.device)
        
        return intra_loss + 0.5 * inter_loss


class StrictKConstraintLoss(nn.Module):
    def __init__(self, k_target: int, lambda_k: float = 2.0):
        super().__init__()
        self.k_target = k_target
        self.lambda_k = lambda_k
    
    def forward(self, k_effective: int, epoch: int, max_epochs: int) -> torch.Tensor:
        diff = k_effective - self.k_target
        tolerance = 2
        
        if abs(diff) <= tolerance:
            penalty = (diff ** 2) * 0.1
        else:
            excess = abs(diff) - tolerance
            penalty = tolerance ** 2 * 0.1 + excess ** 3
        
        epoch_factor = 1.0 + (epoch / max_epochs) * 2.0
        
        return torch.tensor(self.lambda_k * penalty * epoch_factor, dtype=torch.float32)


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def calculate_sequence_similarity(seq1: str, seq2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„ç›¸ä¼¼åº¦"""
    min_len = min(len(seq1), len(seq2))
    if min_len == 0:
        return 0.0
    matches = sum(c1 == c2 for c1, c2 in zip(seq1[:min_len], seq2[:min_len]))
    return matches / min_len


def calculate_cluster_consistency(reads: List[str]) -> float:
    """è®¡ç®—ç°‡å†…ä¸€è‡´æ€§"""
    if len(reads) < 2:
        return 1.0
    
    sample_size = min(20, len(reads))
    sampled_reads = list(np.random.choice(reads, sample_size, replace=False)) if len(reads) > sample_size else reads
    
    max_len = max(len(r) for r in sampled_reads)
    consensus = []
    
    for pos in range(max_len):
        bases = [read[pos] for read in sampled_reads if pos < len(read)]
        if bases:
            base_counts = Counter(bases)
            consensus.append(base_counts.most_common(1)[0][0])
    
    consensus_seq = ''.join(consensus)
    
    consistencies = [calculate_sequence_similarity(read, consensus_seq) for read in sampled_reads]
    return np.mean(consistencies)


def build_consensus_sequence(reads: List[str]) -> str:
    """æ„å»ºconsensusåºåˆ—"""
    if not reads:
        return ""
    if len(reads) == 1:
        return reads[0]
    
    max_len = max(len(r) for r in reads)
    consensus = []
    
    for pos in range(max_len):
        bases = [read[pos] for read in reads if pos < len(read)]
        if bases:
            base_counts = Counter(bases)
            consensus.append(base_counts.most_common(1)[0][0])
    
    return ''.join(consensus)


def find_nearest_healthy_cluster(data_manager: ClusterDataManager, 
                                read_idx: int, 
                                exclude_cids: Set[int] = None,
                                min_cluster_size: int = 5) -> Optional[int]:
    """æ‰¾åˆ°æœ€è¿‘çš„å¥åº·ç°‡"""
    exclude_cids = exclude_cids or set()
    
    read_seq = data_manager.reads[read_idx]
    best_cid = None
    best_similarity = 0.0
    
    for cid in data_manager.cluster_assignments.keys():
        if cid in exclude_cids:
            continue
        if data_manager.cluster_status.get(cid) == 'eliminated':
            continue
        
        cluster_reads = data_manager.get_cluster_reads(cid)
        if len(cluster_reads) < min_cluster_size:
            continue
        
        sample_indices = cluster_reads[:min(5, len(cluster_reads))]
        similarities = []
        
        for center_idx in sample_indices:
            center_seq = data_manager.reads[center_idx]
            sim = calculate_sequence_similarity(read_seq, center_seq)
            similarities.append(sim)
        
        avg_similarity = np.mean(similarities)
        
        if avg_similarity > best_similarity and avg_similarity > 0.65:
            best_similarity = avg_similarity
            best_cid = cid
    
    return best_cid


# ============================================================================
# ç°‡å¥åº·åº¦è¯„ä¼°ä¸æ·˜æ±°
# ============================================================================
def evaluate_cluster_health(data_manager: ClusterDataManager, 
                           config: Config) -> Tuple[List[int], List[int]]:
    """è¯„ä¼°ç°‡å¥åº·åº¦"""
    healthy_clusters = []
    weak_clusters = []
    
    min_cluster_size = max(5, int(data_manager.total_reads * config.min_cluster_ratio))
    
    for cid in data_manager.get_active_clusters():
        cluster_reads = data_manager.get_cluster_reads(cid)
        cluster_size = len(cluster_reads)
        
        is_weak = False
        
        if cluster_size < min_cluster_size:
            is_weak = True
        
        if not is_weak and cluster_size >= 3:
            reads = [data_manager.reads[idx] for idx in cluster_reads[:20]]
            consistency = calculate_cluster_consistency(reads)
            if consistency < config.weak_consistency_threshold:
                is_weak = True
        
        if is_weak:
            weak_clusters.append(cid)
            data_manager.cluster_status[cid] = 'weak'
        else:
            healthy_clusters.append(cid)
            data_manager.cluster_status[cid] = 'healthy'
    
    return healthy_clusters, weak_clusters


def eliminate_weak_clusters(data_manager: ClusterDataManager,
                           config: Config,
                           epoch: int,
                           max_epochs: int) -> int:
    """æ·˜æ±°å¼±ç°‡"""
    eliminated = 0
    
    progress = epoch / max_epochs
    min_cluster_size = max(5, int(data_manager.total_reads * config.min_cluster_ratio))
    
    if progress > 0.7:
        min_cluster_size = max(10, int(data_manager.total_reads * 0.01))
    
    weak_clusters_info = []
    
    for cid in list(data_manager.cluster_assignments.keys()):
        if data_manager.cluster_status.get(cid) == 'eliminated':
            continue
        
        cluster_reads = data_manager.get_cluster_reads(cid)
        cluster_size = len(cluster_reads)
        
        is_weak = False
        reason = ""
        
        if cluster_size < min_cluster_size and cluster_size > 0:
            is_weak = True
            reason = f"size={cluster_size}<{min_cluster_size}"
        
        if not is_weak and cluster_size >= 3:
            reads = [data_manager.reads[idx] for idx in cluster_reads[:15]]
            consistency = calculate_cluster_consistency(reads)
            if consistency < config.weak_consistency_threshold:
                is_weak = True
                reason = f"consistency={consistency:.1%}"
        
        if is_weak:
            weak_clusters_info.append((cid, cluster_size, reason))
    
    weak_clusters_info.sort(key=lambda x: x[1])
    
    if weak_clusters_info:
        print(f"   å‘ç° {len(weak_clusters_info)} ä¸ªå¼±ç°‡å¾…æ·˜æ±°")
    
    for cid, size, reason in weak_clusters_info:
        cluster_reads = data_manager.get_cluster_reads(cid)
        
        reassigned = 0
        marked_noise = 0
        
        for read_idx in list(cluster_reads):
            best_cid = find_nearest_healthy_cluster(
                data_manager, read_idx, exclude_cids={cid}, min_cluster_size=min_cluster_size
            )
            
            if best_cid is not None:
                data_manager.reassign_read(read_idx, best_cid)
                reassigned += 1
            else:
                data_manager.mark_as_noise(read_idx)
                marked_noise += 1
        
        data_manager.remove_cluster(cid)
        eliminated += 1
        
        print(f"   âŒ æ·˜æ±°ç°‡{cid}: {reason}, é‡åˆ†é…{reassigned}, å™ªå£°{marked_noise}")
    
    return eliminated


# ============================================================================
# å›°éš¾æ ·æœ¬æŒ–æ˜
# ============================================================================
def mine_hard_samples(data_manager: ClusterDataManager,
                     model: ClusterReconstructionModel,
                     config: Config) -> Tuple[int, int]:
    """å›°éš¾æ ·æœ¬æŒ–æ˜"""
    device = config.device
    model.eval()
    
    reassigned = 0
    new_noise = 0
    
    encoder = SequenceEncoder()
    
    cluster_centers = {}
    for cid in data_manager.get_active_clusters():
        cluster_reads = data_manager.get_cluster_reads(cid)
        if len(cluster_reads) < 3:
            continue
        
        sample_indices = cluster_reads[:min(20, len(cluster_reads))]
        encodings = []
        
        for idx in sample_indices:
            enc = encoder.encode_sequence(
                data_manager.reads[idx], 
                data_manager.qualities[idx]
            )
            encodings.append(enc)
        
        encodings = torch.stack(encodings).to(device)
        
        with torch.no_grad():
            z, _ = model(encodings)
            center = z.mean(dim=0)
            cluster_centers[cid] = center
    
    if not cluster_centers:
        return 0, 0
    
    for idx in range(data_manager.total_reads):
        if idx in data_manager.noise_reads:
            continue
        
        current_label = data_manager.current_labels[idx]
        if current_label < 0 or current_label not in cluster_centers:
            continue
        
        enc = encoder.encode_sequence(
            data_manager.reads[idx],
            data_manager.qualities[idx]
        ).unsqueeze(0).to(device)
        
        with torch.no_grad():
            z, _ = model(enc)
            z = z.squeeze(0)
        
        current_center = cluster_centers[current_label]
        current_sim = F.cosine_similarity(z.unsqueeze(0), current_center.unsqueeze(0)).item()
        
        if current_sim < 0.5:
            best_cid = None
            best_sim = current_sim
            
            for cid, center in cluster_centers.items():
                if cid == current_label:
                    continue
                
                sim = F.cosine_similarity(z.unsqueeze(0), center.unsqueeze(0)).item()
                if sim > best_sim + 0.1:
                    best_sim = sim
                    best_cid = cid
            
            if best_cid is not None and best_sim > 0.6:
                data_manager.reassign_read(idx, best_cid)
                reassigned += 1
            elif current_sim < 0.3:
                data_manager.mark_as_noise(idx)
                new_noise += 1
    
    model.train()
    return reassigned, new_noise


# ============================================================================
# åºåˆ—é‡å»º
# ============================================================================
def reconstruct_sequences(data_manager: ClusterDataManager,
                         model: ClusterReconstructionModel,
                         config: Config) -> Dict[int, str]:
    """ä¸ºæ¯ä¸ªç°‡é‡å»ºå‚è€ƒåºåˆ—"""
    model.eval()
    
    reconstructed = {}
    
    print("\nğŸ§¬ åºåˆ—é‡å»º...")
    
    for cid in sorted(data_manager.get_active_clusters()):
        cluster_reads = data_manager.get_cluster_reads(cid)
        
        if len(cluster_reads) == 0:
            continue
        
        reads = [data_manager.reads[idx] for idx in cluster_reads]
        consensus = build_consensus_sequence(reads)
        reconstructed[cid] = consensus
        
        print(f"   ç°‡{cid:>2} ({len(cluster_reads):>3} reads): {consensus[:50]}...")
    
    model.train()
    return reconstructed


# ============================================================================
# éªŒè¯
# ============================================================================
def validate_results(reconstructed: Dict[int, str], 
                    data_manager: ClusterDataManager) -> Dict:
    """éªŒè¯ç»“æœ - ä½¿ç”¨å·²åŠ è½½çš„GT"""
    
    results = {
        'cluster_info': [],
        'avg_consistency': 0.0,
        'avg_gt_accuracy': 0.0,
        'total_clusters': 0,
        'gt_matched_clusters': 0
    }
    
    cluster_gt = data_manager.cluster_gt
    
    print("\n" + "=" * 90)
    print("ğŸ“Š éªŒè¯ç»“æœ")
    print("=" * 90)
    print(f"{'ç°‡ID':>6} | {'Reads':>6} | {'ä¸€è‡´æ€§':>10} | {'GTå‡†ç¡®ç‡':>10} | {'åŒ¹é…GTç°‡':>8} | {'çŠ¶æ€':>8}")
    print("-" * 90)
    
    consistency_scores = []
    gt_accuracy_scores = []
    
    for cid in sorted(reconstructed.keys()):
        recon_seq = reconstructed[cid]
        read_indices = data_manager.get_cluster_reads(cid)
        num_reads = len(read_indices)
        status = data_manager.cluster_status.get(cid, 'unknown')
        
        # 1. ç°‡å†…ä¸€è‡´æ€§
        reads = [data_manager.reads[idx] for idx in read_indices]
        avg_consistency = calculate_cluster_consistency(reads) if reads else 0.0
        consistency_scores.append(avg_consistency)
        
        # 2. GTå‡†ç¡®ç‡ - é€šè¿‡åŸå§‹GTæ ‡ç­¾æ‰¾åˆ°å¯¹åº”çš„GTåºåˆ—
        gt_accuracy = None
        matched_gt_cid = None
        
        if cluster_gt:
            # æ‰¾åˆ°è¯¥ç°‡ä¸­readsçš„åŸå§‹GTæ ‡ç­¾
            original_labels = [data_manager.original_gt_labels[idx] for idx in read_indices
                             if data_manager.original_gt_labels[idx] >= 0]
            
            if original_labels:
                # å¤šæ•°æŠ•ç¥¨
                label_counts = Counter(original_labels)
                most_common_label, count = label_counts.most_common(1)[0]
                
                if most_common_label in cluster_gt:
                    gt_seq = cluster_gt[most_common_label]
                    gt_accuracy = calculate_sequence_similarity(recon_seq, gt_seq)
                    gt_accuracy_scores.append(gt_accuracy)
                    matched_gt_cid = most_common_label
        
        # çŠ¶æ€æ˜¾ç¤º
        status_str = 'âœ“ å¥åº·' if status == 'healthy' else ('âš  å¼±' if status == 'weak' else 'â“')
        gt_str = f"{gt_accuracy*100:>8.1f}%" if gt_accuracy is not None else "      N/A"
        gt_cid_str = f"{matched_gt_cid:>8}" if matched_gt_cid is not None else "     N/A"
        
        print(f"{cid:>6} | {num_reads:>6} | {avg_consistency*100:>9.1f}% | {gt_str:>10} | {gt_cid_str:>8} | {status_str:>8}")
        
        results['cluster_info'].append({
            'cluster_id': cid,
            'num_reads': num_reads,
            'consistency': avg_consistency,
            'gt_accuracy': gt_accuracy,
            'matched_gt_cid': matched_gt_cid,
            'status': status,
            'sequence': recon_seq
        })
    
    print("-" * 90)
    
    # æ±‡æ€»
    results['avg_consistency'] = np.mean(consistency_scores) if consistency_scores else 0.0
    results['avg_gt_accuracy'] = np.mean(gt_accuracy_scores) if gt_accuracy_scores else 0.0
    results['total_clusters'] = len(reconstructed)
    results['gt_matched_clusters'] = len(gt_accuracy_scores)
    
    print(f"\nğŸ“ˆ æ±‡æ€»:")
    print(f"   - æ€»ç°‡æ•°: {results['total_clusters']} (ç›®æ ‡: {data_manager.config.k_target})")
    print(f"   - å¹³å‡Readä¸€è‡´æ€§: {results['avg_consistency']*100:.2f}%")
    
    if gt_accuracy_scores:
        print(f"   - å¹³å‡GTå‡†ç¡®ç‡: {results['avg_gt_accuracy']*100:.2f}%")
        print(f"   - GTéªŒè¯è¦†ç›–: {results['gt_matched_clusters']}/{results['total_clusters']} ç°‡")
        
        # åˆ†çº§ç»Ÿè®¡
        excellent = sum(1 for acc in gt_accuracy_scores if acc >= 0.95)
        good = sum(1 for acc in gt_accuracy_scores if 0.9 <= acc < 0.95)
        fair = sum(1 for acc in gt_accuracy_scores if 0.8 <= acc < 0.9)
        poor = sum(1 for acc in gt_accuracy_scores if acc < 0.8)
        
        print(f"   - GTå‡†ç¡®ç‡åˆ†å¸ƒ:")
        print(f"     â‰¥95%: {excellent} ({excellent/len(gt_accuracy_scores)*100:.1f}%)")
        print(f"     90-95%: {good} ({good/len(gt_accuracy_scores)*100:.1f}%)")
        print(f"     80-90%: {fair} ({fair/len(gt_accuracy_scores)*100:.1f}%)")
        print(f"     <80%: {poor} ({poor/len(gt_accuracy_scores)*100:.1f}%)")
    
    noise_ratio = len(data_manager.noise_reads) / data_manager.total_reads * 100
    print(f"   - å™ªå£°Reads: {len(data_manager.noise_reads)} ({noise_ratio:.1f}%)")
    
    return results


# ============================================================================
# ä¿å­˜ç»“æœ
# ============================================================================
def save_results(reconstructed: Dict[int, str],
                data_manager: ClusterDataManager,
                results: Dict,
                output_dir: str,
                training_history: Dict = None):
    """ä¿å­˜æ‰€æœ‰ç»“æœ"""
    
    os.makedirs(output_dir, exist_ok=True)
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
    
    # 1. FASTA
    fasta_path = os.path.join(output_dir, "reconstructed_sequences.fasta")
    with open(fasta_path, 'w') as f:
        for cid in sorted(reconstructed.keys()):
            seq = reconstructed[cid]
            num_reads = len(data_manager.get_cluster_reads(cid))
            status = data_manager.cluster_status.get(cid, 'unknown')
            f.write(f">cluster_{cid}_reads_{num_reads}_status_{status}\n")
            f.write(f"{seq}\n")
    print(f"   âœ… åºåˆ—: reconstructed_sequences.fasta")
    
    # 2. çº¯åºåˆ—
    ref_path = os.path.join(output_dir, "ref.txt")
    with open(ref_path, 'w') as f:
        for cid in sorted(reconstructed.keys()):
            f.write(f"{reconstructed[cid]}\n")
    print(f"   âœ… çº¯åºåˆ—: ref.txt")
    
    # 3. ç°‡åˆ†é…
    assign_path = os.path.join(output_dir, "cluster_assignments.txt")
    with open(assign_path, 'w') as f:
        f.write("Read_Index\tCluster_ID\tOriginal_GT_Cluster\n")
        for idx in range(data_manager.total_reads):
            label = data_manager.current_labels[idx]
            gt_label = data_manager.original_gt_labels[idx]
            f.write(f"{idx}\t{label}\t{gt_label}\n")
    print(f"   âœ… åˆ†é…: cluster_assignments.txt")
    
    # 4. ç°‡å¥åº·åº¦
    health_path = os.path.join(output_dir, "cluster_health.txt")
    with open(health_path, 'w') as f:
        f.write("Cluster_ID\tNum_Reads\tConsistency\tGT_Accuracy\tMatched_GT_Cluster\tStatus\n")
        for info in results['cluster_info']:
            gt_acc = info['gt_accuracy'] if info['gt_accuracy'] is not None else -1
            gt_cid = info['matched_gt_cid'] if info['matched_gt_cid'] is not None else -1
            f.write(f"{info['cluster_id']}\t{info['num_reads']}\t")
            f.write(f"{info['consistency']:.4f}\t{gt_acc:.4f}\t{gt_cid}\t{info['status']}\n")
    print(f"   âœ… å¥åº·åº¦: cluster_health.txt")
    
    # 5. è®­ç»ƒå†å²
    if training_history:
        history_path = os.path.join(output_dir, "training_history.txt")
        with open(history_path, 'w') as f:
            f.write("Epoch\tTotal_Loss\tContrastive\tDEL\tK_Constraint\tK_Effective\n")
            for i in range(len(training_history['total_loss'])):
                f.write(f"{i+1}\t{training_history['total_loss'][i]:.4f}\t")
                f.write(f"{training_history['contrastive_loss'][i]:.4f}\t")
                f.write(f"{training_history['del_loss'][i]:.4f}\t")
                f.write(f"{training_history['k_loss'][i]:.4f}\t")
                f.write(f"{training_history['k_effective'][i]}\n")
        print(f"   âœ… å†å²: training_history.txt")
        
        # ç»˜å›¾
        try:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            epochs = range(1, len(training_history['total_loss']) + 1)
            
            axes[0, 0].plot(epochs, training_history['total_loss'], 'b-')
            axes[0, 0].set_title('Total Loss')
            axes[0, 0].grid(True)
            
            axes[0, 1].plot(epochs, training_history['contrastive_loss'], 'r-', label='Contrastive')
            axes[0, 1].plot(epochs, training_history['del_loss'], 'g-', label='DEL')
            axes[0, 1].set_title('Loss Components')
            axes[0, 1].legend()
            axes[0, 1].grid(True)
            
            axes[1, 0].plot(epochs, training_history['k_loss'], 'm-')
            axes[1, 0].set_title('K Constraint Loss')
            axes[1, 0].grid(True)
            
            axes[1, 1].plot(epochs, training_history['k_effective'], 'c-')
            axes[1, 1].axhline(y=data_manager.config.k_target, color='r', linestyle='--', label=f'Target K={data_manager.config.k_target}')
            axes[1, 1].set_title('Effective Cluster Count')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, "training_history.png"), dpi=150)
            plt.close()
            print(f"   âœ… è®­ç»ƒæ›²çº¿: training_history.png")
        except Exception as e:
            print(f"   âš ï¸ ç»˜å›¾å¤±è´¥: {e}")
    
    # 6. å™ªå£°
    noise_path = os.path.join(output_dir, "noise_reads.txt")
    with open(noise_path, 'w') as f:
        f.write(f"# Total: {len(data_manager.noise_reads)}\n")
        for idx in sorted(data_manager.noise_reads):
            f.write(f"{idx}\n")
    print(f"   âœ… å™ªå£°: noise_reads.txt ({len(data_manager.noise_reads)} reads)")


# ============================================================================
# è®­ç»ƒ
# ============================================================================
def train(data_manager: ClusterDataManager,
         model: ClusterReconstructionModel,
         config: Config) -> Dict:
    """è®­ç»ƒä¸»å¾ªç¯"""
    
    device = config.device
    model = model.to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)
    
    contrastive_loss_fn = ContrastiveLoss()
    del_loss_fn = DELLoss()
    k_constraint_fn = StrictKConstraintLoss(config.k_target, config.lambda_k)
    
    history = {
        'total_loss': [],
        'contrastive_loss': [],
        'del_loss': [],
        'k_loss': [],
        'k_effective': []
    }
    
    print("\n" + "=" * 70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("=" * 70)
    
    for epoch in range(1, config.num_epochs + 1):
        dataset = ClusterDataset(data_manager)
        dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
        
        epoch_losses = {'total': 0, 'contrastive': 0, 'del': 0, 'k': 0}
        num_batches = 0
        
        model.train()
        
        for batch in dataloader:
            encodings = batch['encoding'].to(device)
            labels = batch['label'].to(device)
            
            optimizer.zero_grad()
            
            z, logits = model(encodings)
            
            loss_contrastive = contrastive_loss_fn(z, labels)
            loss_del = del_loss_fn(z, labels)
            
            total_loss = config.lambda_contrastive * loss_contrastive + config.lambda_del * loss_del
            
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_losses['total'] += total_loss.item()
            epoch_losses['contrastive'] += loss_contrastive.item()
            epoch_losses['del'] += loss_del.item()
            num_batches += 1
        
        scheduler.step()
        
        k_effective = data_manager.get_k_effective()
        loss_k = k_constraint_fn(k_effective, epoch, config.num_epochs)
        epoch_losses['k'] = loss_k.item()
        
        avg_total = epoch_losses['total'] / num_batches if num_batches > 0 else 0
        avg_contrastive = epoch_losses['contrastive'] / num_batches if num_batches > 0 else 0
        avg_del = epoch_losses['del'] / num_batches if num_batches > 0 else 0
        
        history['total_loss'].append(avg_total + epoch_losses['k'])
        history['contrastive_loss'].append(avg_contrastive)
        history['del_loss'].append(avg_del)
        history['k_loss'].append(epoch_losses['k'])
        history['k_effective'].append(k_effective)
        
        if epoch % 10 == 0 or epoch == 1:
            print(f"\n{'='*70}")
            print(f"ğŸ“ Epoch {epoch}/{config.num_epochs}")
            print(f"{'='*70}")
            print(f"\nğŸ“ˆ æŸå¤±:")
            print(f"   - Total: {avg_total + epoch_losses['k']:.4f}")
            print(f"   - Contrastive: {avg_contrastive:.4f}")
            print(f"   - DEL: {avg_del:.4f}")
            print(f"   - K-constraint: {epoch_losses['k']:.4f}")
        
        if epoch % 5 == 0:
            if epoch % 10 == 0:
                print(f"\nğŸ¥ ç°‡å¥åº·åº¦è¯„ä¼°...")
            healthy, weak = evaluate_cluster_health(data_manager, config)
            if epoch % 10 == 0:
                print(f"   - å¥åº·ç°‡: {len(healthy)}")
                print(f"   - å¼±ç°‡: {len(weak)}")
        
        if epoch % 10 == 0 and epoch > 10:
            print(f"\nğŸ”§ å›°éš¾æ ·æœ¬ä¿®æ­£...")
            reassigned, new_noise = mine_hard_samples(data_manager, model, config)
            print(f"   - é‡åˆ†é…: {reassigned}")
            print(f"   - æ–°å™ªå£°: {new_noise}")
        
        eliminate_freq = 10 if epoch < config.num_epochs * 0.7 else 5
        if epoch % eliminate_freq == 0 and epoch > 10:
            if epoch % 10 == 0:
                print(f"\nğŸ—‘ï¸ å¼±ç°‡æ·˜æ±°æ£€æŸ¥...")
            eliminated = eliminate_weak_clusters(data_manager, config, epoch, config.num_epochs)
            if eliminated > 0 and epoch % 10 != 0:
                print(f"   æ·˜æ±°äº† {eliminated} ä¸ªå¼±ç°‡")
        
        if epoch % 10 == 0:
            k_eff = data_manager.get_k_effective()
            noise_count = len(data_manager.noise_reads)
            noise_ratio = noise_count / data_manager.total_reads * 100
            
            print(f"\nğŸ“Š å½“å‰çŠ¶æ€:")
            print(f"   - K_effective: {k_eff} (ç›®æ ‡: {config.k_target})")
            print(f"   - å™ªå£°ç‡: {noise_ratio:.1f}%")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 70)
    
    k_final = data_manager.get_k_effective()
    healthy_final, weak_final = evaluate_cluster_health(data_manager, config)
    valid_reads = data_manager.total_reads - len(data_manager.noise_reads)
    avg_cluster_size = valid_reads / k_final if k_final > 0 else 0
    
    print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
    print(f"   - K_effective: {k_final}")
    print(f"   - K_healthy: {len(healthy_final)}")
    print(f"   - æœ‰æ•ˆReads: {valid_reads}")
    print(f"   - å™ªå£°Reads: {len(data_manager.noise_reads)} ({len(data_manager.noise_reads)/data_manager.total_reads*100:.1f}%)")
    print(f"   - å¹³å‡ç°‡å¤§å°: {avg_cluster_size:.1f}")
    
    return history


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    """ä¸»å‡½æ•°"""
    
    # ==========================================
    # é…ç½®è·¯å¾„ - ä¿®æ”¹è¿™é‡Œï¼
    # ==========================================
    # å®éªŒç›®å½• (åŒ…å« 01_RawData, 02_CloverOut, 03_FedDNA_In)
    EXPERIMENT_DIR = "CC/Step0/Experiments/20251217_015615_Cluster_GT_Test"
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = f"./results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # ==========================================
    # é…ç½®å‚æ•°
    # ==========================================
    config = Config(
        experiment_dir=EXPERIMENT_DIR,
        
        # æ¨¡å‹
        hidden_dim=128,
        latent_dim=64,
        num_heads=4,
        num_layers=3,
        dropout=0.1,
        
        # è®­ç»ƒ
        k_target=50,
        batch_size=64,
        num_epochs=50,
        learning_rate=1e-3,
        
        # æŸå¤±
        lambda_contrastive=1.0,
        lambda_del=0.5,
        lambda_k=2.0,
        
        # é˜ˆå€¼
        similarity_threshold=0.7,
        min_cluster_ratio=0.005,
        weak_consistency_threshold=0.6,
    )
    
    print("=" * 70)
    print("ğŸ§¬ FedDNA ç°‡åºåˆ—é‡å»ºç³»ç»Ÿ v3 (é€‚é…ç‰ˆ)")
    print("=" * 70)
    print(f"ğŸ“‚ å®éªŒç›®å½•: {EXPERIMENT_DIR}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ¯ ç›®æ ‡ç°‡æ•°K: {config.k_target}")
    print(f"ğŸ–¥ï¸ è®¾å¤‡: {config.device}")
    print("=" * 70)
    
    # ==========================================
    # æ£€æŸ¥ç›®å½•
    # ==========================================
    if not os.path.exists(EXPERIMENT_DIR):
        print(f"âŒ å®éªŒç›®å½•ä¸å­˜åœ¨: {EXPERIMENT_DIR}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼Œæˆ–å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬")
        return
    
    feddna_dir = os.path.join(EXPERIMENT_DIR, "03_FedDNA_In")
    if not os.path.exists(feddna_dir):
        print(f"âŒ FedDNAæ•°æ®ç›®å½•ä¸å­˜åœ¨: {feddna_dir}")
        return
    
    # ==========================================
    # åŠ è½½æ•°æ®
    # ==========================================
    try:
        data_manager = ClusterDataManager(EXPERIMENT_DIR, config)
    except FileNotFoundError as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # ==========================================
    # åˆ›å»ºæ¨¡å‹
    # ==========================================
    model = ClusterReconstructionModel(config)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ§  æ¨¡å‹å‚æ•°: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
    
    # ==========================================
    # è®­ç»ƒ
    # ==========================================
    training_history = train(data_manager, model, config)
    
    # ==========================================
    # åºåˆ—é‡å»º
    # ==========================================
    reconstructed = reconstruct_sequences(data_manager, model, config)
    
    # ==========================================
    # éªŒè¯ç»“æœ
    # ==========================================
    results = validate_results(reconstructed, data_manager)
    
    # ==========================================
    # ä¿å­˜ç»“æœ
    # ==========================================
    save_results(reconstructed, data_manager, results, OUTPUT_DIR, training_history)
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(OUTPUT_DIR, "model.pth")
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config.__dict__,
        'k_effective': data_manager.get_k_effective(),
    }, model_path)
    print(f"   âœ… æ¨¡å‹: model.pth")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
    print("=" * 70)
    
    return results


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='FedDNA ç°‡åºåˆ—é‡å»ºç³»ç»Ÿ v3')
    
    parser.add_argument('--experiment_dir', type=str, required=False,
                       help='å®éªŒç›®å½•è·¯å¾„ (åŒ…å«01_RawData, 03_FedDNA_Inç­‰)')
    parser.add_argument('--output_dir', type=str, required=False,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--k_target', type=int, default=50,
                       help='ç›®æ ‡ç°‡æ•°K')
    parser.add_argument('--num_epochs', type=int, default=50,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='æ‰¹å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--lambda_k', type=float, default=2.0,
                       help='Kçº¦æŸæƒé‡')
    
    return parser.parse_args()


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        args = parse_args()
        
        if args.experiment_dir:
            # æ›´æ–°ä¸»å‡½æ•°ä¸­çš„è·¯å¾„
            # è¿™é‡Œç®€å•å¤„ç†ï¼šç›´æ¥ä¿®æ”¹å…¨å±€å˜é‡æˆ–é‡æ–°è°ƒç”¨
            config = Config(
                experiment_dir=args.experiment_dir,
                k_target=args.k_target,
                num_epochs=args.num_epochs,
                batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                lambda_k=args.lambda_k,
            )
            
            output_dir = args.output_dir or f"./results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            print("=" * 70)
            print("ğŸ§¬ FedDNA ç°‡åºåˆ—é‡å»ºç³»ç»Ÿ v3 (å‘½ä»¤è¡Œæ¨¡å¼)")
            print("=" * 70)
            
            # åŠ è½½æ•°æ®
            data_manager = ClusterDataManager(args.experiment_dir, config)
            
            # åˆ›å»ºæ¨¡å‹
            model = ClusterReconstructionModel(config)
            
            # è®­ç»ƒ
            training_history = train(data_manager, model, config)
            
            # é‡å»º
            reconstructed = reconstruct_sequences(data_manager, model, config)
            
            # éªŒè¯
            results = validate_results(reconstructed, data_manager)
            
            # ä¿å­˜
            save_results(reconstructed, data_manager, results, output_dir, training_history)
            
            print("\nğŸ‰ å®Œæˆ!")
        else:
            print("è¯·æä¾› --experiment_dir å‚æ•°")
    else:
        # ç›´æ¥è¿è¡Œ
        main()

