import argparse
import os
import sys
import numpy as np

# è¿™ä¸€æ­¥æ˜¯ä¸ºäº†èƒ½ import ä½ çš„ models æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from models.step1_data import CloverDataLoader

def load_fasta(fasta_path):
    """ç®€å•çš„ FASTA è¯»å–å™¨"""
    sequences = {}
    current_header = None
    current_seq = []
    
    if not os.path.exists(fasta_path):
        print(f"âŒ File not found: {fasta_path}")
        return {}

    with open(fasta_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith(">"):
                if current_header:
                    sequences[current_header] = "".join(current_seq)
                current_header = line
                current_seq = []
            else:
                current_seq.append(line)
        if current_header:
            sequences[current_header] = "".join(current_seq)
    
    # è§£æ Header è·å– Cluster ID
    # Header æ ¼å¼: >cluster_0_reads49...
    parsed_seqs = {}
    for header, seq in sequences.items():
        try:
            # æå– cluster_ID
            parts = header.split('_')
            # å‡è®¾æ ¼å¼å›ºå®šä¸º >cluster_X_...
            if parts[0] == ">cluster":
                cluster_id = int(parts[1])
                parsed_seqs[cluster_id] = seq
        except Exception as e:
            print(f"âš ï¸ Warning: Could not parse header {header}: {e}")
            continue
            
    return parsed_seqs

def calculate_identity(seq1, seq2):
    """è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„ä¸€è‡´æ€§ (ç®€å• Hamming å˜ç§ï¼Œå‡è®¾é•¿åº¦å¯¹é½æˆ–å–æœ€å°)"""
    # å¦‚æœä½ ç”Ÿæˆçš„åºåˆ—é•¿åº¦å’Œ GT ä¸ä¸€æ ·ï¼Œå¯èƒ½éœ€è¦ Needleman-Wunsch ç®—æ³•
    # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ Step 1 å·²ç»æŠŠé•¿åº¦ Pad åˆ°äº† 150ï¼Œæˆ–è€…æˆ‘ä»¬åªæ¯”è¾ƒé‡å éƒ¨åˆ†
    
    min_len = min(len(seq1), len(seq2))
    max_len = max(len(seq1), len(seq2))
    
    if min_len == 0:
        return 0.0
        
    matches = sum(1 for a, b in zip(seq1[:min_len], seq2[:min_len]) if a == b)
    # æƒ©ç½šé•¿åº¦å·®å¼‚
    identity = matches / max_len
    return identity

def levenshtein_distance(s1, s2):
    """è®¡ç®—ç¼–è¾‘è·ç¦» (Levenshtein)"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    if len(s2) == 0:
        return len(s1)

    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def main():
    parser = argparse.ArgumentParser(description="Verify Reconstruction Accuracy")
    parser.add_argument('--experiment_dir', type=str, required=True, help="Data directory containing GT")
    parser.add_argument('--consensus_file', type=str, required=True, help="Generated consensus.fasta")
    args = parser.parse_args()

    print(f"\nğŸ” Verifying Accuracy...")
    print(f"   GT Source: {args.experiment_dir}")
    print(f"   Consensus: {args.consensus_file}")

    # 1. åŠ è½½ Ground Truth
    # åˆ©ç”¨ç°æœ‰çš„ DataLoader åŠ è½½ GT
    try:
        loader = CloverDataLoader(args.experiment_dir)
        gt_dict = loader.gt_cluster_seqs # {cluster_id: sequence}
        
        if not gt_dict:
            print("âŒ No Ground Truth clusters found in experiment dir!")
            return
            
        print(f"   âœ… Loaded {len(gt_dict)} GT sequences.")
    except Exception as e:
        print(f"âŒ Failed to load GT: {e}")
        return

    # 2. åŠ è½½é¢„æµ‹çš„ Consensus
    pred_dict = load_fasta(args.consensus_file)
    print(f"   âœ… Loaded {len(pred_dict)} Predicted consensus sequences.")

    # 3. æ¯”å¯¹
    metrics = {
        'identities': [],
        'edit_distances': [],
        'perfect_matches': 0,
        'missing_clusters': 0
    }
    
    print("\n   ğŸ“Š Detailed Comparison (Top 5 examples):")
    print("   " + "-"*60)
    
    count = 0
    for cid, gt_seq in gt_dict.items():
        if cid in pred_dict:
            pred_seq = pred_dict[cid]
            
            # è®¡ç®—æŒ‡æ ‡
            ident = calculate_identity(pred_seq, gt_seq)
            edit_dist = levenshtein_distance(pred_seq, gt_seq)
            
            metrics['identities'].append(ident)
            metrics['edit_distances'].append(edit_dist)
            
            if ident == 1.0 and len(pred_seq) == len(gt_seq):
                metrics['perfect_matches'] += 1
                
            # æ‰“å°å‰å‡ ä¸ªä¾‹å­
            if count < 5:
                print(f"   Cluster {cid}:")
                print(f"     GT  : {gt_seq[:50]}... (len={len(gt_seq)})")
                print(f"     PRED: {pred_seq[:50]}... (len={len(pred_seq)})")
                print(f"     -> Identity: {ident:.2%}, Edit Dist: {edit_dist}")
                count += 1
        else:
            metrics['missing_clusters'] += 1

    # 4. æ±‡æ€»æŠ¥å‘Š
    num_compared = len(metrics['identities'])
    if num_compared == 0:
        print("\nâŒ No common clusters found between GT and Prediction.")
        return

    avg_identity = sum(metrics['identities']) / num_compared
    avg_edit_dist = sum(metrics['edit_distances']) / num_compared
    perfect_rate = metrics['perfect_matches'] / num_compared

    print("\n" + "="*60)
    print("ğŸ† Verification Results")
    print("="*60)
    print(f"   Compared Clusters : {num_compared}")
    print(f"   Missing Clusters  : {metrics['missing_clusters']}")
    print(f"   ---------------------------")
    print(f"   âœ… Average Identity  : {avg_identity:.2%}  (Target: >99%)")
    print(f"   âœ… Avg Edit Distance : {avg_edit_dist:.2f}    (Target: <1.0)")
    print(f"   âœ… Perfect Matches   : {metrics['perfect_matches']} ({perfect_rate:.1%})")
    print("="*60 + "\n")

if __name__ == "__main__":
    main()