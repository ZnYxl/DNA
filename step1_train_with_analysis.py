import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch.nn.functional as F
from collections import defaultdict
import matplotlib.pyplot as plt
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
import seaborn as sns
import json
import pandas as pd
from datetime import datetime
import logging
import shutil

# ==========================================
# è¾“å‡ºç®¡ç†ç³»ç»Ÿ
# ==========================================

class ExperimentManager:
    """å®éªŒè¾“å‡ºç®¡ç†å™¨"""
    
    def __init__(self, experiment_name="DNA_Clustering", base_dir="outputs"):
        self.experiment_name = experiment_name
        self.base_dir = base_dir
        self.timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        
        # åˆ›å»ºå®éªŒæ–‡ä»¶å¤¹
        self.exp_dir = os.path.join(base_dir, f"{self.timestamp}_{experiment_name}")
        self.create_directories()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        print(f"ğŸ“ å®éªŒç›®å½•å·²åˆ›å»º: {self.exp_dir}")
        
    def create_directories(self):
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
        directories = [
            self.exp_dir,
            os.path.join(self.exp_dir, "model"),
            os.path.join(self.exp_dir, "visualizations"), 
            os.path.join(self.exp_dir, "results"),
            os.path.join(self.exp_dir, "logs")
        ]
        
        for dir_path in directories:
            os.makedirs(dir_path, exist_ok=True)
            
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = os.path.join(self.exp_dir, "logs", "training.log")
        
        # åˆ›å»ºlogger
        self.logger = logging.getLogger('experiment')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        self.logger.handlers.clear()
        
        # æ–‡ä»¶handler
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼åŒ–
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
    def log(self, message, level="info"):
        """è®°å½•æ—¥å¿—"""
        if level == "info":
            self.logger.info(message)
        elif level == "warning":
            self.logger.warning(message)
        elif level == "error":
            self.logger.error(message)
        print(message)  # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
        
    def save_config(self, config):
        """ä¿å­˜å®éªŒé…ç½®"""
        config_file = os.path.join(self.exp_dir, "config.json")
        
        # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
        full_config = {
            "experiment_info": {
                "name": self.experiment_name,
                "timestamp": self.timestamp,
                "directory": self.exp_dir
            },
            "model_config": config.get("model_config", {}),
            "training_config": config.get("training_config", {}),
            "refinement_config": config.get("refinement_config", {}),
            "data_config": config.get("data_config", {})
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(full_config, f, indent=2, ensure_ascii=False)
            
        self.log(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_file}")
        
    def save_model(self, model_dict, filename="refined_model.pth"):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.exp_dir, "model", filename)
        torch.save(model_dict, model_path)
        self.log(f"ğŸ¤– æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜æ¨¡å‹æ‘˜è¦
        summary_path = os.path.join(self.exp_dir, "model", "model_summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"æ¨¡å‹ä¿å­˜æ—¶é—´: {datetime.now()}\n")
            f.write(f"æ¨¡å‹æ–‡ä»¶: {filename}\n")
            f.write(f"æ¨¡å‹å¤§å°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\n")
            if 'config' in model_dict:
                f.write(f"æ¨¡å‹é…ç½®: {model_dict['config']}\n")
                
    def save_visualization(self, fig, filename, title=""):
        """ä¿å­˜å¯è§†åŒ–å›¾ç‰‡"""
        viz_path = os.path.join(self.exp_dir, "visualizations", filename)
        fig.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close(fig)  # å…³é—­å›¾ç‰‡é‡Šæ”¾å†…å­˜
        self.log(f"ğŸ“Š å›¾ç‰‡å·²ä¿å­˜: {viz_path} - {title}")
        
    def save_metrics(self, metrics, filename="metrics_summary.json"):
        """ä¿å­˜è¯„ä¼°æŒ‡æ ‡"""
        metrics_path = os.path.join(self.exp_dir, "results", filename)
        
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å¯åºåˆ—åŒ–çš„
        serializable_metrics = {}
        for key, value in metrics.items():
            if isinstance(value, (np.integer, np.floating)):
                serializable_metrics[key] = float(value)
            elif isinstance(value, np.ndarray):
                serializable_metrics[key] = value.tolist()
            else:
                serializable_metrics[key] = value
                
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_metrics, f, indent=2, ensure_ascii=False)
            
        self.log(f"ğŸ“ˆ æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")
        
    def save_cluster_assignments(self, true_labels, predicted_labels, confidences):
        """ä¿å­˜èšç±»åˆ†é…ç»“æœ"""
        results_df = pd.DataFrame({
            'sample_id': range(len(true_labels)),
            'true_cluster': true_labels,
            'predicted_cluster': predicted_labels,
            'confidence': confidences
        })
        
        csv_path = os.path.join(self.exp_dir, "results", "cluster_assignments.csv")
        results_df.to_csv(csv_path, index=False)
        self.log(f"ğŸ“‹ èšç±»ç»“æœå·²ä¿å­˜: {csv_path}")
        
    def generate_report(self, training_history, analysis_results):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report_path = os.path.join(self.exp_dir, "results", "analysis_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write(f"å®éªŒæŠ¥å‘Š: {self.experiment_name}\n")
            f.write(f"æ—¶é—´: {self.timestamp}\n")
            f.write("="*60 + "\n\n")
            
            # è®­ç»ƒæ€»ç»“
            f.write("ğŸš€ è®­ç»ƒæ€»ç»“:\n")
            f.write(f"   è®­ç»ƒè½®æ•°: {len(training_history['losses'])}\n")
            if training_history['losses']:
                f.write(f"   æœ€ç»ˆæŸå¤±: {training_history['losses'][-1]['total_loss']:.6f}\n")
                f.write(f"   æœ€ç»ˆä¿®æ­£æ¯”ä¾‹: {training_history['refinement_ratios'][-1]:.4f}\n")
                f.write(f"   æœ€ç»ˆç½®ä¿¡åº¦: {training_history['confidences'][-1]:.4f}\n")
            f.write("\n")
            
            # èšç±»è¯„ä¼°
            if analysis_results and 'metrics' in analysis_results:
                metrics = analysis_results['metrics']
                f.write("ğŸ“Š èšç±»è¯„ä¼°:\n")
                f.write(f"   ARI: {metrics.get('ARI', 0):.4f}\n")
                f.write(f"   NMI: {metrics.get('NMI', 0):.4f}\n")
                f.write(f"   Silhouette: {metrics.get('Silhouette', 0):.4f}\n")
                f.write(f"   çœŸå®ç°‡æ•°: {metrics.get('True_Clusters', 0)}\n")
                f.write(f"   é¢„æµ‹ç°‡æ•°: {metrics.get('Predicted_Clusters', 0)}\n")
                f.write("\n")
            
            # ç½®ä¿¡åº¦ç»Ÿè®¡
            if analysis_results and 'confidences' in analysis_results:
                confidences = analysis_results['confidences']
                f.write("ğŸ² ç½®ä¿¡åº¦ç»Ÿè®¡:\n")
                f.write(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.4f}\n")
                f.write(f"   ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(confidences):.4f}\n")
                f.write(f"   ç½®ä¿¡åº¦èŒƒå›´: [{np.min(confidences):.4f}, {np.max(confidences):.4f}]\n")
                f.write("\n")
            
            f.write("ğŸ“ è¾“å‡ºæ–‡ä»¶:\n")
            f.write(f"   æ¨¡å‹æ–‡ä»¶: model/refined_model.pth\n")
            f.write(f"   é…ç½®æ–‡ä»¶: config.json\n")
            f.write(f"   å¯è§†åŒ–: visualizations/\n")
            f.write(f"   è¯¦ç»†ç»“æœ: results/\n")
            
        self.log(f"ğŸ“„ å®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
    def create_readme(self, description=""):
        """åˆ›å»ºREADMEæ–‡ä»¶"""
        readme_path = os.path.join(self.exp_dir, "README.md")
        
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(f"# {self.experiment_name}\n\n")
            f.write(f"**å®éªŒæ—¶é—´:** {self.timestamp}\n\n")
            f.write(f"**æè¿°:** {description}\n\n")
            f.write("## æ–‡ä»¶ç»“æ„\n\n")
            f.write("```\n")
            f.write("â”œâ”€â”€ config.json                 # å®éªŒé…ç½®\n")
            f.write("â”œâ”€â”€ model/                      # æ¨¡å‹æ–‡ä»¶\n")
            f.write("â”‚   â”œâ”€â”€ refined_model.pth      # è®­ç»ƒå¥½çš„æ¨¡å‹\n")
            f.write("â”‚   â””â”€â”€ model_summary.txt      # æ¨¡å‹æ‘˜è¦\n")
            f.write("â”œâ”€â”€ visualizations/            # å¯è§†åŒ–ç»“æœ\n")
            f.write("â”‚   â”œâ”€â”€ clustering_analysis.png\n")
            f.write("â”‚   â”œâ”€â”€ confidence_distribution.png\n")
            f.write("â”‚   â””â”€â”€ training_curves.png\n")
            f.write("â”œâ”€â”€ results/                   # åˆ†æç»“æœ\n")
            f.write("â”‚   â”œâ”€â”€ metrics_summary.json\n")
            f.write("â”‚   â”œâ”€â”€ cluster_assignments.csv\n")
            f.write("â”‚   â””â”€â”€ analysis_report.txt\n")
            f.write("â”œâ”€â”€ logs/                      # æ—¥å¿—æ–‡ä»¶\n")
            f.write("â”‚   â””â”€â”€ training.log\n")
            f.write("â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶\n")
            f.write("```\n\n")
            f.write("## å¿«é€ŸæŸ¥çœ‹ç»“æœ\n\n")
            f.write("1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: `logs/training.log`\n")
            f.write("2. æŸ¥çœ‹èšç±»æ•ˆæœ: `visualizations/clustering_analysis.png`\n")
            f.write("3. æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: `results/analysis_report.txt`\n")
            f.write("4. åŠ è½½æ¨¡å‹: `torch.load('model/refined_model.pth')`\n")
            
        self.log(f"ğŸ“– READMEå·²åˆ›å»º: {readme_path}")

# ==========================================
# åŸæœ‰çš„æ‰€æœ‰ç±»ä¿æŒä¸å˜ (SimpleEncoder, ContrastiveLearningç­‰)
# ==========================================

# å®šä¹‰æƒé‡å‚æ•°
alpha = 1.0
beta = 0.01
gamma = 0.01

# å¯¼å…¥åŸºç¡€ç»„ä»¶
try:
    from models.conmamba import ConmambaBlock
    print("âœ… æˆåŠŸå¯¼å…¥ ConmambaBlock")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# [è¿™é‡Œæ’å…¥ä¹‹å‰çš„æ‰€æœ‰ç±»å®šä¹‰ï¼Œä¿æŒä¸å˜]
# SimpleEncoder, ContrastiveLearning, EvidenceDecoder, EvidenceFusion, 
# SimplifiedFedDNA, ComprehensiveLoss, CloverClusterDataset, 
# EvidenceRefinement, RefinementTrainer

class SimpleEncoder(nn.Module):
    """ç®€åŒ–çš„ç¼–ç å™¨ - åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½"""
    def __init__(self, input_dim=4, hidden_dim=64, seq_len=150):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # ç®€å•çš„ç‰¹å¾æå–å±‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # ConMambaå—ç”¨äºåºåˆ—å»ºæ¨¡
        self.conmamba = ConmambaBlock(dim=hidden_dim)
        
    def forward(self, x):
        # x: [B*N, L, 4] 
        x = self.feature_extractor(x)  # [B*N, L, hidden_dim]
        x = self.conmamba(x)           # [B*N, L, hidden_dim]
        return x

class ContrastiveLearning(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å—"""
    def __init__(self, hidden_dim=64, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
    
    def forward(self, embeddings):
        # embeddings: [B*N, L, hidden_dim]
        # å–å¹³å‡æ± åŒ–ä½œä¸ºåºåˆ—è¡¨ç¤º
        seq_repr = torch.mean(embeddings, dim=1)  # [B*N, hidden_dim]
        projected = self.projection(seq_repr)     # [B*N, hidden_dim//2]
        return F.normalize(projected, dim=-1)

class EvidenceDecoder(nn.Module):
    """è¯æ®è§£ç å™¨ - è¾“å‡ºEvidenceå‘é‡"""
    def __init__(self, hidden_dim=64, output_dim=4):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softplus()  # ç¡®ä¿è¾“å‡ºä¸ºæ­£å€¼ (Evidence)
        )
        
    def forward(self, x):
        # x: [B*N, L, hidden_dim]
        evidence = self.decoder(x)  # [B*N, L, 4]
        return evidence

class EvidenceFusion(nn.Module):
    """è¯æ®èåˆæ¨¡å—"""
    def __init__(self):
        super().__init__()
        
    def calculate_strength(self, evidence):
        """è®¡ç®—è¯æ®å¼ºåº¦ä½œä¸ºèåˆæƒé‡"""
        # evidence: [N, L, 4]
        strength = torch.sum(evidence, dim=-1, keepdim=True)  # [N, L, 1]
        return strength
    
    def forward(self, evidence_batch):
        """
        evidence_batch: [N, L, 4] - Næ¡readsçš„evidence
        """
        # è®¡ç®—æ¯æ¡readçš„è¯æ®å¼ºåº¦
        strengths = self.calculate_strength(evidence_batch)  # [N, L, 1]
        
        # åŠ æƒèåˆ
        weighted_evidence = evidence_batch * strengths       # [N, L, 4]
        fused_evidence = torch.sum(weighted_evidence, dim=0) # [L, 4]
        total_weight = torch.sum(strengths, dim=0)           # [L, 1]
        
        # é¿å…é™¤é›¶
        fused_evidence = fused_evidence / (total_weight + 1e-8)
        
        return fused_evidence, strengths

class SimplifiedFedDNA(nn.Module):
    """ç®€åŒ–ç‰ˆFedDNA - åªä¿ç•™æ ¸å¿ƒç»„ä»¶"""
    def __init__(self, input_dim=4, hidden_dim=64, seq_len=150):
        super().__init__()
        self.encoder = SimpleEncoder(input_dim, hidden_dim, seq_len)
        self.contrastive = ContrastiveLearning(hidden_dim)
        self.evidence_decoder = EvidenceDecoder(hidden_dim, input_dim)
        self.evidence_fusion = EvidenceFusion()
        
    def forward(self, reads_batch):
        """
        reads_batch: [B, N, L, 4] - Bä¸ªbatchï¼Œæ¯ä¸ªbatchæœ‰Næ¡reads
        """
        B, N, L, D = reads_batch.shape
        
        # é‡å¡‘ä¸º [B*N, L, D]
        reads_flat = reads_batch.view(B * N, L, D)
        
        # 1. ç¼–ç 
        embeddings = self.encoder(reads_flat)  # [B*N, L, hidden_dim]
        
        # 2. å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        contrastive_features = self.contrastive(embeddings)  # [B*N, hidden_dim//2]
        
        # 3. è¯æ®è§£ç 
        evidence = self.evidence_decoder(embeddings)  # [B*N, L, 4]
        
        # 4. é‡å¡‘å›batchå½¢å¼
        evidence = evidence.view(B, N, L, D)  # [B, N, L, 4]
        contrastive_features = contrastive_features.view(B, N, -1)  # [B, N, hidden_dim//2]
        
        return evidence, contrastive_features

class ComprehensiveLoss(nn.Module):
    """ç»¼åˆæŸå¤±å‡½æ•° - åŒ…å«é‡æ„ã€å¯¹æ¯”å­¦ä¹ ã€KLæ•£åº¦"""
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01, temperature=0.1):
        super().__init__()
        self.alpha = alpha      # é‡æ„æŸå¤±æƒé‡
        self.beta = beta        # å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡  
        self.gamma = gamma      # KLæ•£åº¦æŸå¤±æƒé‡
        self.temperature = temperature
        
    def contrastive_loss(self, features, cluster_labels=None):
        """
        å¯¹æ¯”å­¦ä¹ æŸå¤± - åŒç°‡å†…çš„readsåº”è¯¥ç›¸ä¼¼ï¼Œä¸åŒç°‡åº”è¯¥ä¸åŒ
        features: [N, feature_dim] - Næ¡readsçš„å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        """
        if features.shape[0] <= 1:
            return torch.tensor(0.0, device=features.device)
            
        # ç®€åŒ–ç‰ˆï¼šå‡è®¾åŒä¸€ä¸ªbatchå†…çš„readså±äºåŒä¸€ç°‡
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        features_norm = F.normalize(features, dim=1)
        similarity_matrix = torch.matmul(features_norm, features_norm.T) / self.temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬mask (åŒä¸€batchå†…ä¸ºæ­£æ ·æœ¬)
        batch_size = features.shape[0]
        mask = torch.eye(batch_size, device=features.device).bool()
        
        # è®¡ç®—InfoNCEæŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        exp_sim = exp_sim.masked_fill(mask, 0)  # ç§»é™¤è‡ªå·±å’Œè‡ªå·±çš„ç›¸ä¼¼åº¦
        
        # æ­£æ ·æœ¬ï¼šåŒbatchå†…å…¶ä»–æ ·æœ¬çš„å¹³å‡
        pos_sim = torch.sum(exp_sim, dim=1) / (batch_size - 1)
        # è´Ÿæ ·æœ¬ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨æ‰€æœ‰æ ·æœ¬
        neg_sim = torch.sum(exp_sim, dim=1)
        
        # InfoNCEæŸå¤±
        loss = -torch.log(pos_sim / (neg_sim + 1e-8))
        return torch.mean(loss)
    
    def kl_divergence_loss(self, evidence):
        """
        KLæ•£åº¦æŸå¤± - è¡¡é‡è¯æ®åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§
        evidence: [L, 4] - èåˆåçš„è¯æ®å‘é‡
        """
        # å°†evidenceè½¬æ¢ä¸ºDirichletåˆ†å¸ƒå‚æ•°
        alpha = evidence + 1  # [L, 4]
        
        # è®¡ç®—ä¸å‡åŒ€åˆ†å¸ƒçš„KLæ•£åº¦ (ç®€åŒ–ç‰ˆæœ¬)
        # ä½¿ç”¨è¯æ®çš„æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
        evidence_normalized = F.softmax(evidence, dim=-1)
        uniform_dist = torch.ones_like(evidence_normalized) / evidence_normalized.shape[-1]
        
        # KLæ•£åº¦: KL(P||Q) = sum(P * log(P/Q))
        kl_div = torch.sum(evidence_normalized * torch.log(evidence_normalized / (uniform_dist + 1e-8) + 1e-8), dim=-1)
        
        return torch.mean(kl_div)
    
    def forward(self, fused_evidence, ref, contrastive_features):
        """
        è®¡ç®—æ€»æŸå¤±
        fused_evidence: [L, 4] - èåˆåçš„è¯æ®
        ref: [L, 4] - å‚è€ƒåºåˆ—
        contrastive_features: [N, feature_dim] - å¯¹æ¯”å­¦ä¹ ç‰¹å¾
        """
        # 1. é‡æ„æŸå¤±
        reconstruction_loss = F.mse_loss(fused_evidence, ref)
        
        # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.contrastive_loss(contrastive_features)
        
        # 3. KLæ•£åº¦æŸå¤±
        kl_loss = self.kl_divergence_loss(fused_evidence)
        
        # 4. æ€»æŸå¤±
        total_loss = (self.alpha * reconstruction_loss + 
                     self.beta * contrastive_loss + 
                     self.gamma * kl_loss)
        
        return {
            'total_loss': total_loss,
            'reconstruction_loss': reconstruction_loss,
            'contrastive_loss': contrastive_loss,
            'kl_loss': kl_loss
        }

class CloverClusterDataset(Dataset):
    def __init__(self, data_dir, seq_len=150):
        self.seq_len = seq_len
        self.clusters = [] 
        
        read_path = os.path.join(data_dir, "read.txt")
        ref_path = os.path.join(data_dir, "ref.txt")
        if not os.path.exists(ref_path):
            ref_path = os.path.join(data_dir, "reference.txt")
            
        if not os.path.exists(read_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {read_path}")
            return
        
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®: {data_dir}")
        with open(ref_path, 'r') as f:
            refs = [line.strip() for line in f if line.strip()]
        with open(read_path, 'r') as f:
            content = f.read().strip()
        raw_clusters = content.split("===============================")
        
        for i, cluster_block in enumerate(raw_clusters):
            if not cluster_block.strip(): continue
            if i >= len(refs): break
            reads = [r.strip() for r in cluster_block.strip().split('\n') if r.strip()]
            if len(reads) > 0:
                self.clusters.append({'ref': refs[i], 'reads': reads})
                
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.clusters)} ä¸ªç°‡")

    def one_hot(self, seq):
        mapping = {'A':0, 'C':1, 'G':2, 'T':3}
        arr = np.zeros((self.seq_len, 4), dtype=np.float32)
        l = min(len(seq), self.seq_len)
        for i in range(l):
            char = seq[i]
            if char in mapping: arr[i, mapping[char]] = 1.0
        return arr

    def __len__(self): return len(self.clusters)

    def __getitem__(self, idx):
        cluster = self.clusters[idx]
        reads_vec = np.array([self.one_hot(r) for r in cluster['reads']])
        ref_vec = self.one_hot(cluster['ref'])
        return torch.tensor(reads_vec), torch.tensor(ref_vec)

class EvidenceRefinement(nn.Module):
    """è¯æ®é©±åŠ¨çš„å›°éš¾æ ·æœ¬ä¿®æ­£æ¨¡å—"""
    
    def __init__(self, confidence_threshold=0.5, distance_threshold=2.0):
        super().__init__()
        self.confidence_threshold = confidence_threshold
        self.distance_threshold = distance_threshold
        
    def calculate_confidence_score(self, evidence_strengths):
        """
        è®¡ç®—è¯æ®ç½®ä¿¡åº¦åˆ†æ•°
        evidence_strengths: [N, L, 1] - Næ¡readsçš„è¯æ®å¼ºåº¦
        è¿”å›: [N] - æ¯æ¡readçš„ç½®ä¿¡åº¦åˆ†æ•°
        """
        # æ–¹æ³•1: ä½¿ç”¨è¯æ®å¼ºåº¦çš„å¹³å‡å€¼ä½œä¸ºç½®ä¿¡åº¦
        confidence_scores = torch.mean(evidence_strengths.squeeze(-1), dim=1)  # [N]
        
        return confidence_scores
    
    def compute_cluster_centers(self, embeddings, labels, num_clusters):
        """
        è®¡ç®—ç°‡ä¸­å¿ƒ
        embeddings: [N, feature_dim] - readsçš„åµŒå…¥è¡¨ç¤º
        labels: [N] - å½“å‰æ ‡ç­¾
        num_clusters: int - ç°‡çš„æ•°é‡
        è¿”å›: [K, feature_dim] - Kä¸ªç°‡ä¸­å¿ƒ
        """
        device = embeddings.device
        feature_dim = embeddings.shape[1]
        centers = torch.zeros(num_clusters, feature_dim, device=device)
        
        for k in range(num_clusters):
            mask = (labels == k)
            if mask.sum() > 0:
                centers[k] = torch.mean(embeddings[mask], dim=0)
            else:
                # å¦‚æœæŸä¸ªç°‡ä¸ºç©ºï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–
                centers[k] = torch.randn(feature_dim, device=device)
                
        return centers
    
    def reassign_hard_samples(self, hard_embeddings, cluster_centers):
        """
        é‡åˆ†é…å›°éš¾æ ·æœ¬
        hard_embeddings: [M, feature_dim] - å›°éš¾æ ·æœ¬çš„åµŒå…¥
        cluster_centers: [K, feature_dim] - ç°‡ä¸­å¿ƒ
        è¿”å›: [M] - æ–°çš„æ ‡ç­¾åˆ†é… (-1è¡¨ç¤ºå™ªå£°)
        """
        if hard_embeddings.shape[0] == 0:
            return torch.tensor([], dtype=torch.long, device=hard_embeddings.device)
            
        # è®¡ç®—åˆ°æ‰€æœ‰ç°‡ä¸­å¿ƒçš„è·ç¦»
        distances = torch.cdist(hard_embeddings, cluster_centers)  # [M, K]
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç°‡
        min_distances, nearest_clusters = torch.min(distances, dim=1)  # [M]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°ï¼ˆè·ç¦»æ‰€æœ‰ç°‡éƒ½å¤ªè¿œï¼‰
        new_labels = nearest_clusters.clone()
        noise_mask = min_distances > self.distance_threshold
        new_labels[noise_mask] = -1  # æ ‡è®°ä¸ºå™ªå£°
        
        return new_labels, min_distances
    
    def forward(self, embeddings, evidence_strengths, current_labels, num_clusters):
        """
        æ‰§è¡Œå®Œæ•´çš„ä¿®æ­£æµç¨‹
        
        å‚æ•°:
        - embeddings: [N, feature_dim] - readsçš„åµŒå…¥è¡¨ç¤º
        - evidence_strengths: [N, L, 1] - è¯æ®å¼ºåº¦
        - current_labels: [N] - å½“å‰æ ‡ç­¾
        - num_clusters: int - ç°‡æ•°é‡
        
        è¿”å›:
        - new_labels: [N] - ä¿®æ­£åçš„æ ‡ç­¾
        - refinement_stats: dict - ä¿®æ­£ç»Ÿè®¡ä¿¡æ¯
        """
        N = embeddings.shape[0]
        device = embeddings.device
        
        # 1. è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        confidence_scores = self.calculate_confidence_score(evidence_strengths)
        
        # 2. é˜ˆå€¼åˆ¤æ–­ - è¯†åˆ«å›°éš¾æ ·æœ¬
        high_confidence_mask = confidence_scores > self.confidence_threshold
        hard_sample_mask = ~high_confidence_mask
        
        # 3. ä¿ç•™é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„æ ‡ç­¾
        new_labels = current_labels.clone()
        
        # 4. å¤„ç†å›°éš¾æ ·æœ¬
        if hard_sample_mask.sum() > 0:
            # è®¡ç®—å½“å‰ç°‡ä¸­å¿ƒï¼ˆåŸºäºé«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼‰
            high_conf_embeddings = embeddings[high_confidence_mask]
            high_conf_labels = current_labels[high_confidence_mask]
            
            if high_conf_embeddings.shape[0] > 0:
                cluster_centers = self.compute_cluster_centers(
                    high_conf_embeddings, high_conf_labels, num_clusters
                )
                
                # é‡åˆ†é…å›°éš¾æ ·æœ¬
                hard_embeddings = embeddings[hard_sample_mask]
                reassigned_labels, distances = self.reassign_hard_samples(
                    hard_embeddings, cluster_centers
                )
                
                # æ›´æ–°å›°éš¾æ ·æœ¬çš„æ ‡ç­¾
                new_labels[hard_sample_mask] = reassigned_labels
        
        # 5. ç»Ÿè®¡ä¿®æ­£ä¿¡æ¯
        refinement_stats = {
            'total_samples': N,
            'high_confidence_count': high_confidence_mask.sum().item(),
            'hard_samples_count': hard_sample_mask.sum().item(),
            'noise_samples_count': (new_labels == -1).sum().item(),
            'label_changes': (new_labels != current_labels).sum().item(),
            'refinement_ratio': (new_labels != current_labels).float().mean().item(),
            'avg_confidence': confidence_scores.mean().item(),
            'min_confidence': confidence_scores.min().item(),
            'max_confidence': confidence_scores.max().item()
        }
        
        return new_labels, refinement_stats

class RefinementTrainer:
    """åŒ…å«ä¿®æ­£é˜¶æ®µçš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, criterion, optimizer, refinement_module, 
                 convergence_threshold=0.01, max_epochs=10, exp_manager=None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.refinement = refinement_module
        self.convergence_threshold = convergence_threshold
        self.max_epochs = max_epochs
        self.exp_manager = exp_manager  # æ·»åŠ å®éªŒç®¡ç†å™¨
        
    def train_epoch_with_refinement(self, dataloader, device, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒ…å«ä¿®æ­£é˜¶æ®µ"""
        
        self.model.train()
        epoch_losses = {'total_loss': 0, 'reconstruction_loss': 0, 'contrastive_loss': 0, 'kl_loss': 0}
        all_refinement_stats = []
        step_count = 0
        
        if self.exp_manager:
            self.exp_manager.log(f"ğŸ”„ Epoch {epoch+1} - å¼€å§‹è®­ç»ƒ+ä¿®æ­£é˜¶æ®µ...")
        
        for i, (reads, ref) in enumerate(dataloader):
            reads = reads.to(device)  # [1, N, 150, 4]
            ref = ref.squeeze(0).to(device)  # [150, 4]
            N = reads.shape[1]
            
            # === æ­¥éª¤1: æ­£å¸¸è®­ç»ƒ ===
            self.optimizer.zero_grad()
            
            # Forward pass
            evidence_batch, contrastive_features = self.model(reads)
            
            # è¯æ®èåˆ
            evidence_single_batch = evidence_batch.squeeze(0)  # [N, 150, 4]
            fused_evidence, strengths = self.model.evidence_fusion(evidence_single_batch)
            
            # è®¡ç®—æŸå¤±
            contrastive_features_flat = contrastive_features.squeeze(0)  # [N, feature_dim]
            losses = self.criterion(fused_evidence, ref, contrastive_features_flat)
            
            # åå‘ä¼ æ’­
            losses['total_loss'].backward()
            self.optimizer.step()
            
            # === æ­¥éª¤2: å›°éš¾æ ·æœ¬ä¿®æ­£ ===
            with torch.no_grad():
                # å‡è®¾åˆå§‹æ ‡ç­¾éƒ½æ˜¯0ï¼ˆåŒä¸€ä¸ªç°‡ï¼‰
                current_labels = torch.zeros(N, dtype=torch.long, device=device)
                
                # æ‰§è¡Œä¿®æ­£
                new_labels, refinement_stats = self.refinement(
                    embeddings=contrastive_features_flat,
                    evidence_strengths=strengths.unsqueeze(-1),  # [N, L] -> [N, L, 1]
                    current_labels=current_labels,
                    num_clusters=1  # ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªbatchæ˜¯ä¸€ä¸ªç°‡
                )
                
                all_refinement_stats.append(refinement_stats)
            
            # è®°å½•æŸå¤±
            for key in epoch_losses:
                epoch_losses[key] += losses[key].item()
            step_count += 1
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 5 == 0:
                msg = (f"  ğŸ“Š Step {i+1:3d} | Loss: {losses['total_loss'].item():.4f} | "
                      f"ä¿®æ­£ç‡: {refinement_stats['refinement_ratio']:.3f} | "
                      f"ç½®ä¿¡åº¦: {refinement_stats['avg_confidence']:.3f} | "
                      f"å™ªå£°: {refinement_stats['noise_samples_count']}")
                if self.exp_manager:
                    self.exp_manager.log(msg)
                else:
                    print(msg)
        
        # è®¡ç®—epochç»Ÿè®¡
        avg_losses = {key: val / max(1, step_count) for key, val in epoch_losses.items()}
        
        # æ±‡æ€»ä¿®æ­£ç»Ÿè®¡
        if all_refinement_stats:
            avg_refinement_ratio = np.mean([s['refinement_ratio'] for s in all_refinement_stats])
            avg_confidence = np.mean([s['avg_confidence'] for s in all_refinement_stats])
            total_noise = sum([s['noise_samples_count'] for s in all_refinement_stats])
        else:
            avg_refinement_ratio = 0.0
            avg_confidence = 0.0
            total_noise = 0
            
        return avg_losses, {
            'refinement_ratio': avg_refinement_ratio,
            'avg_confidence': avg_confidence,
            'total_noise_samples': total_noise
        }
    
    def train_with_refinement(self, dataloader, device):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æ”¶æ•›åˆ¤æ–­"""
        
        if self.exp_manager:
            self.exp_manager.log("ğŸš€ å¼€å§‹è¯æ®é©±åŠ¨çš„ä¿®æ­£è®­ç»ƒ...")
            self.exp_manager.log(f"ğŸ“‹ é…ç½®: æ”¶æ•›é˜ˆå€¼={self.convergence_threshold}, æœ€å¤§è½®æ•°={self.max_epochs}")
        
        training_history = {
            'losses': [],
            'refinement_ratios': [],
            'confidences': [],
            'noise_counts': []
        }
        
        for epoch in range(self.max_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            avg_losses, refinement_stats = self.train_epoch_with_refinement(
                dataloader, device, epoch
            )
            
            # è®°å½•å†å²
            training_history['losses'].append(avg_losses)
            training_history['refinement_ratios'].append(refinement_stats['refinement_ratio'])
            training_history['confidences'].append(refinement_stats['avg_confidence'])
            training_history['noise_counts'].append(refinement_stats['total_noise_samples'])
            
            # æ‰“å°epochæ€»ç»“
            summary = (f"ğŸ“ˆ Epoch {epoch+1} å®Œæˆ:\n"
                      f"   æ€»æŸå¤±: {avg_losses['total_loss']:.6f}\n"
                      f"   ä¿®æ­£æ¯”ä¾‹: {refinement_stats['refinement_ratio']:.4f} ({refinement_stats['refinement_ratio']*100:.2f}%)\n"
                      f"   å¹³å‡ç½®ä¿¡åº¦: {refinement_stats['avg_confidence']:.4f}\n"
                      f"   å™ªå£°æ ·æœ¬æ•°: {refinement_stats['total_noise_samples']}")
            
            if self.exp_manager:
                self.exp_manager.log(summary)
            else:
                print(summary)
            
            # æ”¶æ•›åˆ¤æ–­
            if refinement_stats['refinement_ratio'] < self.convergence_threshold:
                convergence_msg = (f"âœ… æ”¶æ•›è¾¾æˆï¼ä¿®æ­£æ¯”ä¾‹ {refinement_stats['refinement_ratio']:.4f} < é˜ˆå€¼ {self.convergence_threshold}\n"
                                 f"ğŸ¯ è®­ç»ƒåœ¨ç¬¬ {epoch+1} è½®æ”¶æ•›")
                if self.exp_manager:
                    self.exp_manager.log(convergence_msg)
                else:
                    print(convergence_msg)
                break
            else:
                continue_msg = f"   ğŸ”„ ç»§ç»­è®­ç»ƒ (ä¿®æ­£æ¯”ä¾‹ {refinement_stats['refinement_ratio']:.4f} >= {self.convergence_threshold})"
                if self.exp_manager:
                    self.exp_manager.log(continue_msg)
                else:
                    print(continue_msg)
            
            if self.exp_manager:
                self.exp_manager.log("-" * 70)
        
        return training_history

# ==========================================
# å¢å¼ºçš„èšç±»åˆ†æå™¨ - é›†æˆè¾“å‡ºç®¡ç†
# ==========================================

class EnhancedClusteringAnalyzer:
    """å¢å¼ºçš„èšç±»ç»“æœåˆ†æå™¨ - é›†æˆè¾“å‡ºç®¡ç†"""
    
    def __init__(self, model, refinement_module, device, exp_manager):
        self.model = model
        self.refinement_module = refinement_module
        self.device = device
        self.exp_manager = exp_manager
        
    def extract_features_and_labels(self, dataloader):
        """æå–æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾"""
        self.model.eval()
        
        all_features = []
        all_cluster_ids = []
        all_confidences = []
        all_refined_labels = []
        
        with torch.no_grad():
            for cluster_id, (reads, ref) in enumerate(dataloader):
                reads = reads.to(self.device)
                N = reads.shape[1]
                
                # å‰å‘ä¼ æ’­
                evidence_batch, contrastive_features = self.model(reads)
                evidence_single_batch = evidence_batch.squeeze(0)
                fused_evidence, strengths = self.model.evidence_fusion(evidence_single_batch)
                
                # æå–ç‰¹å¾
                contrastive_features_flat = contrastive_features.squeeze(0)
                all_features.append(contrastive_features_flat.cpu())
                
                # çœŸå®æ ‡ç­¾ï¼ˆç°‡IDï¼‰
                true_labels = torch.full((N,), cluster_id, dtype=torch.long)
                all_cluster_ids.append(true_labels)
                
                # è®¡ç®—ç½®ä¿¡åº¦å’Œä¿®æ­£æ ‡ç­¾
                current_labels = torch.zeros(N, dtype=torch.long, device=self.device)
                refined_labels, refinement_stats = self.refinement_module(
                    embeddings=contrastive_features_flat,
                    evidence_strengths=strengths.unsqueeze(-1),
                    current_labels=current_labels,
                    num_clusters=1
                )
                
                all_confidences.extend([refinement_stats['avg_confidence']] * N)
                all_refined_labels.append(refined_labels.cpu())
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        features = torch.cat(all_features, dim=0).numpy()  # [total_samples, feature_dim]
        true_labels = torch.cat(all_cluster_ids, dim=0).numpy()  # [total_samples]
        refined_labels = torch.cat(all_refined_labels, dim=0).numpy()  # [total_samples]
        
        return features, true_labels, refined_labels, all_confidences
    
    def perform_clustering(self, features, n_clusters):
        """ä½¿ç”¨K-meansè¿›è¡Œèšç±»"""
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        predicted_labels = kmeans.fit_predict(features)
        return predicted_labels, kmeans.cluster_centers_
    
    def calculate_metrics(self, true_labels, predicted_labels, features):
        """è®¡ç®—èšç±»è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # ARI (Adjusted Rand Index) - è¶Šæ¥è¿‘1è¶Šå¥½
        metrics['ARI'] = adjusted_rand_score(true_labels, predicted_labels)
        
        # NMI (Normalized Mutual Information) - è¶Šæ¥è¿‘1è¶Šå¥½  
        metrics['NMI'] = normalized_mutual_info_score(true_labels, predicted_labels)
        
        # Silhouette Score - è¶Šæ¥è¿‘1è¶Šå¥½
        if len(np.unique(predicted_labels)) > 1:
            metrics['Silhouette'] = silhouette_score(features, predicted_labels)
        else:
            metrics['Silhouette'] = -1
            
        # ç°‡æ•°é‡å¯¹æ¯”
        metrics['True_Clusters'] = len(np.unique(true_labels))
        metrics['Predicted_Clusters'] = len(np.unique(predicted_labels))
        
        return metrics
    
    def visualize_clustering(self, features, true_labels, predicted_labels):
        """å¯è§†åŒ–èšç±»ç»“æœ"""
        
        self.exp_manager.log("ğŸ”„ æ­£åœ¨è¿›è¡Œt-SNEé™ç»´...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1))
        features_2d = tsne.fit_transform(features)
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # 1. çœŸå®æ ‡ç­¾
        scatter1 = axes[0].scatter(features_2d[:, 0], features_2d[:, 1], 
                                  c=true_labels, cmap='tab20', alpha=0.7, s=20)
        axes[0].set_title(f'çœŸå®æ ‡ç­¾ ({len(np.unique(true_labels))} ä¸ªç°‡)', fontsize=14)
        axes[0].set_xlabel('t-SNE 1')
        axes[0].set_ylabel('t-SNE 2')
        plt.colorbar(scatter1, ax=axes[0])
        
        # 2. é¢„æµ‹æ ‡ç­¾  
        scatter2 = axes[1].scatter(features_2d[:, 0], features_2d[:, 1], 
                                  c=predicted_labels, cmap='tab20', alpha=0.7, s=20)
        axes[1].set_title(f'é¢„æµ‹æ ‡ç­¾ ({len(np.unique(predicted_labels))} ä¸ªç°‡)', fontsize=14)
        axes[1].set_xlabel('t-SNE 1')
        axes[1].set_ylabel('t-SNE 2')
        plt.colorbar(scatter2, ax=axes[1])
        
        # 3. æ ‡ç­¾ä¸€è‡´æ€§ï¼ˆç»¿è‰²=ä¸€è‡´ï¼Œçº¢è‰²=ä¸ä¸€è‡´ï¼‰
        consistency = (true_labels == predicted_labels).astype(int)
        scatter3 = axes[2].scatter(features_2d[:, 0], features_2d[:, 1], 
                                  c=consistency, cmap='RdYlGn', alpha=0.7, s=20)
        axes[2].set_title(f'æ ‡ç­¾ä¸€è‡´æ€§ ({np.mean(consistency)*100:.1f}% ä¸€è‡´)', fontsize=14)
        axes[2].set_xlabel('t-SNE 1')
        axes[2].set_ylabel('t-SNE 2')
        plt.colorbar(scatter3, ax=axes[2])
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        self.exp_manager.save_visualization(fig, "clustering_analysis.png", "èšç±»åˆ†æç»“æœ")
    
    def visualize_confidence_distribution(self, confidences):
        """åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒ"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        axes[0].hist(confidences, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0].axvline(np.mean(confidences), color='red', linestyle='--', 
                       label=f'å¹³å‡å€¼: {np.mean(confidences):.3f}')
        axes[0].axvline(self.refinement_module.confidence_threshold, color='orange', linestyle='--',
                       label=f'é˜ˆå€¼: {self.refinement_module.confidence_threshold}')
        axes[0].set_xlabel('ç½®ä¿¡åº¦')
        axes[0].set_ylabel('é¢‘æ¬¡')
        axes[0].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        axes[1].boxplot(confidences)
        axes[1].set_ylabel('ç½®ä¿¡åº¦')
        axes[1].set_title('ç½®ä¿¡åº¦ç®±çº¿å›¾')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        self.exp_manager.save_visualization(fig, "confidence_distribution.png", "ç½®ä¿¡åº¦åˆ†å¸ƒåˆ†æ")
    
    def plot_training_curves(self, training_history):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        epochs = range(1, len(training_history['losses']) + 1)
        total_losses = [loss['total_loss'] for loss in training_history['losses']]
        recon_losses = [loss['reconstruction_loss'] for loss in training_history['losses']]
        
        axes[0, 0].plot(epochs, total_losses, 'b-', label='æ€»æŸå¤±', linewidth=2)
        axes[0, 0].plot(epochs, recon_losses, 'r--', label='é‡æ„æŸå¤±', linewidth=2)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # ä¿®æ­£æ¯”ä¾‹æ›²çº¿
        axes[0, 1].plot(epochs, training_history['refinement_ratios'], 'g-', linewidth=2)
        axes[0, 1].axhline(y=0.05, color='r', linestyle='--', alpha=0.7, label='æ”¶æ•›é˜ˆå€¼')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('ä¿®æ­£æ¯”ä¾‹')
        axes[0, 1].set_title('ä¿®æ­£æ¯”ä¾‹å˜åŒ–')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # ç½®ä¿¡åº¦æ›²çº¿
        axes[1, 0].plot(epochs, training_history['confidences'], 'purple', linewidth=2)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('å¹³å‡ç½®ä¿¡åº¦')
        axes[1, 0].set_title('ç½®ä¿¡åº¦å˜åŒ–')
        axes[1, 0].grid(True, alpha=0.3)
        
        # å™ªå£°æ ·æœ¬æ•°æ›²çº¿
        axes[1, 1].plot(epochs, training_history['noise_counts'], 'orange', linewidth=2)
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('å™ªå£°æ ·æœ¬æ•°')
        axes[1, 1].set_title('å™ªå£°æ ·æœ¬æ•°å˜åŒ–')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        self.exp_manager.save_visualization(fig, "training_curves.png", "è®­ç»ƒè¿‡ç¨‹æ›²çº¿")
    
    def full_analysis(self, dataloader, training_history, expected_clusters=50):
        """å®Œæ•´çš„èšç±»åˆ†æ"""
        self.exp_manager.log("ğŸ” å¼€å§‹èšç±»ç»“æœåˆ†æ...")
        self.exp_manager.log("=" * 60)
        
        # 1. æå–ç‰¹å¾å’Œæ ‡ç­¾
        self.exp_manager.log("ğŸ“Š æå–ç‰¹å¾å’Œæ ‡ç­¾...")
        features, true_labels, refined_labels, confidences = self.extract_features_and_labels(dataloader)
        
        self.exp_manager.log(f"   æ€»æ ·æœ¬æ•°: {len(features)}")
        self.exp_manager.log(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]}")
        self.exp_manager.log(f"   çœŸå®ç°‡æ•°: {len(np.unique(true_labels))}")
        
        # 2. ä½¿ç”¨K-meansé‡æ–°èšç±»
        self.exp_manager.log(f"ğŸ¯ ä½¿ç”¨K-meansè¿›è¡Œ {expected_clusters} ç°‡èšç±»...")
        predicted_labels, cluster_centers = self.perform_clustering(features, expected_clusters)
        
        # 3. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        self.exp_manager.log("ğŸ“ˆ è®¡ç®—èšç±»è¯„ä¼°æŒ‡æ ‡...")
        metrics = self.calculate_metrics(true_labels, predicted_labels, features)
        
        metrics_summary = (f"ğŸ“Š èšç±»è´¨é‡è¯„ä¼°:\n"
                          f"   ARI (è°ƒæ•´å…°å¾·æŒ‡æ•°):     {metrics['ARI']:.4f}\n"
                          f"   NMI (æ ‡å‡†åŒ–äº’ä¿¡æ¯):     {metrics['NMI']:.4f}\n"
                          f"   Silhouette Score:      {metrics['Silhouette']:.4f}\n"
                          f"   çœŸå®ç°‡æ•°:              {metrics['True_Clusters']}\n"
                          f"   é¢„æµ‹ç°‡æ•°:              {metrics['Predicted_Clusters']}")
        self.exp_manager.log(metrics_summary)
        
        # 4. åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_stats = (f"ğŸ² ç½®ä¿¡åº¦ç»Ÿè®¡:\n"
                           f"   å¹³å‡ç½®ä¿¡åº¦:            {np.mean(confidences):.4f}\n"
                           f"   ç½®ä¿¡åº¦æ ‡å‡†å·®:          {np.std(confidences):.4f}\n"
                           f"   æœ€å°ç½®ä¿¡åº¦:            {np.min(confidences):.4f}\n"
                           f"   æœ€å¤§ç½®ä¿¡åº¦:            {np.max(confidences):.4f}\n"
                           f"   ä½äºé˜ˆå€¼çš„æ ·æœ¬æ¯”ä¾‹:     {np.mean(np.array(confidences) < self.refinement_module.confidence_threshold)*100:.2f}%")
        self.exp_manager.log(confidence_stats)
        
        # 5. ç”Ÿæˆå¯è§†åŒ–
        self.exp_manager.log("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        self.visualize_clustering(features, true_labels, predicted_labels)
        self.visualize_confidence_distribution(confidences)
        self.plot_training_curves(training_history)
        
        # 6. ä¿å­˜æ•°æ®
        self.exp_manager.save_metrics(metrics)
        self.exp_manager.save_cluster_assignments(true_labels, predicted_labels, confidences)
        
        # 7. ç»™å‡ºæ”¹è¿›å»ºè®®
        suggestions = []
        if metrics['ARI'] < 0.3:
            suggestions.append("âš ï¸  ARIè¿‡ä½ï¼Œæ¨¡å‹èšç±»æ•ˆæœå·®")
            suggestions.append("å»ºè®®: é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå¢åŠ è®­ç»ƒè½®æ•°")
        if np.mean(confidences) > 1.0:
            suggestions.append("âš ï¸  ç½®ä¿¡åº¦è¿‡é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
            suggestions.append("å»ºè®®: è°ƒæ•´è¯æ®å¼ºåº¦è®¡ç®—æ–¹æ³•")
        if np.mean(np.array(confidences) < self.refinement_module.confidence_threshold) < 0.1:
            suggestions.append("âš ï¸  å‡ ä¹æ²¡æœ‰å›°éš¾æ ·æœ¬ï¼Œä¿®æ­£æ¨¡å—æœªå‘æŒ¥ä½œç”¨")
            suggestions.append("å»ºè®®: é™ä½ç½®ä¿¡åº¦é˜ˆå€¼åˆ°0.1-0.2")
            
        if suggestions:
            self.exp_manager.log("ğŸ’¡ æ”¹è¿›å»ºè®®:")
            for suggestion in suggestions:
                self.exp_manager.log(f"   {suggestion}")
            
        return {
            'features': features,
            'true_labels': true_labels, 
            'predicted_labels': predicted_labels,
            'metrics': metrics,
            'confidences': confidences
        }

# ==========================================
# ä¸»è®­ç»ƒå‡½æ•° - å®Œæ•´çš„å®éªŒç®¡ç†ç‰ˆæœ¬
# ==========================================

def train_with_full_management():
    """å®Œæ•´çš„å®éªŒç®¡ç†ç‰ˆæœ¬è®­ç»ƒå‡½æ•°"""
    
    # å®éªŒé…ç½®
    experiment_name = "HighIndel_DNA_Clustering"
    DATA_DIR = "Dataset/CloverExp/train"
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    exp_manager = ExperimentManager(experiment_name)
    
    # è®­ç»ƒå‚æ•°
    input_dim = 4
    hidden_dim = 64
    seq_len = 150
    lr = 1e-3
    
    # æŸå¤±æƒé‡
    alpha = 1.0
    beta = 0.01
    gamma = 0.01
    
    # ä¿®æ­£å‚æ•°
    confidence_threshold = 0.15
    distance_threshold = 1.0
    convergence_threshold = 0.05
    max_epochs = 10
    
    # ä¿å­˜é…ç½®
    config = {
        "model_config": {
            "input_dim": input_dim,
            "hidden_dim": hidden_dim,
            "seq_len": seq_len,
        },
        "training_config": {
            "learning_rate": lr,
            "loss_weights": {"alpha": alpha, "beta": beta, "gamma": gamma},
            "max_epochs": max_epochs,
            "device": str(DEVICE)
        },
        "refinement_config": {
            "confidence_threshold": confidence_threshold,
            "distance_threshold": distance_threshold,
            "convergence_threshold": convergence_threshold
        },
        "data_config": {
            "data_dir": DATA_DIR,
            "seq_length": seq_len
        }
    }
    
    exp_manager.save_config(config)
    
    try:
        # æ£€æŸ¥æ•°æ®ç›®å½•
        if not os.path.exists(DATA_DIR):
            exp_manager.log(f"âŒ ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}", "error")
            return None, None
            
        # åŠ è½½æ•°æ®
        dataset = CloverClusterDataset(DATA_DIR)
        if len(dataset) == 0:
            exp_manager.log("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶", "error")
            return None, None
            
        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = SimplifiedFedDNA(input_dim, hidden_dim, seq_len).to(DEVICE)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = ComprehensiveLoss(alpha=alpha, beta=beta, gamma=gamma)
        
        # åˆå§‹åŒ–ä¿®æ­£æ¨¡å—
        refinement_module = EvidenceRefinement(
            confidence_threshold=confidence_threshold,
            distance_threshold=distance_threshold
        ).to(DEVICE)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RefinementTrainer(
            model=model,
            criterion=criterion,
            optimizer=optimizer,
            refinement_module=refinement_module,
            convergence_threshold=convergence_threshold,
            max_epochs=max_epochs,
            exp_manager=exp_manager
        )
        
        exp_manager.log(f"ğŸ”§ æ¨¡å‹é…ç½®:")
        exp_manager.log(f"   è®¾å¤‡: {DEVICE}")
        exp_manager.log(f"   æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªç°‡")
        exp_manager.log(f"   æŸå¤±æƒé‡: é‡æ„={alpha}, å¯¹æ¯”å­¦ä¹ ={beta}, KLæ•£åº¦={gamma}")
        exp_manager.log(f"   ä¿®æ­£å‚æ•°: ç½®ä¿¡åº¦é˜ˆå€¼={confidence_threshold}, è·ç¦»é˜ˆå€¼={distance_threshold}")
        exp_manager.log(f"   æ”¶æ•›æ¡ä»¶: ä¿®æ­£æ¯”ä¾‹ < {convergence_threshold*100}%")
        
        # å¼€å§‹è®­ç»ƒ
        training_history = trainer.train_with_refinement(dataloader, DEVICE)
        
        # èšç±»ç»“æœåˆ†æ
        exp_manager.log("\n" + "="*60)
        analyzer = EnhancedClusteringAnalyzer(model, refinement_module, DEVICE, exp_manager)
        analysis_results = analyzer.full_analysis(dataloader, training_history, expected_clusters=50)
        
        # ä¿å­˜æ¨¡å‹
        save_dict = {
            'model_state_dict': model.state_dict(),
            'refinement_state_dict': refinement_module.state_dict(),
            'training_history': training_history,
            'analysis_results': analysis_results,
            'config': config
        }
        
        exp_manager.save_model(save_dict)
        
        # ç”ŸæˆæŠ¥å‘Š
        exp_manager.generate_report(training_history, analysis_results)
        exp_manager.create_readme(f"é«˜æ’å…¥ç¼ºå¤±DNAèšç±»å®éªŒ - {len(dataset)}ä¸ªç°‡ï¼Œ{sum(len(cluster['reads']) for cluster in dataset.clusters)}æ¡reads")
        
        exp_manager.log(f"ğŸ‰ å®éªŒå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {exp_manager.exp_dir}")
        exp_manager.log("ğŸ“‹ å¿«é€ŸæŸ¥çœ‹ç»“æœ:")
        exp_manager.log(f"   - è®­ç»ƒæ—¥å¿—: logs/training.log")
        exp_manager.log(f"   - èšç±»æ•ˆæœ: visualizations/clustering_analysis.png")
        exp_manager.log(f"   - è¯¦ç»†æŠ¥å‘Š: results/analysis_report.txt")
        exp_manager.log(f"   - æ¨¡å‹æ–‡ä»¶: model/refined_model.pth")
        
        return training_history, analysis_results, exp_manager.exp_dir
        
    except Exception as e:
        exp_manager.log(f"âŒ è®­ç»ƒå¤±è´¥: {e}", "error")
        import traceback
        error_details = traceback.format_exc()
        exp_manager.log(f"é”™è¯¯è¯¦æƒ…:\n{error_details}", "error")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶
        error_file = os.path.join(exp_manager.exp_dir, "error_log.txt")
        with open(error_file, 'w', encoding='utf-8') as f:
            f.write(f"å®éªŒå¤±è´¥æ—¶é—´: {datetime.now()}\n")
            f.write(f"é”™è¯¯ä¿¡æ¯: {e}\n\n")
            f.write("è¯¦ç»†é”™è¯¯å †æ ˆ:\n")
            f.write(error_details)
        
        return None, None, exp_manager.exp_dir

# ==========================================
# å®éªŒç»“æœæŸ¥çœ‹å™¨
# ==========================================

class ExperimentViewer:
    """å®éªŒç»“æœæŸ¥çœ‹å™¨"""
    
    def __init__(self, exp_dir):
        self.exp_dir = exp_dir
        
    def list_experiments(self, base_dir="outputs"):
        """åˆ—å‡ºæ‰€æœ‰å®éªŒ"""
        if not os.path.exists(base_dir):
            print(f"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return []
            
        experiments = []
        for item in os.listdir(base_dir):
            item_path = os.path.join(base_dir, item)
            if os.path.isdir(item_path):
                config_file = os.path.join(item_path, "config.json")
                if os.path.exists(config_file):
                    experiments.append(item)
                    
        experiments.sort(reverse=True)  # æŒ‰æ—¶é—´å€’åº
        return experiments
    
    def show_experiment_summary(self):
        """æ˜¾ç¤ºå®éªŒæ‘˜è¦"""
        config_file = os.path.join(self.exp_dir, "config.json")
        report_file = os.path.join(self.exp_dir, "results", "analysis_report.txt")
        
        print(f"ğŸ“ å®éªŒç›®å½•: {self.exp_dir}")
        print("=" * 60)
        
        # è¯»å–é…ç½®
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            exp_info = config.get("experiment_info", {})
            print(f"ğŸ·ï¸  å®éªŒåç§°: {exp_info.get('name', 'Unknown')}")
            print(f"â° å®éªŒæ—¶é—´: {exp_info.get('timestamp', 'Unknown')}")
            
            model_config = config.get("model_config", {})
            print(f"ğŸ¤– æ¨¡å‹é…ç½®: éšè—ç»´åº¦={model_config.get('hidden_dim', 'Unknown')}")
            
            training_config = config.get("training_config", {})
            print(f"ğŸš€ è®­ç»ƒé…ç½®: å­¦ä¹ ç‡={training_config.get('learning_rate', 'Unknown')}, æœ€å¤§è½®æ•°={training_config.get('max_epochs', 'Unknown')}")
        
        # è¯»å–æŠ¥å‘Š
        if os.path.exists(report_file):
            print("\nğŸ“Š å®éªŒç»“æœ:")
            with open(report_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                in_metrics = False
                for line in lines:
                    line = line.strip()
                    if "èšç±»è¯„ä¼°:" in line:
                        in_metrics = True
                        continue
                    elif in_metrics and line.startswith("   "):
                        print(f"  {line}")
                    elif in_metrics and not line.startswith("   "):
                        break
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        print(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
        files_to_check = [
            ("æ¨¡å‹æ–‡ä»¶", "model/refined_model.pth"),
            ("èšç±»åˆ†æå›¾", "visualizations/clustering_analysis.png"),
            ("ç½®ä¿¡åº¦åˆ†å¸ƒå›¾", "visualizations/confidence_distribution.png"),
            ("è®­ç»ƒæ›²çº¿å›¾", "visualizations/training_curves.png"),
            ("è¯¦ç»†æŠ¥å‘Š", "results/analysis_report.txt"),
            ("èšç±»ç»“æœ", "results/cluster_assignments.csv")
        ]
        
        for name, path in files_to_check:
            full_path = os.path.join(self.exp_dir, path)
            status = "âœ…" if os.path.exists(full_path) else "âŒ"
            print(f"  {status} {name}: {path}")
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_file = os.path.join(self.exp_dir, "model", "refined_model.pth")
        if not os.path.exists(model_file):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
            return None
            
        try:
            model_dict = torch.load(model_file, map_location='cpu')
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_file}")
            return model_dict
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    
    def show_metrics(self):
        """æ˜¾ç¤ºè¯¦ç»†æŒ‡æ ‡"""
        metrics_file = os.path.join(self.exp_dir, "results", "metrics_summary.json")
        if not os.path.exists(metrics_file):
            print(f"âŒ æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {metrics_file}")
            return
            
        with open(metrics_file, 'r', encoding='utf-8') as f:
            metrics = json.load(f)
            
        print("ğŸ“ˆ è¯¦ç»†è¯„ä¼°æŒ‡æ ‡:")
        print("=" * 40)
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
            else:
                print(f"  {key}: {value}")

def list_all_experiments():
    """åˆ—å‡ºæ‰€æœ‰å®éªŒ"""
    base_dir = "outputs"
    if not os.path.exists(base_dir):
        print(f"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {base_dir}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œå®éªŒç”Ÿæˆç»“æœ")
        return
        
    experiments = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            config_file = os.path.join(item_path, "config.json")
            if os.path.exists(config_file):
                experiments.append(item)
                
    if not experiments:
        print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
        return
        
    experiments.sort(reverse=True)  # æŒ‰æ—¶é—´å€’åº
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(experiments)} ä¸ªå®éªŒ:")
    print("=" * 80)
    
    for i, exp in enumerate(experiments, 1):
        exp_path = os.path.join(base_dir, exp)
        config_file = os.path.join(exp_path, "config.json")
        
        # è¯»å–åŸºæœ¬ä¿¡æ¯
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            exp_info = config.get("experiment_info", {})
            name = exp_info.get("name", "Unknown")
            timestamp = exp_info.get("timestamp", "Unknown")
        except:
            name = "Unknown"
            timestamp = "Unknown"
            
        print(f"{i:2d}. {exp}")
        print(f"    åç§°: {name}")
        print(f"    æ—¶é—´: {timestamp}")
        print(f"    è·¯å¾„: {exp_path}")
        print()

def view_experiment(exp_name=None):
    """æŸ¥çœ‹æŒ‡å®šå®éªŒ"""
    if exp_name is None:
        # æ˜¾ç¤ºæ‰€æœ‰å®éªŒè®©ç”¨æˆ·é€‰æ‹©
        list_all_experiments()
        return
        
    exp_path = os.path.join("outputs", exp_name)
    if not os.path.exists(exp_path):
        print(f"âŒ å®éªŒä¸å­˜åœ¨: {exp_path}")
        return
        
    viewer = ExperimentViewer(exp_path)
    viewer.show_experiment_summary()
    
    print(f"\nğŸ’¡ æŸ¥çœ‹è¯¦ç»†ç»“æœ:")
    print(f"   viewer = ExperimentViewer('{exp_path}')")
    print(f"   viewer.show_metrics()  # æŸ¥çœ‹è¯¦ç»†æŒ‡æ ‡")
    print(f"   viewer.load_model()    # åŠ è½½æ¨¡å‹")

# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="DNAèšç±»å®éªŒç®¡ç†ç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["train", "list", "view"], default="train",
                       help="è¿è¡Œæ¨¡å¼: train=è®­ç»ƒ, list=åˆ—å‡ºå®éªŒ, view=æŸ¥çœ‹å®éªŒ")
    parser.add_argument("--exp_name", type=str, help="å®éªŒåç§° (ç”¨äºviewæ¨¡å¼)")
    
    args = parser.parse_args()
    
    if args.mode == "train":
        print("ğŸ¯ å¯åŠ¨å®Œæ•´è®­ç»ƒ+åˆ†ææ¨¡å¼...")
        train_history, analysis_results, exp_dir = train_with_full_management()
        
        if train_history is not None:
            print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼å®éªŒç»“æœä¿å­˜åœ¨: {exp_dir}")
            print(f"ğŸ’¡ æŸ¥çœ‹ç»“æœ: python {__file__} --mode view --exp_name {os.path.basename(exp_dir)}")
        else:
            print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯æ—¥å¿—")
            
    elif args.mode == "list":
        list_all_experiments()
        
    elif args.mode == "view":
        view_experiment(args.exp_name)
        
    else:
        print("âŒ æœªçŸ¥æ¨¡å¼ï¼Œè¯·ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")
