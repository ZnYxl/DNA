#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§¬ è¯æ®é©±åŠ¨çš„DNAèšç±»ä¼˜åŒ–æ¨¡å‹ (Evidence-Driven Clustering)

æ ¸å¿ƒæ€æƒ³:
1. ä¸çº æ­£ä»£è¡¨åºåˆ—ï¼Œè€Œæ˜¯ä¼˜åŒ–readsçš„ç°‡åˆ†é…
2. åŸºäºè¯æ®å¼ºåº¦è¯†åˆ«å›°éš¾æ ·æœ¬
3. æ¸è¿›å¼ä¼˜åŒ–ï¼Œç¨³å®šæ”¶æ•›

ç°‡æ•°é‡ç­–ç•¥:
- åˆå§‹Kæ¥è‡ªClover
- è®­ç»ƒä¸­å…è®¸: åˆ é™¤ç©ºç°‡ã€ä¸¢å¼ƒå™ªå£°
- ä¿æŒKç›¸å¯¹ç¨³å®š
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ==========================================
# å¯¼å…¥FedDNAæ ¸å¿ƒç»„ä»¶
# ==========================================
try:
    from models.conmamba import ConmambaBlock
    print("âœ… æˆåŠŸå¯¼å…¥ ConmambaBlock")
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥å¤±è´¥: {e}, ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    ConmambaBlock = None

# ==========================================
# å·¥å…·å‡½æ•°
# ==========================================

def safe_log(x, eps=1e-8):
    return torch.log(torch.clamp(x, min=eps))

def safe_div(x, y, eps=1e-8):
    return x / torch.clamp(y, min=eps)

# ==========================================
# æ•°æ®ç®¡ç†å™¨
# ==========================================

class DynamicClusterDataset:
    """
    åŠ¨æ€èšç±»æ•°æ®é›†
    - æ”¯æŒæ ‡ç­¾åŠ¨æ€æ›´æ–°
    - æ”¯æŒå™ªå£°ç‚¹æ ‡è®°å’Œè¿‡æ»¤
    - ç»´æŠ¤ç°‡ä¸­å¿ƒåµŒå…¥
    """
    
    def __init__(self, data_dir: str, seq_len: int = 150):
        self.seq_len = seq_len
        self.base_mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}
        self.idx_to_base = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}
        
        # æ ¸å¿ƒæ•°æ®ç»“æ„
        self.reads = []           # æ‰€æœ‰readsçš„åºåˆ—
        self.read_ids = []        # readæ ‡è¯†
        self.labels = []          # å½“å‰ç°‡æ ‡ç­¾ (-1è¡¨ç¤ºå™ªå£°)
        self.original_labels = [] # åˆå§‹æ ‡ç­¾ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        self.confidence = []      # æ¯ä¸ªreadçš„ç½®ä¿¡åº¦åˆ†æ•°
        
        # ç°‡ä¿¡æ¯
        self.num_clusters = 0     # å½“å‰æœ‰æ•ˆç°‡æ•°é‡
        self.cluster_centers = {} # ç°‡ä¸­å¿ƒåµŒå…¥ {cluster_id: embedding}
        self.cluster_refs = {}    # ç°‡çš„ä»£è¡¨åºåˆ—ï¼ˆæ¥è‡ªCloverï¼‰
        
        # åŠ è½½æ•°æ®
        self._load_data(data_dir)
        
    def _load_data(self, data_dir: str):
        """åŠ è½½FedDNAæ ¼å¼çš„æ•°æ®"""
        read_path = os.path.join(data_dir, "read.txt")
        ref_path = os.path.join(data_dir, "ref.txt")
        if not os.path.exists(ref_path):
            ref_path = os.path.join(data_dir, "reference.txt")
        
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_dir}")
        
        # åŠ è½½ä»£è¡¨åºåˆ—ï¼ˆç°‡ä¸­å¿ƒï¼‰
        with open(ref_path, 'r') as f:
            refs = [line.strip() for line in f if line.strip()]
        
        # åŠ è½½reads
        with open(read_path, 'r') as f:
            content = f.read().strip()
        
        raw_clusters = content.split("===============================")
        
        read_idx = 0
        for cluster_id, cluster_block in enumerate(raw_clusters):
            if not cluster_block.strip() or cluster_id >= len(refs):
                continue
            
            reads_in_cluster = [r.strip() for r in cluster_block.strip().split('\n') if r.strip()]
            
            # ä¿å­˜ç°‡çš„ä»£è¡¨åºåˆ—
            self.cluster_refs[cluster_id] = refs[cluster_id]
            
            for read_seq in reads_in_cluster:
                self.reads.append(read_seq)
                self.read_ids.append(f"read_{read_idx}")
                self.labels.append(cluster_id)
                self.original_labels.append(cluster_id)
                self.confidence.append(1.0)  # åˆå§‹ç½®ä¿¡åº¦ä¸º1
                read_idx += 1
        
        self.num_clusters = len(self.cluster_refs)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºæ“ä½œ
        self.labels = np.array(self.labels)
        self.original_labels = np.array(self.original_labels)
        self.confidence = np.array(self.confidence)
        
        print(f"âœ… åŠ è½½å®Œæˆ:")
        print(f"   - Reads: {len(self.reads)}")
        print(f"   - åˆå§‹ç°‡æ•°é‡ K: {self.num_clusters}")
        print(f"   - Reads/ç°‡: {len(self.reads) / self.num_clusters:.1f}")
    
    def one_hot_encode(self, seq: str) -> np.ndarray:
        """åºåˆ—è½¬one-hotç¼–ç """
        arr = np.zeros((self.seq_len, 4), dtype=np.float32)
        for i, char in enumerate(seq[:self.seq_len]):
            if char in self.base_mapping:
                arr[i, self.base_mapping[char]] = 1.0
        return arr
    
    def get_cluster_reads(self, cluster_id: int, exclude_noise: bool = True) -> List[int]:
        """è·å–æŸä¸ªç°‡çš„æ‰€æœ‰readç´¢å¼•"""
        if exclude_noise:
            return [i for i, l in enumerate(self.labels) if l == cluster_id and l != -1]
        return [i for i, l in enumerate(self.labels) if l == cluster_id]
    
    def get_valid_reads(self) -> List[int]:
        """è·å–æ‰€æœ‰éå™ªå£°çš„readç´¢å¼•"""
        return [i for i, l in enumerate(self.labels) if l != -1]
    
    def get_active_clusters(self) -> List[int]:
        """è·å–å½“å‰æœ‰æ•ˆçš„ç°‡IDåˆ—è¡¨"""
        active = set(self.labels[self.labels != -1])
        return sorted(list(active))
    
    def update_label(self, read_idx: int, new_label: int):
        """æ›´æ–°å•ä¸ªreadçš„æ ‡ç­¾"""
        self.labels[read_idx] = new_label
    
    def mark_as_noise(self, read_idx: int):
        """å°†readæ ‡è®°ä¸ºå™ªå£°"""
        self.labels[read_idx] = -1
    
    def update_confidence(self, read_idx: int, conf: float):
        """æ›´æ–°ç½®ä¿¡åº¦"""
        self.confidence[read_idx] = conf
    
    def get_statistics(self) -> Dict:
        """è·å–å½“å‰æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        active_clusters = self.get_active_clusters()
        noise_count = np.sum(self.labels == -1)
        
        cluster_sizes = []
        for cid in active_clusters:
            size = np.sum(self.labels == cid)
            cluster_sizes.append(size)
        
        return {
            'total_reads': len(self.reads),
            'active_clusters': len(active_clusters),
            'noise_count': noise_count,
            'noise_ratio': noise_count / len(self.reads),
            'avg_cluster_size': np.mean(cluster_sizes) if cluster_sizes else 0,
            'min_cluster_size': np.min(cluster_sizes) if cluster_sizes else 0,
            'max_cluster_size': np.max(cluster_sizes) if cluster_sizes else 0,
        }

# ==========================================
# ç¼–ç å™¨
# ==========================================

def calc_same_padding(kernel_size):
    pad = kernel_size // 2
    return (pad, pad - (kernel_size + 1) % 2)

class Conv2dUpsampling(nn.Module):
    def __init__(self, in_channels: int, out_channels: int, dropout_p: float = 0.1):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Dropout(dropout_p)
        )
    
    def forward(self, x):
        # x: [B, L, 4]
        x = x.unsqueeze(1)  # [B, 1, L, 4]
        x = self.conv(x)    # [B, C, L, 4]
        B, C, L, D = x.shape
        x = x.permute(0, 2, 1, 3).contiguous()  # [B, L, C, D]
        x = x.view(B, L, C * D)  # [B, L, C*D]
        return x

class DNAEncoder(nn.Module):
    """DNAåºåˆ—ç¼–ç å™¨"""
    
    def __init__(self, hidden_dim: int = 128, seq_len: int = 150):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.seq_len = seq_len
        
        self.upsampling = Conv2dUpsampling(1, hidden_dim // 4, dropout_p=0.1)
        
        # ç‰¹å¾ç»´åº¦é€‚é…
        self.feature_proj = nn.Linear(hidden_dim // 4 * 4, hidden_dim)
        
        # ä½¿ç”¨ConmambaBlockæˆ–LSTM
        if ConmambaBlock is not None:
            self.encoder = ConmambaBlock(
                dim=hidden_dim, ff_mult=4, conv_expansion_factor=2,
                conv_kernel_size=31, attn_dropout=0.1, ff_dropout=0.1, conv_dropout=0.1
            )
        else:
            self.encoder = nn.LSTM(
                hidden_dim, hidden_dim // 2, num_layers=2,
                batch_first=True, bidirectional=True, dropout=0.1
            )
        
        # æŠ•å½±åˆ°åµŒå…¥ç©ºé—´
        self.embed_proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
    def forward(self, x):
        """
        x: [B, L, 4] - one-hotç¼–ç çš„åºåˆ—
        è¿”å›: 
            - sequence_features: [B, L, hidden_dim] - åºåˆ—çº§ç‰¹å¾
            - embedding: [B, hidden_dim] - å…¨å±€åµŒå…¥
        """
        # ä¸Šé‡‡æ ·
        x = self.upsampling(x)  # [B, L, C*4]
        x = self.feature_proj(x)  # [B, L, hidden_dim]
        
        # ç¼–ç 
        if isinstance(self.encoder, nn.LSTM):
            x, _ = self.encoder(x)  # [B, L, hidden_dim]
        else:
            x = self.encoder(x)  # [B, L, hidden_dim]
        
        sequence_features = x
        
        # å…¨å±€åµŒå…¥ (å¹³å‡æ± åŒ–)
        embedding = torch.mean(x, dim=1)  # [B, hidden_dim]
        embedding = self.embed_proj(embedding)  # [B, hidden_dim]
        
        return sequence_features, embedding

# ==========================================
# è¯æ®è§£ç å™¨
# ==========================================

class EvidenceDecoder(nn.Module):
    """
    è¯æ®è§£ç å™¨
    - ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆ4ä¸ªç¢±åŸºçš„è¯æ®
    - è®¡ç®—è¯æ®å¼ºåº¦ï¼ˆç½®ä¿¡åº¦ï¼‰
    """
    
    def __init__(self, hidden_dim: int = 128, seq_len: int = 150):
        super().__init__()
        
        self.rnn = nn.LSTM(
            hidden_dim, hidden_dim, num_layers=2,
            batch_first=True, dropout=0.1
        )
        
        self.evidence_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 4),
            nn.Softplus()  # ä¿è¯è¯æ®éè´Ÿ
        )
        
        self.min_evidence = 0.1
        
    def forward(self, sequence_features):
        """
        sequence_features: [B, L, hidden_dim]
        è¿”å›:
            - evidence: [B, L, 4] - æ¯ä¸ªä½ç½®çš„è¯æ®
            - strength: [B] - æ¯ä¸ªæ ·æœ¬çš„è¯æ®å¼ºåº¦
        """
        x, _ = self.rnn(sequence_features)
        evidence = self.evidence_head(x) + self.min_evidence  # [B, L, 4]
        
        # è®¡ç®—è¯æ®å¼ºåº¦ (æ€»è¯æ®é‡)
        strength = torch.sum(evidence, dim=(1, 2))  # [B]
        
        # å½’ä¸€åŒ–å¼ºåº¦åˆ°[0, 1]èŒƒå›´
        strength = torch.sigmoid(strength / 1000 - 3)  # è°ƒæ•´é˜ˆå€¼
        
        return evidence, strength

# ==========================================
# å¯¹æ¯”å­¦ä¹ æ¨¡å—
# ==========================================

class ContrastiveLearning(nn.Module):
    """
    ç›‘ç£å¯¹æ¯”å­¦ä¹ 
    - åŒä¸€ç°‡çš„readsæ‹‰è¿‘
    - ä¸åŒç°‡çš„readsæ¨è¿œ
    """
    
    def __init__(self, temperature: float = 0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        embeddings: [B, D] - L2å½’ä¸€åŒ–çš„åµŒå…¥
        labels: [B] - ç°‡æ ‡ç­¾
        """
        # L2å½’ä¸€åŒ–
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature  # [B, B]
        
        # åˆ›å»ºæ ‡ç­¾æ©ç 
        labels = labels.view(-1, 1)
        mask_positive = (labels == labels.T).float()  # åŒç°‡ä¸º1
        mask_negative = 1 - mask_positive
        
        # ç§»é™¤å¯¹è§’çº¿
        eye = torch.eye(embeddings.shape[0], device=embeddings.device)
        mask_positive = mask_positive - eye
        
        # InfoNCEæŸå¤±
        exp_sim = torch.exp(sim_matrix - torch.max(sim_matrix, dim=1, keepdim=True)[0])
        
        # åˆ†å­ï¼šæ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦
        pos_sim = (exp_sim * mask_positive).sum(dim=1)
        
        # åˆ†æ¯ï¼šæ‰€æœ‰æ ·æœ¬å¯¹ï¼ˆé™¤äº†è‡ªå·±ï¼‰
        all_sim = (exp_sim * (1 - eye)).sum(dim=1)
        
        # é¿å…é™¤é›¶
        loss = -safe_log(safe_div(pos_sim, all_sim))
        
        # åªè®¡ç®—æœ‰æ­£æ ·æœ¬çš„
        valid_mask = mask_positive.sum(dim=1) > 0
        if valid_mask.sum() > 0:
            return loss[valid_mask].mean()
        return torch.tensor(0.0, device=embeddings.device)

# ==========================================
# è¯æ®èåˆ
# ==========================================

def evidence_fusion(evidences: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
    """
    åŠ æƒè¯æ®èåˆ
    
    evidences: [N, L, 4] - Næ¡readsçš„è¯æ®
    weights: [N] - æ¯æ¡readçš„æƒé‡ï¼ˆåŸºäºå¼ºåº¦ï¼‰
    
    è¿”å›: [L, 4] - èåˆåçš„è¯æ®
    """
    # å½’ä¸€åŒ–æƒé‡
    weights = F.softmax(weights, dim=0)  # [N]
    
    # åŠ æƒèåˆ
    weights = weights.view(-1, 1, 1)  # [N, 1, 1]
    fused = torch.sum(evidences * weights, dim=0)  # [L, 4]
    
    return fused

# ==========================================
# ä¸»æ¨¡å‹
# ==========================================

class EvidenceDrivenClusteringModel(nn.Module):
    """
    è¯æ®é©±åŠ¨èšç±»æ¨¡å‹
    
    åŠŸèƒ½:
    1. ç¼–ç readså¾—åˆ°åµŒå…¥
    2. ç”Ÿæˆè¯æ®å’Œå¼ºåº¦
    3. å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–ç‰¹å¾ç©ºé—´
    4. èåˆè¯æ®è¿›è¡Œé¢„æµ‹
    """
    
    def __init__(self, hidden_dim: int = 128, seq_len: int = 150):
        super().__init__()
        
        self.encoder = DNAEncoder(hidden_dim, seq_len)
        self.decoder = EvidenceDecoder(hidden_dim, seq_len)
        self.contrastive = ContrastiveLearning(temperature=0.1)
        
        self.hidden_dim = hidden_dim
        self.seq_len = seq_len
        
    def forward(self, reads: torch.Tensor):
        """
        reads: [B, L, 4]
        è¿”å›:
            - embeddings: [B, hidden_dim]
            - evidence: [B, L, 4]
            - strength: [B]
        """
        sequence_features, embeddings = self.encoder(reads)
        evidence, strength = self.decoder(sequence_features)
        
        return embeddings, evidence, strength
    
    def compute_losses(self, reads: torch.Tensor, labels: torch.Tensor, 
                       ref_evidence: torch.Tensor = None) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ‰€æœ‰æŸå¤±
        
        reads: [B, L, 4]
        labels: [B] - ç°‡æ ‡ç­¾
        ref_evidence: [L, 4] - å‚è€ƒè¯æ®ï¼ˆèåˆåçš„ï¼‰
        """
        embeddings, evidence, strength = self.forward(reads)
        
        losses = {}
        
        # 1. å¯¹æ¯”å­¦ä¹ æŸå¤±
        losses['contrastive'] = self.contrastive(embeddings, labels)
        
        # 2. è¯æ®ä¸€è‡´æ€§æŸå¤±ï¼ˆåŒä¸€ç°‡çš„evidenceåº”è¯¥ç›¸ä¼¼ï¼‰
        unique_labels = torch.unique(labels[labels >= 0])
        consistency_loss = torch.tensor(0.0, device=reads.device)
        count = 0
        
        for label in unique_labels:
            mask = labels == label
            if mask.sum() > 1:
                cluster_evidence = evidence[mask]  # [n, L, 4]
                mean_evidence = cluster_evidence.mean(dim=0, keepdim=True)  # [1, L, 4]
                diff = cluster_evidence - mean_evidence
                consistency_loss = consistency_loss + torch.mean(diff ** 2)
                count += 1
        
        if count > 0:
            losses['consistency'] = consistency_loss / count
        else:
            losses['consistency'] = torch.tensor(0.0, device=reads.device)
        
        # 3. å¦‚æœæœ‰å‚è€ƒè¯æ®ï¼Œè®¡ç®—é‡å»ºæŸå¤±
        if ref_evidence is not None:
            # KLæ•£åº¦
            alpha_pred = evidence + 1.0
            alpha_ref = ref_evidence.unsqueeze(0) + 1.0
            
            S_pred = alpha_pred.sum(dim=-1, keepdim=True)
            S_ref = alpha_ref.sum(dim=-1, keepdim=True)
            
            prob_pred = alpha_pred / S_pred
            prob_ref = alpha_ref / S_ref
            
            kl = torch.sum(prob_pred * (safe_log(prob_pred) - safe_log(prob_ref)), dim=-1)
            losses['reconstruction'] = kl.mean()
        
        # 4. æ€»æŸå¤±
        losses['total'] = (
            losses['contrastive'] + 
            0.5 * losses['consistency'] + 
            losses.get('reconstruction', torch.tensor(0.0, device=reads.device))
        )
        
        return losses, embeddings, evidence, strength

# ==========================================
# è®­ç»ƒå™¨
# ==========================================

class EvidenceDrivenTrainer:
    """
    è¯æ®é©±åŠ¨è®­ç»ƒå™¨
    
    è®­ç»ƒæµç¨‹:
    1. Mini-batchè®­ç»ƒï¼ˆå¯¹æ¯”å­¦ä¹  + è¯æ®ç”Ÿæˆï¼‰
    2. å›°éš¾æ ·æœ¬æ£€æµ‹ï¼ˆåŸºäºè¯æ®å¼ºåº¦ï¼‰
    3. æ ‡ç­¾ä¿®æ­£ï¼ˆé‡åˆ†é…æˆ–æ ‡è®°å™ªå£°ï¼‰
    """
    
    def __init__(self, 
                 model: EvidenceDrivenClusteringModel,
                 dataset: DynamicClusterDataset,
                 device: torch.device,
                 lr: float = 1e-4,
                 confidence_threshold: float = 0.3,
                 noise_distance_threshold: float = 2.0):
        
        self.model = model.to(device)
        self.dataset = dataset
        self.device = device
        
        # é˜ˆå€¼
        self.confidence_threshold = confidence_threshold  # ä½äºæ­¤å€¼ä¸ºå›°éš¾æ ·æœ¬
        self.noise_distance_threshold = noise_distance_threshold  # é«˜äºæ­¤å€¼ä¸ºå™ªå£°
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        
        # ç°‡ä¸­å¿ƒåµŒå…¥
        self.cluster_centers = {}
        
        # å†å²è®°å½•
        self.history = {
            'epoch': [],
            'total_loss': [],
            'contrastive_loss': [],
            'consistency_loss': [],
            'refinement_count': [],
            'noise_count': [],
            'active_clusters': []
        }
    
    def compute_cluster_centers(self):
        """è®¡ç®—æ‰€æœ‰ç°‡çš„ä¸­å¿ƒåµŒå…¥"""
        self.model.eval()
        
        cluster_embeddings = defaultdict(list)
        
        with torch.no_grad():
            for cid in self.dataset.get_active_clusters():
                read_indices = self.dataset.get_cluster_reads(cid)
                
                if len(read_indices) == 0:
                    continue
                
                # åˆ†æ‰¹å¤„ç†
                batch_size = 32
                embeddings = []
                
                for i in range(0, len(read_indices), batch_size):
                    batch_indices = read_indices[i:i+batch_size]
                    reads = [self.dataset.reads[idx] for idx in batch_indices]
                    reads_encoded = torch.tensor(
                        np.array([self.dataset.one_hot_encode(r) for r in reads])
                    ).to(self.device)
                    
                    emb, _, _ = self.model(reads_encoded)
                    embeddings.append(emb.cpu())
                
                all_emb = torch.cat(embeddings, dim=0)
                self.cluster_centers[cid] = all_emb.mean(dim=0)
        
        print(f"   âœ… æ›´æ–°äº† {len(self.cluster_centers)} ä¸ªç°‡ä¸­å¿ƒ")
    
    def train_epoch(self, batch_size: int = 16, reads_per_cluster: int = 4) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = defaultdict(float)
        num_batches = 0
        
        # è·å–æœ‰æ•ˆç°‡
        active_clusters = self.dataset.get_active_clusters()
        
        if len(active_clusters) < 2:
            print("âš ï¸ æœ‰æ•ˆç°‡æ•°é‡ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ")
            return epoch_losses
        
        # éšæœºé‡‡æ ·batch
        num_iterations = len(self.dataset.get_valid_reads()) // (batch_size * reads_per_cluster)
        num_iterations = max(num_iterations, 10)
        
        for _ in range(num_iterations):
            # éšæœºé€‰æ‹©ç°‡
            selected_clusters = random.sample(
                active_clusters, 
                min(batch_size, len(active_clusters))
            )
            
            # ä»æ¯ä¸ªç°‡ä¸­é‡‡æ ·reads
            batch_reads = []
            batch_labels = []
            
            for cid in selected_clusters:
                cluster_reads = self.dataset.get_cluster_reads(cid)
                if len(cluster_reads) < reads_per_cluster:
                    selected_reads = cluster_reads
                else:
                    selected_reads = random.sample(cluster_reads, reads_per_cluster)
                
                for idx in selected_reads:
                    batch_reads.append(self.dataset.one_hot_encode(self.dataset.reads[idx]))
                    batch_labels.append(cid)
            
            if len(batch_reads) < 4:
                continue
            
            # è½¬æ¢ä¸ºtensor
            reads_tensor = torch.tensor(np.array(batch_reads)).to(self.device)
            labels_tensor = torch.tensor(batch_labels).to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            losses, embeddings, evidence, strength = self.model.compute_losses(
                reads_tensor, labels_tensor
            )
            
            # åå‘ä¼ æ’­
            loss = losses['total']
            if torch.isnan(loss) or torch.isinf(loss):
                continue
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # è®°å½•
            for key, value in losses.items():
                epoch_losses[key] += value.item()
            num_batches += 1
            
            # æ›´æ–°ç½®ä¿¡åº¦ï¼ˆç”¨äºåç»­refinementï¼‰
            for i, idx in enumerate([self.dataset.get_cluster_reads(cid) 
                                     for cid in selected_clusters for _ in range(reads_per_cluster)]):
                if i < len(strength):
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è·Ÿè¸ªå…·ä½“çš„readç´¢å¼•
                    pass
        
        # å¹³å‡æŸå¤±
        if num_batches > 0:
            for key in epoch_losses:
                epoch_losses[key] /= num_batches
        
        return dict(epoch_losses)
    
    def refine_labels(self) -> Tuple[int, int]:
        """
        å›°éš¾æ ·æœ¬ä¿®æ­£
        
        è¿”å›: (ä¿®æ­£æ•°é‡, æ–°å™ªå£°æ•°é‡)
        """
        self.model.eval()
        
        # é¦–å…ˆæ›´æ–°ç°‡ä¸­å¿ƒ
        self.compute_cluster_centers()
        
        if len(self.cluster_centers) == 0:
            return 0, 0
        
        refinement_count = 0
        new_noise_count = 0
        
        # å¤„ç†æ‰€æœ‰éå™ªå£°çš„reads
        valid_indices = self.dataset.get_valid_reads()
        
        with torch.no_grad():
            # åˆ†æ‰¹å¤„ç†
            batch_size = 64
            
            for start in range(0, len(valid_indices), batch_size):
                batch_indices = valid_indices[start:start+batch_size]
                
                reads = [self.dataset.reads[idx] for idx in batch_indices]
                reads_encoded = torch.tensor(
                    np.array([self.dataset.one_hot_encode(r) for r in reads])
                ).to(self.device)
                
                # è·å–åµŒå…¥å’Œå¼ºåº¦
                embeddings, _, strength = self.model(reads_encoded)
                embeddings = embeddings.cpu()
                strength = strength.cpu().numpy()
                
                # å¤„ç†æ¯ä¸ªæ ·æœ¬
                for i, (idx, emb, conf) in enumerate(zip(batch_indices, embeddings, strength)):
                    current_label = self.dataset.labels[idx]
                    
                    # æ›´æ–°ç½®ä¿¡åº¦
                    self.dataset.update_confidence(idx, conf)
                    
                    # åªå¤„ç†ä½ç½®ä¿¡åº¦æ ·æœ¬
                    if conf >= self.confidence_threshold:
                        continue
                    
                    # è®¡ç®—åˆ°æ‰€æœ‰ç°‡ä¸­å¿ƒçš„è·ç¦»
                    distances = {}
                    for cid, center in self.cluster_centers.items():
                        dist = torch.norm(emb - center).item()
                        distances[cid] = dist
                    
                    if not distances:
                        continue
                    
                    # æ‰¾æœ€è¿‘çš„ç°‡
                    nearest_cluster = min(distances, key=distances.get)
                    min_distance = distances[nearest_cluster]
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®æ­£
                    if min_distance > self.noise_distance_threshold:
                        # å¤ªè¿œï¼Œæ ‡è®°ä¸ºå™ªå£°
                        self.dataset.mark_as_noise(idx)
                        new_noise_count += 1
                    elif nearest_cluster != current_label:
                        # é‡åˆ†é…åˆ°æ›´è¿‘çš„ç°‡
                        self.dataset.update_label(idx, nearest_cluster)
                        refinement_count += 1
        
        return refinement_count, new_noise_count
    
    def train(self, 
              max_epochs: int = 30,
              refinement_interval: int = 3,
              convergence_threshold: float = 0.01) -> Dict:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        """
        print("="*60)
        print("ğŸ§¬ è¯æ®é©±åŠ¨DNAèšç±»ä¼˜åŒ–")
        print("="*60)
        
        # åˆå§‹ç»Ÿè®¡
        stats = self.dataset.get_statistics()
        print(f"\nğŸ“Š åˆå§‹çŠ¶æ€:")
        print(f"   - æ€»Reads: {stats['total_reads']}")
        print(f"   - ç°‡æ•°é‡ K: {stats['active_clusters']}")
        print(f"   - ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")
        print(f"   - å™ªå£°è·ç¦»é˜ˆå€¼: {self.noise_distance_threshold}")
        
        for epoch in range(max_epochs):
            print(f"\n{'='*60}")
            print(f"ğŸ“ Epoch {epoch + 1}/{max_epochs}")
            print(f"{'='*60}")
            
            # è®­ç»ƒ
            losses = self.train_epoch()
            
            print(f"\nğŸ“ˆ è®­ç»ƒæŸå¤±:")
            print(f"   - Total: {losses.get('total', 0):.4f}")
            print(f"   - Contrastive: {losses.get('contrastive', 0):.4f}")
            print(f"   - Consistency: {losses.get('consistency', 0):.4f}")
            
            # è®°å½•å†å²
            self.history['epoch'].append(epoch + 1)
            self.history['total_loss'].append(losses.get('total', 0))
            self.history['contrastive_loss'].append(losses.get('contrastive', 0))
            self.history['consistency_loss'].append(losses.get('consistency', 0))
            
            # æ ‡ç­¾ä¿®æ­£
            if (epoch + 1) % refinement_interval == 0:
                print(f"\nğŸ”§ æ ‡ç­¾ä¿®æ­£...")
                refinement_count, new_noise = self.refine_labels()
                
                stats = self.dataset.get_statistics()
                
                print(f"   - é‡åˆ†é…: {refinement_count} reads")
                print(f"   - æ–°å™ªå£°: {new_noise} reads")
                print(f"   - å½“å‰å™ªå£°ç‡: {stats['noise_ratio']*100:.2f}%")
                print(f"   - æœ‰æ•ˆç°‡æ•°: {stats['active_clusters']}")
                
                self.history['refinement_count'].append(refinement_count)
                self.history['noise_count'].append(stats['noise_count'])
                self.history['active_clusters'].append(stats['active_clusters'])
                
                # æ”¶æ•›æ£€æŸ¥
                total_valid = len(self.dataset.get_valid_reads())
                refinement_ratio = refinement_count / max(total_valid, 1)
                
                if refinement_ratio < convergence_threshold:
                    print(f"\nâœ… æ”¶æ•›! ä¿®æ­£æ¯”ä¾‹ {refinement_ratio*100:.2f}% < {convergence_threshold*100}%")
                    break
            
            self.scheduler.step()
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n{'='*60}")
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"{'='*60}")
        
        final_stats = self.dataset.get_statistics()
        print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        print(f"   - æœ‰æ•ˆReads: {final_stats['total_reads'] - final_stats['noise_count']}")
        print(f"   - å™ªå£°Reads: {final_stats['noise_count']} ({final_stats['noise_ratio']*100:.1f}%)")
        print(f"   - æœ€ç»ˆç°‡æ•°: {final_stats['active_clusters']}")
        print(f"   - å¹³å‡ç°‡å¤§å°: {final_stats['avg_cluster_size']:.1f}")
        
        return self.history
    
    def get_final_clustering(self) -> Dict[int, List[str]]:
        """è·å–æœ€ç»ˆèšç±»ç»“æœ"""
        result = defaultdict(list)
        
        for i, (read, label) in enumerate(zip(self.dataset.reads, self.dataset.labels)):
            if label != -1:
                result[label].append(read)
        
        return dict(result)

# ==========================================
# å¯è§†åŒ–
# ==========================================

def plot_training_history(history: Dict, output_path: str = "training_history.png"):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    fig.suptitle('Evidence-Driven Clustering Training', fontsize=14, fontweight='bold')
    
    # 1. æŸå¤±æ›²çº¿
    ax1 = axes[0, 0]
    ax1.plot(history['epoch'], history['total_loss'], 'b-', label='Total', linewidth=2)
    ax1.plot(history['epoch'], history['contrastive_loss'], 'r--', label='Contrastive', linewidth=2)
    ax1.set_title('Loss Curves')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ä¿®æ­£æ•°é‡
    ax2 = axes[0, 1]
    if history['refinement_count']:
        refinement_epochs = [e for i, e in enumerate(history['epoch']) if (i+1) % 3 == 0][:len(history['refinement_count'])]
        ax2.bar(refinement_epochs, history['refinement_count'], alpha=0.7, color='orange')
    ax2.set_title('Label Refinements per Interval')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Count')
    ax2.grid(True, alpha=0.3)
    
    # 3. å™ªå£°æ•°é‡
    ax3 = axes[1, 0]
    if history['noise_count']:
        noise_epochs = refinement_epochs[:len(history['noise_count'])]
        ax3.plot(noise_epochs, history['noise_count'], 'r-o', linewidth=2)
    ax3.set_title('Noise Count')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Count')
    ax3.grid(True, alpha=0.3)
    
    # 4. æœ‰æ•ˆç°‡æ•°
    ax4 = axes[1, 1]
    if history['active_clusters']:
        cluster_epochs = refinement_epochs[:len(history['active_clusters'])]
        ax4.plot(cluster_epochs, history['active_clusters'], 'g-o', linewidth=2)
    ax4.set_title('Active Clusters (K)')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('K')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {output_path}")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================

def main():
    print("="*60)
    print("ğŸ§¬ è¯æ®é©±åŠ¨DNAèšç±»ä¼˜åŒ–æ¨¡å‹")
    print("="*60)
    
    # é…ç½®
    DATA_DIR = "CC/Step0/Experiments/20251216_145746_Improved_Data_Test/03_FedDNA_In"
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    
    # éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # åŠ è½½æ•°æ®
    dataset = DynamicClusterDataset(DATA_DIR, seq_len=150)
    
    # åˆ›å»ºæ¨¡å‹
    model = EvidenceDrivenClusteringModel(
        hidden_dim=128,
        seq_len=150
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EvidenceDrivenTrainer(
        model=model,
        dataset=dataset,
        device=device,
        lr=1e-4,
        confidence_threshold=0.3,   # ä½äº30%ç½®ä¿¡åº¦ä¸ºå›°éš¾æ ·æœ¬
        noise_distance_threshold=2.0  # è·ç¦»é˜ˆå€¼
    )
    
    # è®­ç»ƒ
    history = trainer.train(
        max_epochs=30,
        refinement_interval=3,
        convergence_threshold=0.01
    )
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜èšç±»ç»“æœ
    final_clusters = trainer.get_final_clustering()
    with open("final_clustering.txt", 'w') as f:
        for cid, reads in final_clusters.items():
            f.write(f"=== Cluster {cid} ({len(reads)} reads) ===\n")
            for read in reads[:5]:  # åªä¿å­˜å‰5æ¡ä½œä¸ºç¤ºä¾‹
                f.write(f"{read}\n")
            f.write("\n")
    print(f"âœ… èšç±»ç»“æœ: final_clustering.txt")
    
    # ä¿å­˜æ ‡ç­¾
    with open("final_labels.txt", 'w') as f:
        f.write("read_id\toriginal_label\tfinal_label\tconfidence\n")
        for i in range(len(dataset.reads)):
            f.write(f"{dataset.read_ids[i]}\t{dataset.original_labels[i]}\t"
                    f"{dataset.labels[i]}\t{dataset.confidence[i]:.4f}\n")
    print(f"âœ… æ ‡ç­¾æ–‡ä»¶: final_labels.txt")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history(history)
    
    # ä¿å­˜æ¨¡å‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = f"evidence_driven_model_{timestamp}.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'history': history,
        'final_stats': dataset.get_statistics()
    }, model_path)
    print(f"âœ… æ¨¡å‹: {model_path}")
    
    return model, dataset, history

if __name__ == "__main__":
    model, dataset, history = main()
