import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.autograd import Variable
import argparse
import pathlib
import sys
import os
import numpy as np

# å‡è®¾ä½ çš„ my_train.py æ”¾åœ¨ code/ ç›®å½•ä¸‹ï¼Œå’Œå…¶ä»–å­æ–‡ä»¶å¤¹åŒçº§
# å¦‚æœä¸åœ¨ï¼Œè¯·è°ƒæ•´ sys.path æˆ–ç§»åŠ¨æ–‡ä»¶
from data.DNA_data import MyDataset, collater, CustomSampler, CustomBatchSampler
from models.Model import Encoder, Model
from utils.Loss import CEBayesRiskLoss, KLDivergenceLoss

def main():
    # 1. ç®€å•çš„å‚æ•°é…ç½®
    parser = argparse.ArgumentParser(description="Single Client Training Baseline")
    parser.add_argument('--batch_size', type=int, default=16, help="Batch size (Clusters per batch)")
    parser.add_argument('--lr', type=float, default=0.001, help="Learning rate")
    parser.add_argument('--epochs', type=int, default=5, help="Number of epochs")
    parser.add_argument('--dim', type=int, default=256, help="Model dimension")
    # é»˜è®¤ä½¿ç”¨ 'I' æ•°æ®é›†ï¼Œä½ å¯ä»¥æ”¹æˆ 'B', 'P', 'S'
    parser.add_argument('--dataset_name', type=str, default='I', help="Dataset name: I, B, P, or S")
    args = parser.parse_args()

    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device}")

    # 2. æ•°æ®å‡†å¤‡ (Data Loading)
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®: {args.dataset_name} ...")
    
    # è·¯å¾„å­—å…¸ (è¯·ç¡®ä¿ Dataset æ–‡ä»¶å¤¹åœ¨ä¸Šä¸€çº§æˆ–è€…è·¯å¾„æ­£ç¡®)
    # å‡è®¾ç›®å½•ç»“æ„æ˜¯ code/my_train.py å’Œ code/../Dataset
    path_dict = {
        'I': pathlib.Path('../Dataset/I'),
        'B': pathlib.Path('../Dataset/B'),
        'P': pathlib.Path('../Dataset/P'),
        'S': pathlib.Path('../Dataset/S')
    }
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not path_dict[args.dataset_name].exists():
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥ä»å½“å‰ç›®å½•æ‰¾ (é€‚åº”ä¸åŒçš„è¿è¡Œä½ç½®)
        path_dict = {
            'I': pathlib.Path('Dataset/I'),
            'B': pathlib.Path('Dataset/B'),
            'P': pathlib.Path('Dataset/P'),
            'S': pathlib.Path('Dataset/S')
        }
    
    # ç¡¬ç¼–ç çš„é•¿åº¦å‚æ•° (æ¥è‡ª main_fl_dna.py)
    padding_length_dict = {'I': 155, 'B': 205, 'P': 188, 'S': 201}
    label_length_dict = {'I': 150, 'B': 200, 'P': 183, 'S': 196}
    
    if args.dataset_name not in padding_length_dict:
        raise ValueError(f"æœªçŸ¥çš„æ•°æ®é›†: {args.dataset_name}")

    padding_len = padding_length_dict[args.dataset_name]
    label_len = label_length_dict[args.dataset_name]

    # å®ä¾‹åŒ–æ•°æ®é›†
    train_set = MyDataset(path_dict, datasets=[args.dataset_name], mode='train')
    
    # --- ğŸ”´ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å®šåˆ¶é‡‡æ ·å™¨é˜²æ­¢ IndexError ---
    # 1. Sampler: å¯¹æ•°æ®æŒ‰é•¿åº¦è¿›è¡Œåˆ†ç»„æ’åº
    train_sampler = CustomSampler(data=train_set)
    # 2. BatchSampler: ä¿è¯æ¯ä¸ª Batch é‡Œçš„æ•°æ®é•¿åº¦ä¸€è‡´
    train_batch_sampler = CustomBatchSampler(sampler=train_sampler, batch_size=args.batch_size, drop_last=True)
    # 3. Collater: è´Ÿè´£ Padding å’Œ One-Hot
    train_collate_fn = collater(padding_len)
    
    train_loader = DataLoader(
        dataset=train_set, 
        batch_sampler=train_batch_sampler, # ä½¿ç”¨ batch_sampler
        collate_fn=train_collate_fn,
        num_workers=0 # è°ƒè¯•æ—¶è®¾ä¸º0æ›´å®‰å…¨
    )
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼è®­ç»ƒé›†å¤§å°: {len(train_set)} ä¸ªç°‡")

    # 3. æ¨¡å‹åˆå§‹åŒ– (Model Initialization)
    print("ğŸ§  æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    
    encoder = Encoder(dim=args.dim).to(device)
    model = Model(encoder, args.dim, padding_len, label_len).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-5)
    
    # æŸå¤±å‡½æ•° (DEL æ ‡é…: è´å¶æ–¯é£é™© + KLæ•£åº¦)
    criterion_risk = CEBayesRiskLoss().to(device)
    criterion_kld = KLDivergenceLoss().to(device)

    # 4. è®­ç»ƒå¾ªç¯ (Training Loop)
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    model.train()

    for epoch in range(args.epochs):
        epoch_loss = 0.0
        total_batches = 0
        
        for i, (inputs, labels) in enumerate(train_loader):
            inputs = inputs.to(device).float() # Shape: (B, N, L, 4)
            labels = labels.to(device)         # Shape: (B, L)

            # --- å‰å‘ä¼ æ’­ ---
            # è¿™é‡Œçš„ outputs æ˜¯ fused_evidence
            outputs = model(inputs)

            # --- Label å¤„ç† ---
            # å¸ˆå§çš„ Loss éœ€è¦ One-Hot æ ‡ç­¾
            eye = torch.eye(4, dtype=torch.float32, device=device)
            labels_onehot = eye[labels.long()] 
            
            # --- æŸå¤±è®¡ç®— (åŒ…å« Annealing) ---
            # KL æ•£åº¦çš„æƒé‡éš epoch å¢åŠ è€Œå¢åŠ  (ä» 0 åˆ° 1)
            annealing_coef = min(1.0, (epoch + 0.1) / args.epochs) 
            
            loss_risk = criterion_risk(outputs, labels_onehot)
            loss_kld = criterion_kld(outputs, labels_onehot)
            
            loss = loss_risk + annealing_coef * loss_kld

            # --- åå‘ä¼ æ’­ ---
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            total_batches += 1

            if i % 10 == 0:
                print(f"   Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f} (Risk: {loss_risk.item():.4f}, KLD: {loss_kld.item():.4f})")
        
        avg_loss = epoch_loss / total_batches if total_batches > 0 else 0
        print(f"â­ï¸ Epoch {epoch+1} å®Œæˆ! å¹³å‡ Loss: {avg_loss:.4f}")

    print("ğŸ‰ è®­ç»ƒè„šæœ¬è¿è¡Œç»“æŸï¼Baseline éªŒè¯æˆåŠŸã€‚")

if __name__ == '__main__':
    main()