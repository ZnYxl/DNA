#å‰©ä¸‹çš„ 0.08% å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
import os
import argparse
import numpy as np
import multiprocessing
from functools import partial
from collections import defaultdict
import time

# ================= é…ç½®åŒºåŸŸ =================
HARDCODED_CONFIG = {
    "gt_file": "/mnt/st_data/liangxinyi/code/CC/Step0/Experiments/20251224_155232_Cluster_GT_Test/01_RawData/ground_truth_clusters.txt",
    "read_gt_file": "/mnt/st_data/liangxinyi/code/CC/Step0/Experiments/20251224_155232_Cluster_GT_Test/01_RawData/ground_truth_reads.txt",
    # è‡ªåŠ¨æŒ‡å‘å»é‡ç‰ˆæ–‡ä»¶
    "pred_file": "/mnt/st_data/liangxinyi/code/iterative_results/20251224_155232_Cluster_GT_Test copy/round_3/step2/consensus_sequences_deduplicated.fasta"
}
# ===========================================

def calculate_identity(seq1, seq2):
    if not seq1 or not seq2: return 0.0
    L = min(len(seq1), len(seq2))
    matches = sum(1 for a, b in zip(seq1[:L], seq2[:L]) if a == b)
    return matches / max(len(seq1), len(seq2))

def load_gt_data(path):
    """åŠ è½½ GT ID å’Œ åºåˆ—"""
    gt_map = {}
    with open(path, 'r') as f:
        f.readline()
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                gt_map[int(parts[0])] = parts[1]
    return gt_map

def load_pred_seqs(path):
    """åŠ è½½é¢„æµ‹åºåˆ—"""
    seqs = []
    if not os.path.exists(path): return seqs
    with open(path, 'r') as f:
        seq = []
        for line in f:
            line = line.strip()
            if line.startswith(">"):
                if seq: seqs.append("".join(seq))
                seq = []
            else:
                seq.append(line)
        if seq: seqs.append("".join(seq))
    return seqs

def find_recovered_ids_chunk(chunk_preds, gt_map):
    """
    å¤šè¿›ç¨‹å­ä»»åŠ¡ï¼š
    åªè¿”å›é‚£äº›è¢«å®Œç¾æ‰¾å› (Identity > 0.999) çš„ GT ID
    """
    recovered_in_chunk = set()
    gt_items = list(gt_map.items())
    
    for pseq in chunk_preds:
        # 1. å°è¯•å®Œå…¨åŒ¹é… (æé€Ÿ)
        # æ³¨æ„ï¼šè¿™éœ€è¦åå‘ç´¢å¼•ï¼Œä¸ºäº†ç®€åŒ–å¤šè¿›ç¨‹é€»è¾‘ï¼Œè¿™é‡Œä¸»è¦é æ‰«æ
        # å¦‚æœè¿½æ±‚æè‡´é€Ÿåº¦ï¼Œå¯ä»¥åœ¨ä¸»è¿›ç¨‹åš exact matchï¼Œè¿™é‡Œåªåš fuzzy
        
        best_score = 0.0
        best_id = -1
        
        # ä¼˜åŒ–ï¼šä¸€æ—¦æ‰¾åˆ°å®Œç¾åŒ¹é…å°±åœæ­¢
        for gid, gseq in gt_items:
            # ç®€å•é•¿åº¦è¿‡æ»¤ï¼ŒåŠ é€Ÿæ¯”å¯¹
            if abs(len(pseq) - len(gseq)) > 5: continue
            
            score = calculate_identity(pseq, gseq)
            if score > 0.999:
                best_score = score
                best_id = gid
                break # æ‰¾åˆ°äº†ï¼ä¸‹ä¸€ä¸ªé¢„æµ‹åºåˆ—
        
        if best_score > 0.999:
            recovered_in_chunk.add(best_id)
            
    return recovered_in_chunk

def analyze_read_coverage(read_gt_path, missing_ids):
    print(f"\n   ğŸ“Š æ­£åœ¨æ‰«æåŸå§‹ Reads æ•°æ® (åˆ†æä¸¢å¤±åŸå› )...")
    counts = defaultdict(int)
    
    with open(read_gt_path, 'r') as f:
        f.readline()
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                cid = int(parts[1])
                if cid in missing_ids: # åªè®°å½•æˆ‘ä»¬å…³å¿ƒçš„
                    counts[cid] += 1
    
    print(f"\n   ğŸ•µï¸â€â™‚ï¸ ã€ä¸¢å¤±ç°‡åˆ†ææŠ¥å‘Šã€‘")
    print(f"   GT_ID\tRaw Reads Count\tStatus")
    print("-" * 40)
    
    low_cov_count = 0
    for mid in sorted(list(missing_ids)):
        count = counts[mid]
        flag = "ğŸ”´ æä½ (ç‰©ç†ä¸¢å¤±)" if count < 5 else "âš ï¸ éœ€æ£€æŸ¥"
        if count < 5: low_cov_count += 1
        print(f"   {mid}\t\t{count}\t\t{flag}")
        
    return low_cov_count

def main():
    print(f"ğŸš€ å¼€å§‹é«˜æ€§èƒ½ä¸¢å¤±ç°‡è°ƒæŸ¥...")
    
    # 1. åŠ è½½æ•°æ®
    gt_map = load_gt_data(HARDCODED_CONFIG['gt_file'])
    all_gt_ids = set(gt_map.keys())
    print(f"   âœ… GT æ€»æ•°: {len(all_gt_ids)}")
    
    pred_seqs = load_pred_seqs(HARDCODED_CONFIG['pred_file'])
    print(f"   âœ… é¢„æµ‹åºåˆ—æ•°: {len(pred_seqs)}")
    
    # 2. å¤šè¿›ç¨‹æ¯”å¯¹ (å¤ç”¨ verify_final çš„é«˜æ€§èƒ½é€»è¾‘)
    num_cpus = min(64, multiprocessing.cpu_count())
    print(f"   âš¡ å¯åŠ¨å¤šè¿›ç¨‹æ¯”å¯¹ (CPU: {num_cpus})...")
    start_time = time.time()
    
    chunk_size = len(pred_seqs) // num_cpus + 1
    chunks = [pred_seqs[i:i + chunk_size] for i in range(0, len(pred_seqs), chunk_size)]
    
    pool = multiprocessing.Pool(processes=num_cpus)
    func = partial(find_recovered_ids_chunk, gt_map=gt_map)
    
    recovered_ids = set()
    for res_set in pool.map(func, chunks):
        recovered_ids.update(res_set)
        
    pool.close()
    pool.join()
    
    print(f"   âœ… æ¯”å¯¹å®Œæˆ! è€—æ—¶: {time.time() - start_time:.1f}ç§’")
    
    # 3. è®¡ç®—ä¸¢å¤±
    missing_ids = all_gt_ids - recovered_ids
    print(f"   âŒ ä¸¢å¤±æ€»æ•°: {len(missing_ids)}")
    
    if len(missing_ids) == 0:
        print("\nğŸ‰ å®Œç¾ï¼æ‰€æœ‰ç°‡éƒ½æ‰¾å›æ¥äº†ï¼")
        return

    # 4. åˆ†æåŸå› 
    low_cov = analyze_read_coverage(HARDCODED_CONFIG['read_gt_file'], missing_ids)
    
    print("\n" + "="*50)
    print("ğŸ” æœ€ç»ˆç»“è®º")
    print("="*50)
    print(f"ä¸¢å¤±çš„ {len(missing_ids)} ä¸ªç°‡ä¸­ï¼Œæœ‰ {low_cov} ä¸ªå±äºä½è¦†ç›–åº¦ (<5 reads)ã€‚")
    
    if low_cov == len(missing_ids):
        print("\nâœ… ç»“è®ºæˆç«‹ï¼šæ‰€æœ‰ä¸¢å¤±å‡ä¸ºç‰©ç†å±‚é¢çš„è¦†ç›–åº¦ä¸è¶³å¯¼è‡´ã€‚ç®—æ³•å·²è¾¾ç†è®ºæé™ã€‚")
    else:
        print("\nâš ï¸ è¿˜æœ‰éƒ¨åˆ†é«˜è¦†ç›–åº¦ç°‡ä¸¢å¤±ï¼Œè¯·è®°å½• ID è¿›è¡Œä¸ªæ¡ˆåˆ†æã€‚")

if __name__ == "__main__":
    main()